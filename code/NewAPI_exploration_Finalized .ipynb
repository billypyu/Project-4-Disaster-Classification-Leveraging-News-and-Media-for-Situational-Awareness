{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mission: To webscrap news articles from a variety of sources\n",
    "\n",
    "- Use news API to identify interesting articles\n",
    "- Build scrapping code to scrape the articles from the source provided by the news API\n",
    "- Store the news articles in a permanent database\n",
    "- Process the text of the articles in preparation for NLP\n",
    "- Build interesting models around the text to understand sentiment\n",
    "\n",
    "API source: https://newsapi.org/docs/endpoints/everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "import requests\n",
    "import time\n",
    "now = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_news (search_terms,file_name, n_pagesize, start_page, end_pages, save_to_csv): \n",
    "    '''\n",
    "    term_request = which is the key word for search.\n",
    "    save_to_csv = True indicates csv will be saved\n",
    "    '''\n",
    "    \n",
    "    # API requests\n",
    "    #for term in search_terms: \n",
    "    url = 'https://newsapi.org/v2/everything?'\n",
    "        \n",
    "    param = {\n",
    "    #'country' : 'us',\n",
    "    'q': search_terms,  #search term \n",
    "    'apiKey' : 'e685d6e1420f4882b86d029ed3c1a11d',\n",
    "    'pageSize': n_pagesize, #max page\n",
    "    'language': 'en'}\n",
    "    print (search_terms)\n",
    "        \n",
    "    every_term = requests.get(url, params = param)\n",
    "\n",
    "    articles = every_term.json()['articles'] \n",
    "    \n",
    "    for page in range(start_page, end_pages): #go throught 10 times, and get more pages, 10 more pages\n",
    "        param['page'] = page\n",
    "        \n",
    "        more_term = requests.get(url, params = param)\n",
    "        more_term = more_term.json()['articles']\n",
    "        \n",
    "        articles.extend(more_term)\n",
    "    arts = pd.DataFrame(articles)\n",
    "    \n",
    "    # Drop null and duplicate \n",
    "    arts.dropna(inplace=True)\n",
    "    arts.drop_duplicates(subset='content',inplace = True)\n",
    "    \n",
    "    # Creahttp://localhost:8888/notebooks/dsi/Project4_Disaster_Test_Classification/code/NewAPI_exploration.ipynb#te columns\n",
    "    arts['source_id'] = arts['source'].map(lambda x: x['id'])\n",
    "    arts['source_name'] = arts['source'].map(lambda x: x['name']) #break up the source, source id, and name colums seperate\n",
    "    arts.drop (columns = ['source'], axis=1)\n",
    "    arts['types'] = str(search_terms)\n",
    "    arts['yes_disaster'] = 1\n",
    "\n",
    "    # Save df to csv\n",
    "    if save_to_csv == True: \n",
    "        arts.to_csv('../data/'+str(file_name)+'_'+str(search_terms)+'_'+str(now) +'.csv' ,index = False, sep = \",\") #index = False for no extra columns\n",
    "        print (f'{len(articles)} unique news haved been saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "listb = ['disaster', 'catastrophe','fire','wildfire','flood','flash-flood',\n",
    "'earthquake','quake','tremor','rumbler','Richter scale','Richter',\n",
    "'magnitude','epicenter','damage','casualty','levee','crest','flood-stage',\n",
    "'evacuation','hurricane','class-1','class-2','class-3','class-4','class-5',\n",
    "'Hurricane','damage','Hurricane Maria','winds','Irma','storm','tropical',\n",
    "'landfall','Harvey','flooding','storm','fires','burned',\n",
    "'National','Smoky Mountains','Hurricane Matthew','wildfires']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disaster\n",
      "240 unique news haved been saved\n",
      "catastrophe\n",
      "240 unique news haved been saved\n",
      "fire\n",
      "240 unique news haved been saved\n",
      "wildfire\n",
      "240 unique news haved been saved\n",
      "flood\n",
      "240 unique news haved been saved\n",
      "flash-flood\n",
      "240 unique news haved been saved\n",
      "earthquake\n",
      "240 unique news haved been saved\n",
      "quake\n",
      "240 unique news haved been saved\n",
      "tremor\n",
      "240 unique news haved been saved\n",
      "rumbler\n",
      "80 unique news haved been saved\n",
      "Richter scale\n",
      "240 unique news haved been saved\n",
      "Richter\n",
      "240 unique news haved been saved\n",
      "magnitude\n",
      "240 unique news haved been saved\n",
      "epicenter\n",
      "240 unique news haved been saved\n",
      "damage\n",
      "240 unique news haved been saved\n",
      "casualty\n",
      "240 unique news haved been saved\n",
      "levee\n",
      "240 unique news haved been saved\n",
      "crest\n",
      "240 unique news haved been saved\n",
      "flood-stage\n",
      "240 unique news haved been saved\n",
      "evacuation\n",
      "240 unique news haved been saved\n",
      "hurricane\n",
      "240 unique news haved been saved\n",
      "class-1\n",
      "240 unique news haved been saved\n",
      "class-2\n",
      "240 unique news haved been saved\n",
      "class-3\n",
      "240 unique news haved been saved\n",
      "class-4\n",
      "240 unique news haved been saved\n",
      "class-5\n",
      "240 unique news haved been saved\n",
      "Hurricane\n",
      "240 unique news haved been saved\n",
      "damage\n",
      "240 unique news haved been saved\n",
      "Hurricane Maria\n",
      "240 unique news haved been saved\n",
      "winds\n",
      "240 unique news haved been saved\n",
      "Irma\n",
      "240 unique news haved been saved\n",
      "storm\n",
      "240 unique news haved been saved\n",
      "tropical\n",
      "240 unique news haved been saved\n",
      "landfall\n",
      "240 unique news haved been saved\n",
      "Harvey\n",
      "240 unique news haved been saved\n",
      "flooding\n",
      "240 unique news haved been saved\n",
      "storm\n",
      "240 unique news haved been saved\n",
      "fires\n",
      "240 unique news haved been saved\n",
      "burned\n",
      "240 unique news haved been saved\n",
      "National\n",
      "240 unique news haved been saved\n",
      "Smoky Mountains\n",
      "240 unique news haved been saved\n",
      "Hurricane Matthew\n",
      "240 unique news haved been saved\n",
      "wildfires\n",
      "240 unique news haved been saved\n"
     ]
    }
   ],
   "source": [
    "for i in listb: \n",
    "    save_news (i, file_name ='b', n_pagesize=10, start_page=2, end_pages=25, save_to_csv=True)\n",
    "#save_news (['disaster'], file_name ='b', n_pagesize=10, start_page=2, end_pages=3, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'content', 'description', 'publishedAt', 'source', 'title',\n",
       "       'url', 'urlToImage', 'source_id', 'source_name', 'types',\n",
       "       'yes_disaster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df= pd.read_csv('../data/b_Harvey_1555193272.906144.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billyu/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel_launcher.py:11: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#combine the csv files\n",
    "all_files = glob.glob(\"../data/b_*.csv\")\n",
    "lst = []\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    lst.append(df)\n",
    "\n",
    "df = pd.concat(lst, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../code/'+'bill_'+'consolidated'+'_'+str(now)+'.csv' ,index = False, sep = \",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
