{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mission: To webscrap news articles from a variety of sources\n",
    "\n",
    "- Use news API to identify interesting articles\n",
    "- Build scrapping code to scrape the articles from the source provided by the news API\n",
    "- Store the news articles in a permanent database\n",
    "- Process the text of the articles in preparation for NLP\n",
    "- Build interesting models around the text to understand sentiment\n",
    "\n",
    "API source: https://newsapi.org/docs/endpoints/everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "import requests\n",
    "import time\n",
    "now = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_news (search_terms,file_name, n_pagesize, start_page, end_pages, save_to_csv): \n",
    "    '''\n",
    "    term_request = which is the key word for search.\n",
    "    save_to_csv = True indicates csv will be saved\n",
    "    '''\n",
    "    \n",
    "    # API requests\n",
    "    #for term in search_terms: \n",
    "    url = 'https://newsapi.org/v2/everything?'\n",
    "        \n",
    "    param = {\n",
    "    #'country' : 'us',\n",
    "    'q': search_terms,  #search term \n",
    "    'apiKey' : 'e685d6e1420f4882b86d029ed3c1a11d',\n",
    "    'pageSize': n_pagesize, #max page\n",
    "    'language': 'en'}\n",
    "    print (search_terms)\n",
    "        \n",
    "    every_term = requests.get(url, params = param)\n",
    "\n",
    "    articles = every_term.json()['articles'] \n",
    "    \n",
    "    for page in range(start_page, end_pages): #go throught 10 times, and get more pages, 10 more pages\n",
    "        param['page'] = page\n",
    "        \n",
    "        more_term = requests.get(url, params = param)\n",
    "        more_term = more_term.json()['articles']\n",
    "        \n",
    "        articles.extend(more_term)\n",
    "    arts = pd.DataFrame(articles)\n",
    "    \n",
    "    # Drop null and duplicate \n",
    "    arts.dropna(inplace=True)\n",
    "    arts.drop_duplicates(subset='content',inplace = True)\n",
    "    \n",
    "    # Creahttp://localhost:8888/notebooks/dsi/Project4_Disaster_Test_Classification/code/NewAPI_exploration.ipynb#te columns\n",
    "    arts['source_id'] = arts['source'].map(lambda x: x['id'])\n",
    "    arts['source_name'] = arts['source'].map(lambda x: x['name']) #break up the source, source id, and name colums seperate\n",
    "    arts.drop (columns = ['source'], axis=1)\n",
    "    arts['types'] = str(search_terms)\n",
    "    arts['yes_disaster'] = 1\n",
    "\n",
    "    # Save df to csv\n",
    "    if save_to_csv == True: \n",
    "        arts.to_csv('../data/'+str(file_name)+'_'+str(search_terms)+'_'+str(now) +'.csv' ,index = False, sep = \",\") #index = False for no extra columns\n",
    "        print (f'{len(articles)} unique news haved been saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#call the function \n",
    "# save_news (['forest fire'], file_name ='k1', n_pagesize=10, start_page=2, end_pages=25, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_k = ['severe temperatures',\n",
    "'whiteout'\n",
    ",'60 mph winds'\n",
    ",'50 mph winds'\n",
    ",'75 mph winds'\n",
    ",'devastation'\n",
    ",'cataclysm'\n",
    ",'volcano'\n",
    ",'eruption'\n",
    ",'lava'\n",
    ",'lava flow'\n",
    ",'steam vent'\n",
    ",'acres burned'\n",
    ",'toxic airquality'\n",
    ",'civil unrest'\n",
    ",'riot'\n",
    ",'civil disturbance'\n",
    ",'crisis'\n",
    ",'nuclear accident'\n",
    ",'radiation leak'\n",
    ",'hot zone'\n",
    ",'patient zero'\n",
    ",'oil spill'\n",
    ",'toxic spill'\n",
    ",'dam burst'\n",
    ",'dam failure'\n",
    ",'dam breach'\n",
    ",'levee breach'\n",
    ",'died'\n",
    ",'deadliest' \n",
    ",'devastated'\n",
    ",'dry weather' \n",
    ",'famine'\n",
    ",'refugee'\n",
    ",'displaced population'\n",
    ",'avalanche'\n",
    ",'landslide'\n",
    ",'forest service' \n",
    ",'plague'\n",
    ",'drought'\n",
    ",'wildfire concerns'\n",
    ",'deadliest'\n",
    ",'destructive' \n",
    ",'wildfire emergency'\n",
    ",'forest fire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "severe temperatures\n",
      "240 unique news haved been saved\n",
      "whiteout\n",
      "240 unique news haved been saved\n",
      "60 mph winds\n",
      "240 unique news haved been saved\n",
      "50 mph winds\n",
      "240 unique news haved been saved\n",
      "75 mph winds\n",
      "240 unique news haved been saved\n",
      "devastation\n",
      "240 unique news haved been saved\n",
      "cataclysm\n",
      "240 unique news haved been saved\n",
      "volcano\n",
      "240 unique news haved been saved\n",
      "eruption\n",
      "240 unique news haved been saved\n",
      "lava\n",
      "240 unique news haved been saved\n",
      "lava flow\n",
      "240 unique news haved been saved\n",
      "steam vent\n",
      "240 unique news haved been saved\n",
      "acres burned\n",
      "240 unique news haved been saved\n",
      "toxic airquality\n",
      "11 unique news haved been saved\n",
      "civil unrest\n",
      "240 unique news haved been saved\n",
      "riot\n",
      "240 unique news haved been saved\n",
      "civil disturbance\n",
      "240 unique news haved been saved\n",
      "crisis\n",
      "240 unique news haved been saved\n",
      "nuclear accident\n",
      "240 unique news haved been saved\n",
      "radiation leak\n",
      "240 unique news haved been saved\n",
      "hot zone\n",
      "240 unique news haved been saved\n",
      "patient zero\n",
      "240 unique news haved been saved\n",
      "oil spill\n",
      "240 unique news haved been saved\n",
      "toxic spill\n",
      "240 unique news haved been saved\n",
      "dam burst\n",
      "240 unique news haved been saved\n",
      "dam failure\n",
      "240 unique news haved been saved\n",
      "dam breach\n",
      "240 unique news haved been saved\n",
      "levee breach\n",
      "240 unique news haved been saved\n",
      "died\n",
      "240 unique news haved been saved\n",
      "deadliest\n",
      "240 unique news haved been saved\n",
      "devastated\n",
      "240 unique news haved been saved\n",
      "dry weather\n",
      "240 unique news haved been saved\n",
      "famine\n",
      "240 unique news haved been saved\n",
      "refugee\n",
      "240 unique news haved been saved\n",
      "displaced population\n",
      "240 unique news haved been saved\n",
      "avalanche\n",
      "240 unique news haved been saved\n",
      "landslide\n",
      "240 unique news haved been saved\n",
      "forest service\n",
      "240 unique news haved been saved\n",
      "plague\n",
      "240 unique news haved been saved\n",
      "drought\n",
      "240 unique news haved been saved\n",
      "wildfire concerns\n",
      "240 unique news haved been saved\n",
      "deadliest\n",
      "240 unique news haved been saved\n",
      "destructive\n",
      "240 unique news haved been saved\n",
      "wildfire emergency\n",
      "240 unique news haved been saved\n",
      "forest fire\n",
      "240 unique news haved been saved\n"
     ]
    }
   ],
   "source": [
    "for i in list_k: \n",
    "    save_news (i, file_name ='k', n_pagesize=10, start_page=2, end_pages=25, save_to_csv=True)\n",
    "#save_news (['disaster'], file_name ='b', n_pagesize=10, start_page=2, end_pages=3, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-4071a1fba099>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-4071a1fba099>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    --\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/k_nuclear accident_1555376363.073326.csv\n",
      "../data/k_wildfire concerns_1555376363.073326.csv\n",
      "../data/k_1555263909.821907.csv\n",
      "../data/k_acres burned_1555376363.073326.csv\n",
      "../data/k_1555263862.27465.csv\n",
      "../data/k_hot zone_1555376363.073326.csv\n",
      "../data/k_landslide_1555376363.073326.csv\n",
      "../data/k_civil disturbance_1555376363.073326.csv\n",
      "../data/k_devastation_1555376363.073326.csv\n",
      "../data/k_eruption_1555376363.073326.csv\n",
      "../data/k_1555111727.8464398.csv\n",
      "../data/k_toxic spill_1555376363.073326.csv\n",
      "../data/k_levee breach_1555376363.073326.csv\n",
      "../data/k_destructive_1555376363.073326.csv\n",
      "../data/k_dam failure_1555376363.073326.csv\n",
      "../data/k_dam breach_1555376363.073326.csv\n",
      "../data/k_lava flow_1555376363.073326.csv\n",
      "../data/k_dry weather_1555376363.073326.csv\n",
      "../data/k_1555264346.csv\n",
      "../data/k_toxic airquality_1555376363.073326.csv\n",
      "../data/k_civil unrest_1555376363.073326.csv\n",
      "../data/k_steam vent_1555376363.073326.csv\n",
      "../data/k_75 mph winds_1555376363.073326.csv\n",
      "../data/k_drought_1555376363.073326.csv\n",
      "../data/k_wildfire emergency_1555376363.073326.csv\n",
      "../data/k_severe temperatures_1555376363.073326.csv\n",
      "../data/k_whiteout_1555376363.073326.csv\n",
      "../data/k_dam burst_1555376363.073326.csv\n",
      "../data/k_forest service_1555376363.073326.csv\n",
      "../data/k_famine_1555376363.073326.csv\n",
      "../data/k_deadliest_1555376363.073326.csv\n",
      "../data/k_oil spill_1555376363.073326.csv\n",
      "../data/k_lava_1555376363.073326.csv\n",
      "../data/k_displaced population_1555376363.073326.csv\n",
      "../data/k_died_1555376363.073326.csv\n",
      "../data/k_crisis_1555376363.073326.csv\n",
      "../data/k_avalanche_1555376363.073326.csv\n",
      "../data/k_radiation leak_1555376363.073326.csv\n",
      "../data/k_60 mph winds_1555376363.073326.csv\n",
      "../data/k_patient zero_1555376363.073326.csv\n",
      "../data/k_volcano_1555376363.073326.csv\n",
      "../data/k_refugee_1555376363.073326.csv\n",
      "../data/k_plague_1555376363.073326.csv\n",
      "../data/k_cataclysm_1555376363.073326.csv\n",
      "../data/k_forest fire_1555376363.073326 2.csv\n",
      "../data/k_devastated_1555376363.073326.csv\n",
      "../data/k_riot_1555376363.073326.csv\n",
      "../data/k_forest fire_1555376363.073326.csv\n",
      "../data/k_50 mph winds_1555376363.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "path = \"../data/*.csv\"\n",
    "for fname in glob.glob(path):\n",
    "    print (fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ed4667cc3b4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/b_1554939746.6693828.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'urlToImage'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'source_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('../data/b_1554939746.6693828.csv')\n",
    "df = df.drop(['source','urlToImage','source_id'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moyang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#combine the csv files\n",
    "\n",
    "all_files = glob.glob(\"../data/k_*.csv\")\n",
    "list_k = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    list_k.append(df)\n",
    "\n",
    "df = pd.concat(list_k, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../code/'+'kun_'+'consolidated'+'_'+str(now)+'.csv' ,index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
