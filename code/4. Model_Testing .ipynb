{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from afinn import Afinn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "\n",
    "from project4_function import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "from datetime import date\n",
    "import requests\n",
    "now = time.time()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Model-Testing\" data-toc-modified-id=\"Model-Testing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Model Testing</a></span></li><li><span><a href=\"#Acquire-News-From-News-API-With-Keyword-:-&quot;flood&quot;\" data-toc-modified-id=\"Acquire-News-From-News-API-With-Keyword-:-&quot;flood&quot;-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Acquire News From News API With Keyword : \"flood\"</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Importing-and-Cleaning-dataframe\" data-toc-modified-id=\"Importing-and-Cleaning-dataframe-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Importing and Cleaning dataframe</a></span></li><li><span><a href=\"#second-sets-of-testing-data\" data-toc-modified-id=\"second-sets-of-testing-data-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>second sets of testing data</a></span></li><li><span><a href=\"#Preparing-Test-Data\" data-toc-modified-id=\"Preparing-Test-Data-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Preparing Test Data</a></span></li></ul></li></ul></li><li><span><a href=\"#Training-Models\" data-toc-modified-id=\"Training-Models-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Term-Frequency–Inverse-Document-Frequency-(TFIDF)\" data-toc-modified-id=\"Term-Frequency–Inverse-Document-Frequency-(TFIDF)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Term Frequency–Inverse Document Frequency (TFIDF)</a></span></li></ul></li><li><span><a href=\"#LogisticRegression:-Feature-Importance\" data-toc-modified-id=\"LogisticRegression:-Feature-Importance-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>LogisticRegression: Feature Importance</a></span></li><li><span><a href=\"#MultinomialNB:-Feature-Importance\" data-toc-modified-id=\"MultinomialNB:-Feature-Importance-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>MultinomialNB: Feature Importance</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Prediction-Dataframe\" data-toc-modified-id=\"Prediction-Dataframe-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;</span>Prediction Dataframe</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire News From News API With Keyword : \"flood\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_news (search_terms, file_name, n_pagesize, start_page, end_pages, save_to_csv): \n",
    "    '''\n",
    "    term_request = which is the key word for search.\n",
    "    save_to_csv = True indicates csv will be saved\n",
    "    '''\n",
    "    \n",
    "    # API requests\n",
    "    #for term in search_terms: \n",
    "    url = 'https://newsapi.org/v2/everything?'\n",
    "        \n",
    "    param = {\n",
    "    #'country' : 'us',\n",
    "    'q': search_terms,  #search term \n",
    "    'apiKey' : 'e685d6e1420f4882b86d029ed3c1a11d',\n",
    "    'pageSize': n_pagesize, #max page\n",
    "    'language': 'en'}\n",
    "    print (search_terms)\n",
    "        \n",
    "    every_term = requests.get(url, params = param)\n",
    "\n",
    "    articles = every_term.json()['articles'] \n",
    "    \n",
    "    for page in range(start_page, end_pages): #go throught 10 times, and get more pages, 10 more pages\n",
    "        param['page'] = page\n",
    "        \n",
    "        more_term = requests.get(url, params = param)\n",
    "        more_term = more_term.json()['articles']\n",
    "        \n",
    "        articles.extend(more_term)\n",
    "    arts = pd.DataFrame(articles)\n",
    "    \n",
    "    # Drop null and duplicate \n",
    "    arts.dropna(inplace=True)\n",
    "    arts.drop_duplicates(subset=['content','description'],inplace = True)\n",
    "    \n",
    "    # Creahttp://localhost:8888/notebooks/dsi/Project4_Disaster_Test_Classification/code/NewAPI_exploration.ipynb#te columns\n",
    "    arts['source_id'] = arts['source'].map(lambda x: x['id'])\n",
    "    arts['source_name'] = arts['source'].map(lambda x: x['name']) #break up the source, source id, and name colums seperate\n",
    "    arts.drop (columns = ['source'], axis=1)\n",
    "    arts['types'] = str(search_terms)\n",
    "    arts['yes_disaster'] = 1\n",
    "\n",
    "    # Save df to csv\n",
    "    if save_to_csv == True: \n",
    "        arts.to_csv('../data/'+str(file_name)+'_'+str(search_terms)+'_'+str(now) +'.csv' ,index = False, sep = \",\") #index = False for no extra columns\n",
    "        print (f'{len(articles)} unique news haved been saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_news ('flood', file_name ='e', n_pagesize=10, start_page=2, end_pages=25, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today\n",
      "240 unique news haved been saved\n"
     ]
    }
   ],
   "source": [
    "save_news ('today', file_name ='e', n_pagesize=10, start_page=2, end_pages=25, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Cleaning dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>description</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>types</th>\n",
       "      <th>yes_disaster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eric Holthaus</td>\n",
       "      <td>This story originally appeared on Grist and is...</td>\n",
       "      <td>Rapid collapse of Antarctic glaciers could flo...</td>\n",
       "      <td>2017-11-30T14:00:00Z</td>\n",
       "      <td>{'id': 'wired', 'name': 'Wired'}</td>\n",
       "      <td>Two Melting Antarctic Glaciers Could Decide th...</td>\n",
       "      <td>https://www.wired.com/story/two-melting-glacie...</td>\n",
       "      <td>https://media.wired.com/photos/5a1f4e7f0cb1f52...</td>\n",
       "      <td>wired</td>\n",
       "      <td>Wired</td>\n",
       "      <td>flood</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Patrick Allan</td>\n",
       "      <td>Flash floods can strike with almost no warning...</td>\n",
       "      <td>Flash floods can strike with almost no warning...</td>\n",
       "      <td>2018-06-11T19:00:00Z</td>\n",
       "      <td>{'id': None, 'name': 'Lifehacker.com'}</td>\n",
       "      <td>Here's Everything You Need to Know to Survive ...</td>\n",
       "      <td>https://lifehacker.com/heres-everything-you-ne...</td>\n",
       "      <td>https://i.kinja-img.com/gawker-media/image/upl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lifehacker.com</td>\n",
       "      <td>flood</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mike Butcher</td>\n",
       "      <td>The flood underinsurance problem is arguably t...</td>\n",
       "      <td>The flood underinsurance problem is arguably t...</td>\n",
       "      <td>2018-08-06T15:47:30Z</td>\n",
       "      <td>{'id': 'techcrunch', 'name': 'TechCrunch'}</td>\n",
       "      <td>FloodFlash insurance startup raises £1.9M via ...</td>\n",
       "      <td>http://techcrunch.com/2018/08/06/floodflash-in...</td>\n",
       "      <td>https://techcrunch.com/wp-content/uploads/2017...</td>\n",
       "      <td>techcrunch</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>flood</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marcello Rossi</td>\n",
       "      <td>This story originally appeared on CityLab and ...</td>\n",
       "      <td>Finally, construction is finishing on the dela...</td>\n",
       "      <td>2018-04-05T12:00:00Z</td>\n",
       "      <td>{'id': 'wired', 'name': 'Wired'}</td>\n",
       "      <td>Will a Huge New Flood Barrier Save Venice?</td>\n",
       "      <td>https://www.wired.com/story/will-a-huge-new-fl...</td>\n",
       "      <td>https://media.wired.com/photos/5ac569774738fe0...</td>\n",
       "      <td>wired</td>\n",
       "      <td>Wired</td>\n",
       "      <td>flood</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Martha Pskowski</td>\n",
       "      <td>“SCANDAL!!,” read the Facebook post of Mexican...</td>\n",
       "      <td>As Mexico gears up for the largest election in...</td>\n",
       "      <td>2018-06-27T14:46:30Z</td>\n",
       "      <td>{'id': 'the-verge', 'name': 'The Verge'}</td>\n",
       "      <td>Mexico struggles to weed out fake news ahead o...</td>\n",
       "      <td>https://www.theverge.com/2018/6/27/17503444/me...</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/ZWrJDEF_PfP768...</td>\n",
       "      <td>the-verge</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>flood</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                            content  \\\n",
       "0    Eric Holthaus  This story originally appeared on Grist and is...   \n",
       "1    Patrick Allan  Flash floods can strike with almost no warning...   \n",
       "2     Mike Butcher  The flood underinsurance problem is arguably t...   \n",
       "3   Marcello Rossi  This story originally appeared on CityLab and ...   \n",
       "4  Martha Pskowski  “SCANDAL!!,” read the Facebook post of Mexican...   \n",
       "\n",
       "                                         description           publishedAt  \\\n",
       "0  Rapid collapse of Antarctic glaciers could flo...  2017-11-30T14:00:00Z   \n",
       "1  Flash floods can strike with almost no warning...  2018-06-11T19:00:00Z   \n",
       "2  The flood underinsurance problem is arguably t...  2018-08-06T15:47:30Z   \n",
       "3  Finally, construction is finishing on the dela...  2018-04-05T12:00:00Z   \n",
       "4  As Mexico gears up for the largest election in...  2018-06-27T14:46:30Z   \n",
       "\n",
       "                                       source  \\\n",
       "0            {'id': 'wired', 'name': 'Wired'}   \n",
       "1      {'id': None, 'name': 'Lifehacker.com'}   \n",
       "2  {'id': 'techcrunch', 'name': 'TechCrunch'}   \n",
       "3            {'id': 'wired', 'name': 'Wired'}   \n",
       "4    {'id': 'the-verge', 'name': 'The Verge'}   \n",
       "\n",
       "                                               title  \\\n",
       "0  Two Melting Antarctic Glaciers Could Decide th...   \n",
       "1  Here's Everything You Need to Know to Survive ...   \n",
       "2  FloodFlash insurance startup raises £1.9M via ...   \n",
       "3         Will a Huge New Flood Barrier Save Venice?   \n",
       "4  Mexico struggles to weed out fake news ahead o...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.wired.com/story/two-melting-glacie...   \n",
       "1  https://lifehacker.com/heres-everything-you-ne...   \n",
       "2  http://techcrunch.com/2018/08/06/floodflash-in...   \n",
       "3  https://www.wired.com/story/will-a-huge-new-fl...   \n",
       "4  https://www.theverge.com/2018/6/27/17503444/me...   \n",
       "\n",
       "                                          urlToImage   source_id  \\\n",
       "0  https://media.wired.com/photos/5a1f4e7f0cb1f52...       wired   \n",
       "1  https://i.kinja-img.com/gawker-media/image/upl...         NaN   \n",
       "2  https://techcrunch.com/wp-content/uploads/2017...  techcrunch   \n",
       "3  https://media.wired.com/photos/5ac569774738fe0...       wired   \n",
       "4  https://cdn.vox-cdn.com/thumbor/ZWrJDEF_PfP768...   the-verge   \n",
       "\n",
       "      source_name  types  yes_disaster  \n",
       "0           Wired  flood             1  \n",
       "1  Lifehacker.com  flood             1  \n",
       "2      TechCrunch  flood             1  \n",
       "3           Wired  flood             1  \n",
       "4       The Verge  flood             1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/e_flood_1556224208.220971.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>yes_disaster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This story originally appeared on Grist and is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flash floods can strike with almost no warning...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The flood underinsurance problem is arguably t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This story originally appeared on CityLab and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“SCANDAL!!,” read the Facebook post of Mexican...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  yes_disaster\n",
       "0  This story originally appeared on Grist and is...             1\n",
       "1  Flash floods can strike with almost no warning...             1\n",
       "2  The flood underinsurance problem is arguably t...             1\n",
       "3  This story originally appeared on CityLab and ...             1\n",
       "4  “SCANDAL!!,” read the Facebook post of Mexican...             1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flood = df[['content', 'yes_disaster']]\n",
    "flood.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second sets of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>description</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>types</th>\n",
       "      <th>yes_disaster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mallory Locklear</td>\n",
       "      <td>If you want to avoid the fee, you'll have to s...</td>\n",
       "      <td>Last month, MoviePass CEO Mitch Lowe announced...</td>\n",
       "      <td>2018-07-05T17:39:00Z</td>\n",
       "      <td>{'id': 'engadget', 'name': 'Engadget'}</td>\n",
       "      <td>MoviePass’ surge pricing starts today</td>\n",
       "      <td>https://www.engadget.com/2018/07/05/moviepass-...</td>\n",
       "      <td>https://o.aolcdn.com/images/dims?thumbnail=120...</td>\n",
       "      <td>engadget</td>\n",
       "      <td>Engadget</td>\n",
       "      <td>today</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jessica Conditt</td>\n",
       "      <td>The Watchlist is now bundled into the \"My Stuf...</td>\n",
       "      <td>Hulu.com has a fresh face today, following a d...</td>\n",
       "      <td>2018-09-20T19:06:00Z</td>\n",
       "      <td>{'id': 'engadget', 'name': 'Engadget'}</td>\n",
       "      <td>Hulu’s website looks different today</td>\n",
       "      <td>https://www.engadget.com/2018/09/20/hulu-redes...</td>\n",
       "      <td>https://o.aolcdn.com/images/dims?thumbnail=120...</td>\n",
       "      <td>engadget</td>\n",
       "      <td>Engadget</td>\n",
       "      <td>today</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shannon Liao</td>\n",
       "      <td>Tumblr’s ban on adult content is now in effect...</td>\n",
       "      <td>Tumblr’s ban on adult content is now in effect...</td>\n",
       "      <td>2018-12-17T15:00:04Z</td>\n",
       "      <td>{'id': 'the-verge', 'name': 'The Verge'}</td>\n",
       "      <td>Tumblr porn vanishes today</td>\n",
       "      <td>https://www.theverge.com/2018/12/17/18141106/t...</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/O7zIGrXVdra3E5...</td>\n",
       "      <td>the-verge</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>today</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Josh Ocampo</td>\n",
       "      <td>Headed to the airport this morning? You might ...</td>\n",
       "      <td>Headed to the airport this morning? You might ...</td>\n",
       "      <td>2019-04-01T14:31:00Z</td>\n",
       "      <td>{'id': None, 'name': 'Lifehacker.com'}</td>\n",
       "      <td>Check For Flight Delays Before Flying Today</td>\n",
       "      <td>https://lifehacker.com/check-for-flight-delays...</td>\n",
       "      <td>https://i.kinja-img.com/gawker-media/image/upl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lifehacker.com</td>\n",
       "      <td>today</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alicia Adamczyk</td>\n",
       "      <td>It might be the most Monday (Mondayiest?) Mond...</td>\n",
       "      <td>It might be the most “Monday” (Mondayiest?) Mo...</td>\n",
       "      <td>2018-11-26T15:00:00Z</td>\n",
       "      <td>{'id': None, 'name': 'Lifehacker.com'}</td>\n",
       "      <td>Focus on Accomplishing One Task Today</td>\n",
       "      <td>https://lifehacker.com/focus-on-accomplishing-...</td>\n",
       "      <td>https://i.kinja-img.com/gawker-media/image/upl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lifehacker.com</td>\n",
       "      <td>today</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                                            content  \\\n",
       "0  Mallory Locklear  If you want to avoid the fee, you'll have to s...   \n",
       "1   Jessica Conditt  The Watchlist is now bundled into the \"My Stuf...   \n",
       "2      Shannon Liao  Tumblr’s ban on adult content is now in effect...   \n",
       "3       Josh Ocampo  Headed to the airport this morning? You might ...   \n",
       "4   Alicia Adamczyk  It might be the most Monday (Mondayiest?) Mond...   \n",
       "\n",
       "                                         description           publishedAt  \\\n",
       "0  Last month, MoviePass CEO Mitch Lowe announced...  2018-07-05T17:39:00Z   \n",
       "1  Hulu.com has a fresh face today, following a d...  2018-09-20T19:06:00Z   \n",
       "2  Tumblr’s ban on adult content is now in effect...  2018-12-17T15:00:04Z   \n",
       "3  Headed to the airport this morning? You might ...  2019-04-01T14:31:00Z   \n",
       "4  It might be the most “Monday” (Mondayiest?) Mo...  2018-11-26T15:00:00Z   \n",
       "\n",
       "                                     source  \\\n",
       "0    {'id': 'engadget', 'name': 'Engadget'}   \n",
       "1    {'id': 'engadget', 'name': 'Engadget'}   \n",
       "2  {'id': 'the-verge', 'name': 'The Verge'}   \n",
       "3    {'id': None, 'name': 'Lifehacker.com'}   \n",
       "4    {'id': None, 'name': 'Lifehacker.com'}   \n",
       "\n",
       "                                         title  \\\n",
       "0        MoviePass’ surge pricing starts today   \n",
       "1         Hulu’s website looks different today   \n",
       "2                   Tumblr porn vanishes today   \n",
       "3  Check For Flight Delays Before Flying Today   \n",
       "4        Focus on Accomplishing One Task Today   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.engadget.com/2018/07/05/moviepass-...   \n",
       "1  https://www.engadget.com/2018/09/20/hulu-redes...   \n",
       "2  https://www.theverge.com/2018/12/17/18141106/t...   \n",
       "3  https://lifehacker.com/check-for-flight-delays...   \n",
       "4  https://lifehacker.com/focus-on-accomplishing-...   \n",
       "\n",
       "                                          urlToImage  source_id  \\\n",
       "0  https://o.aolcdn.com/images/dims?thumbnail=120...   engadget   \n",
       "1  https://o.aolcdn.com/images/dims?thumbnail=120...   engadget   \n",
       "2  https://cdn.vox-cdn.com/thumbor/O7zIGrXVdra3E5...  the-verge   \n",
       "3  https://i.kinja-img.com/gawker-media/image/upl...        NaN   \n",
       "4  https://i.kinja-img.com/gawker-media/image/upl...        NaN   \n",
       "\n",
       "      source_name  types  yes_disaster  \n",
       "0        Engadget  today             1  \n",
       "1        Engadget  today             1  \n",
       "2       The Verge  today             1  \n",
       "3  Lifehacker.com  today             1  \n",
       "4  Lifehacker.com  today             1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('../data/e_today_1556233834.9498.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>yes_disaster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If you want to avoid the fee, you'll have to s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Watchlist is now bundled into the \"My Stuf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tumblr’s ban on adult content is now in effect...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headed to the airport this morning? You might ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It might be the most Monday (Mondayiest?) Mond...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  yes_disaster\n",
       "0  If you want to avoid the fee, you'll have to s...             1\n",
       "1  The Watchlist is now bundled into the \"My Stuf...             1\n",
       "2  Tumblr’s ban on adult content is now in effect...             1\n",
       "3  Headed to the airport this morning? You might ...             1\n",
       "4  It might be the most Monday (Mondayiest?) Mond...             1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = df2[['content', 'yes_disaster']]\n",
    "today.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer processed: 213\n",
      "lemmatizer processed: 213\n"
     ]
    }
   ],
   "source": [
    "tokenizer_lemmatizer(flood, 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>yes_disaster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this story originally appeared on grist and is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flash flood can strike with almost no warning ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the flood underinsurance problem is arguably t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this story originally appeared on citylab and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scandal read the facebook post of mexican come...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  yes_disaster\n",
       "0  this story originally appeared on grist and is...             1\n",
       "1  flash flood can strike with almost no warning ...             1\n",
       "2  the flood underinsurance problem is arguably t...             1\n",
       "3  this story originally appeared on citylab and ...             1\n",
       "4  scandal read the facebook post of mexican come...             1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flood.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer processed: 235\n",
      "lemmatizer processed: 235\n"
     ]
    }
   ],
   "source": [
    "tokenizer_lemmatizer(today, 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>yes_disaster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if you want to avoid the fee you ll have to se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the watchlist is now bundled into the my stuff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tumblr s ban on adult content is now in effect...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>headed to the airport this morning you might w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it might be the most monday mondayiest monday ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  yes_disaster\n",
       "0  if you want to avoid the fee you ll have to se...             1\n",
       "1  the watchlist is now bundled into the my stuff...             1\n",
       "2  tumblr s ban on adult content is now in effect...             1\n",
       "3  headed to the airport this morning you might w...             1\n",
       "4  it might be the most monday mondayiest monday ...             1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency–Inverse Document Frequency (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/balanced_df_tokenized_lemmatized.csv')\n",
    "df['content'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['content']\n",
    "y = df['yes_disaster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models Metrics**\n",
    "\n",
    "We decided to adapt NB for for model because of the interpretability and overall performance, and then applied customized stop words for the final models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "lr  = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "nb  = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project4_function import model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the custom_stop_words_final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2490"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append the cuustomized words to the english stopwords\n",
    "import pickle\n",
    "custom_stop_words_final = pickle.load( open( \"save.p\", \"rb\" ) )\n",
    "len(custom_stop_words_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_not_use = stopwords.words('english')\n",
    "words_not_use.extend(custom_stop_words_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2669"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_not_use) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = words_not_use) \n",
    "\n",
    "X_train_tvec_sw = tvec.fit_transform(X_train)\n",
    "X_test_tvec_sw = tvec.transform(X_test)\n",
    "X_evaluate_sw = tvec.transform(flood['content'])\n",
    "X_evaluate2_sw = tvec.transform(today['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy score</th>\n",
       "      <th>cv train score</th>\n",
       "      <th>cv test score</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "      <th>train-test gap</th>\n",
       "      <th>model status</th>\n",
       "      <th>bias vs variance</th>\n",
       "      <th>fit time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>0.753963</td>\n",
       "      <td>0.770596</td>\n",
       "      <td>0.746768</td>\n",
       "      <td>0.842181</td>\n",
       "      <td>0.753963</td>\n",
       "      <td>0.088219</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.795029</td>\n",
       "      <td>0.793056</td>\n",
       "      <td>0.789287</td>\n",
       "      <td>0.852991</td>\n",
       "      <td>0.795029</td>\n",
       "      <td>0.057962</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>0.785663</td>\n",
       "      <td>0.786933</td>\n",
       "      <td>0.776668</td>\n",
       "      <td>0.822844</td>\n",
       "      <td>0.785663</td>\n",
       "      <td>0.037181</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  accuracy score  \\\n",
       "0  KNeighborsClassifier(algorithm='auto', leaf_si...        0.753963   \n",
       "0  LogisticRegression(C=1.0, class_weight=None, d...        0.795029   \n",
       "0  MultinomialNB(alpha=1.0, class_prior=None, fit...        0.785663   \n",
       "\n",
       "   cv train score  cv test score  train score  test score  train-test gap  \\\n",
       "0        0.770596       0.746768     0.842181    0.753963        0.088219   \n",
       "0        0.793056       0.789287     0.852991    0.795029        0.057962   \n",
       "0        0.786933       0.776668     0.822844    0.785663        0.037181   \n",
       "\n",
       "  model status bias vs variance  fit time  \n",
       "0      overfit    high variance  0.000000  \n",
       "0      overfit    high variance  0.000001  \n",
       "0      overfit    high variance  0.000001  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [knn, lr, nb]\n",
    "\n",
    "score_matrix = pd.DataFrame() \n",
    "\n",
    "for i in models:\n",
    "    score_matrix = score_matrix.append(model_scores(i, X_train_tvec_sw, y_train, X_test_tvec_sw , y_test))\n",
    "score_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression: Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a binary classification problems this is basically the log of the estimated probability of a feature given the positive class. It means that higher values mean more important features for the positive class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7937785251020898\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "my_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.5, 1.0, 25],\n",
    "}\n",
    "gs = GridSearchCV(lr, param_grid=my_params, cv=5)\n",
    "gs = gs.fit(X_train_tvec_sw, y_train)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#make prediction after training the model \n",
    "prediction = gs.predict(X_evaluate_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood['prediction_lr'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_prediction = gs.predict(X_evaluate2_sw )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "today['prediction_lr'] = today_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultinomialNB: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec_nb = TfidfVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = words_not_use) \n",
    "\n",
    "X_train_tvec_sw_nb = tvec_nb.fit_transform(X_train)\n",
    "X_test_tvec_sw_nb = tvec_nb.transform(X_test)\n",
    "X_evaluate_sw_nb = tvec_nb.transform(flood['content'])\n",
    "X_evaluate2_sw_nb = tvec_nb.transform(today['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "gs_nb = nb.fit(X_train_tvec_sw_nb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_nb = gs_nb.predict(X_evaluate_sw_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood['prediction_nb'] = prediction_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_nb_prediction = gs_nb.predict(X_evaluate2_sw_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "today['prediction_nb'] = today_nb_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood['title'] = df['title']\n",
    "today['title'] = df2['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>yes_disaster</th>\n",
       "      <th>prediction_lr</th>\n",
       "      <th>prediction_nb</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this story originally appeared on grist and is...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>people hospitalized after missouri tourist boa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flash flood can strike with almost no warning ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>passenger stranded after iceland s wow air col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the flood underinsurance problem is arguably t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mgm resort sue   victim of la vega shooting  s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this story originally appeared on citylab and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>santa fe high school  multiple fatality  repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scandal read the facebook post of mexican come...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ied blast in kashmir  km from pulwama terror a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  yes_disaster  \\\n",
       "0  this story originally appeared on grist and is...             1   \n",
       "1  flash flood can strike with almost no warning ...             1   \n",
       "2  the flood underinsurance problem is arguably t...             1   \n",
       "3  this story originally appeared on citylab and ...             1   \n",
       "4  scandal read the facebook post of mexican come...             1   \n",
       "\n",
       "   prediction_lr  prediction_nb  \\\n",
       "0              0              1   \n",
       "1              1              1   \n",
       "2              0              0   \n",
       "3              0              1   \n",
       "4              0              0   \n",
       "\n",
       "                                               title  \n",
       "0  people hospitalized after missouri tourist boa...  \n",
       "1  passenger stranded after iceland s wow air col...  \n",
       "2  mgm resort sue   victim of la vega shooting  s...  \n",
       "3  santa fe high school  multiple fatality  repor...  \n",
       "4  ied blast in kashmir  km from pulwama terror a...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flood.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>yes_disaster</th>\n",
       "      <th>prediction_lr</th>\n",
       "      <th>prediction_nb</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if you want to avoid the fee you ll have to se...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MoviePass’ surge pricing starts today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the watchlist is now bundled into the my stuff...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hulu’s website looks different today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tumblr s ban on adult content is now in effect...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Tumblr porn vanishes today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>headed to the airport this morning you might w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Check For Flight Delays Before Flying Today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it might be the most monday mondayiest monday ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Focus on Accomplishing One Task Today</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  yes_disaster  \\\n",
       "0  if you want to avoid the fee you ll have to se...             1   \n",
       "1  the watchlist is now bundled into the my stuff...             1   \n",
       "2  tumblr s ban on adult content is now in effect...             1   \n",
       "3  headed to the airport this morning you might w...             1   \n",
       "4  it might be the most monday mondayiest monday ...             1   \n",
       "\n",
       "   prediction_lr  prediction_nb                                        title  \n",
       "0              0              0        MoviePass’ surge pricing starts today  \n",
       "1              0              0         Hulu’s website looks different today  \n",
       "2              0              0                   Tumblr porn vanishes today  \n",
       "3              0              1  Check For Flight Delays Before Flying Today  \n",
       "4              0              0        Focus on Accomplishing One Task Today  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "today.to_csv('../data/today_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood.to_csv('../data/flood_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
