author,content,description,publishedAt,source,title,url,urlToImage,source_id,source_name,types,yes_disaster
Ron Miller,"Adobe announced a record quarter yesterday with $2.01 billion in revenue for Q42017. That represents a healthy 25 percent year over year increase for the company, but about half of that continues to come from Creative Cloud. Experience Cloud, which includes Adobe Marketing Cloud, Adobe Analytics Cloud and Adobe Advertising Cloud in many ways represents promise for even greater revenue in the future. Creative Cloud, which includes popular tools like PhotoShop and InDesign, took in $1.16 billion for the quarter as it continues to be the chief revenue cash cow for the company. That’s not to say the other parts of the company did poorly, but it has to be troubling to Adobe that Creative Cloud represents so much of its revenue with so many enterprise products in other categories. Last quarter CEO Shantanu Narayen was not happy with the Experience Cloud’s overall performance. Narayan actually called it out in the Q3 Earnings Call saying, “Despite this success with global enterprise customers, we were disappointed with our Experience Cloud bookings in Q3 but remain confident in our ability to execute against this large opportunity,” he said. He’s right. In many ways, the Experience Cloud set of products represents a much bigger potential opportunity than even Creative Cloud, as it is aimed at a much more lucrative enterprise market. While the Experience Cloud generated record revenue of its own in Q4 with $535 million reported, an 18 percent year over year increase, you can’t help but notice that it represents about half the amount of quarterly revenue the Creative Cloud division brought in. While Narayan has to be happy to see the growth after calling the division out last quarter, he has to know that there is still a substantial untapped opportunity in Experience Cloud that could be much larger than half a billion dollars. Adobe is the 11th biggest software company in the world, according to the list compiled by PWC. Yet it has the potential to be so much bigger if Experience Cloud could begin generating a similar amount of revenue that the company garners from the Creative Cloud. In spite of the record quarter, the stock price was down.60 cents this morning as of this writing. Featured Image: Adobe","Adobe announced a record quarter yesterday with $2.01 billion in revenue for Q42017. That represents a healthy 25 percent year over year increase for the company, but about half of that continues to come from Creative Cloud. Experience Cloud, which includes A…",2017-12-15T15:41:03Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","Adobe had a record quarter, but still has substantial untapped potential",https://techcrunch.com/2017/12/15/adobe-had-a-record-quarter-but-still-has-substantial-untapped-potential/,https://tctechcrunch2011.files.wordpress.com/2017/12/adobe-headquarters.jpg,techcrunch,TechCrunch,cloud,1
Patrick Lucas Austin,"Got an email address? Use a computer? Is that a smartphone in your pocket? Then you need to get yourself some cloud storage. Having an always-accessible repository of your most important photos and files makes sharing files with friends less of a hassle and grants you peace of mind. With options from every major tech company, you might find yourself drawn to one or another based on the tech you use on a daily basis and what each service offers. Each option offers both desktop apps and at least one mobile app, and all will automatically upload your photos for safe keeping. Some come bundled with office suites while others are able to function as a primitive backup solution for the lazy. Whatever your preference, here are the cloud storage services you should consider before your computer’s hard disk decides to kick the bucket. Google Drive Platform: PC, Mac, iOS, Android, Web Starting price per month: $1.99 (100GB) Storage Options: 100GB, 1TB, 10TB, 20TB, 30TB Google Drive is my personal choice for cloud storage thanks to the ecosystem in which it exists. In fairness, I also rely on Gmail, but every other aspect of Google Drive is appealing no matter who your email provider is. Every Google account comes standard with 15GB of cloud storage spread across all your Google services (think Gmail file attachments, full-resolution videos and images in Google Photos, and files stored in Google Drive). Google Drive also works across nearly every platform (Linux users can use the third-party Insync app to get Google Drive on their desktop), and offers syncing options that surpass its competition. You can use Google Drive offline to edit your Google Docs, Sheets, and Slides by installing the Google Docs offline Chrome extension or by syncing your Docs, Sheets, Slides, and Drawing files in your Google Drive settings page. Its web interface is good enough for some basic file management and sharing with other users, but if you’re dealing with lots of large files (like full-resolution images) you might notice a slowdown in your browser. On the desktop, Google’s Backup &amp; Sync app for PC or Mac can back up particular folders on your own machine, no matter where it is on your hard drive. That means you won’t have to drag and drop your folder of family photos or your repository of GIFs into your computer’s Google Drive folder to keep them synced and secure. You can simply pick and choose which folders and files to keep synced and available in the cloud. You can squeeze a few more megabytes of storage out of your plan by changing how you store certain items. Items in Google’s office suite (Docs, Sheets, Slides, Forms, Sites), along with files shared with you by someone else, do not count against your storage limit. Photos stored as “High Quality” images in Google Photos are also exempt from your storage cap, though images larger than 16MP will be resized to 16MP, and high resolution videos will be converted to 1080p videos. If you care about fidelity, store your photos and videos in “Original Quality” mode (and get ready to buy more storage). That space-saving image storage feature might make 15GB just enough storage for you to use it for free without worry. Microsoft OneDrive Platform: PC, Mac, iOS, Android, Web Starting price per month: $1.99/5GB Storage Options: 5GB, 50GB, 1TB Microsoft OneDrive is perfect for Windows 10 users, or at least people who swear by Microsoft Office. For starters, every Premium OneDrive subscription (starting at $6.99 per month) comes bundled with an Office 365 subscription and 1TB of storage, allowing for collaboration with other Office 365 users. OneDrive’s web interface is similar to the Google Drive web layout, albeit a bit cleaner. You can automatically upload photos taken on your iOS or Android device (though OneDrive will not upload iOS photos when they are optimized for iCloud). While you can sign up for a free 5GB plan or pay $1.99 per month for a 50GB storage plan, neither includes the very capable Office 365, and Google’s 15GB of free storage is triple Microsoft’s free offering. Might as well pay the $6.99 per month for 1TB of cloud storage and an Office 365 subscription. Apple iCloud Drive Platform: PC, Mac, iOS, Web Starting price per month: $0.99/50GB Storage Options: 50GB, 200GB, 2TB Apple’s iCloud Drive is, of course, the company’s cloud storage option aimed at macOS and iOS users. Much like every other cloud storage option, apps for PC and Mac are available, although Android users are out of luck (unless they want to manage their iCloud Drive storage through a web browser, I guess). Mac users, hello! Let’s argue about which side of your screen is best for your dock. (PC users,… Read more Read You’ll probably be storing mostly photos taken with your iOS device. If that’s the case, first consider Apple’s My Photo Stream, which syncs up to 1000 images taken over the last 30 days across your iCloud-connected devices, and does not count against your standard (and pitifully small) 5GB storage cap. It’s perfect for those who don’t take many photos, or use another cloud storage service. Most iOS users should sign up for the 50GB storage plan. At a buck a month, it’s worth the cost of admission to easily store the entirety of your photo library using iCloud Photo Library (which stores all photos instead of the last month’s worth) without worrying about their expiration date. Upgrading your storage lets you store even more files and photos, and if you sign up for the 200GB or 2TB plan you can share the extra storage with family members part of your Family Sharing feature, which lets you share apps, music, iCloud storage, and location with up to six people in your household. You can snag 2TB for $9.99 per month, undercutting Google Drive’s 1TB offering for the same price. On your iOS device, you can get to your stored iCloud files in the Files app, which lets you manage both your iCloud Drive files as well as files stored in other cloud storage services (like Google Drive or OneDrive), or locally on your iOS device. At WWDC 2017 Apple announced iOS 11, and with it a slew of space-saving features for smaller… Read more Read So, which one do you get? For most users, no matter their device, the platform-agnostic Google Drive is the option I recommend. It’s bundled with a suite of free productivity tools, offers the most storage at no cost, and lets you sync files stored in places besides your Google Drive folder. If you’re a fan of Apple devices of all sorts, and don’t rely on Google for anything more than its search functionality, iCloud Drive is the way to go. Windows users, whether relying on iOS or Android devices on the go, should turn to OneDrive for its seamless integration with Windows 10, along with its willingness to play nice with other competing platforms like iOS and Android, unlike Apple’s iCloud Drive.",Got an email address? Use a computer? Is that a smartphone in your pocket? Then you need to get yourself some cloud storage. Having an always-accessible repository of your most important photos and files makes sharing files with friends less of a hassle and g…,2018-02-06T18:30:00Z,"{'id': None, 'name': 'Lifehacker.com'}",Which Cloud Storage Service Should You Use?,https://lifehacker.com/which-cloud-storage-service-should-you-use-1822761662,"https://i.kinja-img.com/gawker-media/image/upload/s--XFqD5huJ--/c_fill,fl_progressive,g_center,h_450,q_80,w_800/aq86dojk2rjjxej62or6.jpg",,Lifehacker.com,cloud,1
Frederic Lardinois,"Cloud Foundry, the open source platform-as-a-service project that more than half of the Fortune 500 companies use to help them build, test and deploy their applications, launched well before Kubernetes existed. Because of this, the team ended up building Diego, its own container management service. Unsurprisingly, given the popularity of Kubernetes, which has become somewhat of the de facto standard for container orchestration, a number of companies in the Cloud Foundry ecosystem starting looking into how they could use Kubernetes to replace Diego.
The result of this is Project Eirini, which was first proposed by IBM. As the Cloud Foundry Foundation announced today, Project Eirini now passes the core functional tests the team runs to validate the software releases of its application runtime, the core Cloud Foundry service that deploys and manages applications (if that’s a bit confusing, don’t even think about the fact that there’s also a Cloud Foundry Container Runtime, which already uses Kubernetes, but which is mostly meant to give enterprise a single platform for running their own applications and pre-built containers from third-party vendors).
“That’s a pretty big milestone,” Cloud Foundry Foundation CTO Chip Childers told me. “The project team now gets to shift to a mode where they’re focused on hardening the solution and making it a bit more production-ready. But at this point, early adopters are also starting to deploy that [new] architecture.”
Childers stressed that while the project was incubated by IBM, which has been a long-time backer of overall Cloud Foundry project, Google, Pivotal and others are now also contributing and have dedicated full-time engineers working on the project. In addition, SUSE, SAP and IBM are also active in developing Eirini.
Eirini started out as an incubation project, and while few doubted that this would be a successful project, there was a bit of confusion around how Cloud Foundry would move forward now that it essentially had two container engines for running its core service. At the time, there was even some concern that the project could fork. “I pushed back at the time and said: no, this is the natural exploration process that open source communities need to go through,” Childers said. “What we’re seeing now is that with Pivotal and Google stepping in, that’s a very clear sign that this is going to be the go-forward architecture for the future of the Cloud Foundry Application Runtime.”
A few months ago, by the way, Kubernetes was still missing a few crucial pieces the Cloud Foundry ecosystem needed to make this move. Childers specifically noted that Windows support — something the project’s enterprise users really need — was still problematic and lacked some important features. In recent releases, though, the Kubernetes team fixed most of these issues and improved its Windows support, rendering those issues moot.
What does all of this mean for Diego? Childers noted that the community isn’t at a point where it’ll hold developing that tool. At some point, though, it seems likely that the community will decide that it’s time to start the transition period and make the move to Kubernetes official.
It’s worth noting that IBM today announced its own preview of Eirini in its Cloud Foundry Enterprise Environment and that the latest version of SUSE’s Cloud Foundry-based Application Platform includes a similar preview as well.
In addition, the Cloud Foundry Foundation, which is hosting its semi-annual developer conference in Philadelphia this week, also announced that it has certified its first to systems integrators, Accenture and HCL, as part of its recently launched certification program for companies that work in the Cloud Foundry ecosystem and have at least ten certified developers on their teams.","Cloud Foundry, the open source platform-as-a-service project that more than half of the Fortune 500 companies use to help them build, test and deploy their applications, launched well before Kubernetes existed. Because of this, the team ended up building Dieg…",2019-04-02T13:00:38Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Cloud Foundry ❤ Kubernetes,http://techcrunch.com/2019/04/02/cloud-foundry-kubernetes/,https://techcrunch.com/wp-content/uploads/2019/04/img_20180420_113201.jpg?w=644,techcrunch,TechCrunch,cloud,1
Devindra Hardawar,"While early attempts at game streaming have crashed and burned, like OnLive, it's still an intriguing market. Sony's PlayStation Now service is still alive and kicking on the PS4 and PC, and NVIDIA is doubling down on game streaming with GeForce Now. The latter is still just a beta test though -- even the graphics card giant hasn't figured out how to charge for high-end game streaming. Microsoft isn't saying much about what the new gaming cloud division is working on yet, but its leader, Kareem Choudhry, hints that they're exploring ways to bring content to gamers on any device.","With little fanfare, Microsoft has announced that it's launching a new gaming cloud division, a move that would set the company up to enter the world of game streaming. As The Verge reports, it's something Microsoft been building up for a while with the acqui…",2018-03-15T14:16:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Microsoft forms a new gaming cloud division,https://www.engadget.com/2018/03/15/microsoft-unveils-a-new-gaming-cloud-division/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fs.aolcdn.com%2Fhss%2Fstorage%2Fmidas%2F88f98eca05b4d7e4294c35bf092b93ff%2F206217000%2FXbox%2BOne%2BX%2Breview%2Bgallery%2B11.jpg&client=cbc79c14efcebee57402&signature=cdb7c92f47e61eafec604c1f3546e96d6eb2cb34,engadget,Engadget,cloud,1
David Murphy,"Google launched its new “Google One” cloud storage plans back in May and started rolling its paid storage customers over to the new service. Today, Google is opening up Google One for new customers. Here’s what you get and how Google’s new cloud storage offerings stack up against the competition. This post was originally published in May 2018 and was updated with new information about Google One in August 2018. Google One: Way better than older Google Drive storage plans Let’s talk storage. Here’s how Google One changes up what the company previously offered for Google Drive: 15GB: Still free. Keep going crazy. 100GB: $2/mo 200GB: $3/mo ( new! ) 1TB: $10/mo ( gone! ) 2TB: $20/mo $10/mo 10TB: $100/mo 20TB: $200/mo 30TB: $300/mo In addition to that, Google One subscribers can share their storage allotment with up to five family members—or roommates, if you want to go that route, so long as one person doesn’t mind being responsible for the bill. (And, no, the account owner won’t be able to see or access files that “family” members store on the cloud). Google One subscribers also get access to dedicated customer service from Google, which they can call up and pepper with questions about any Google service they’re struggling with. If you’re the tech resource for members of your family, spending $24 a year to give them the cheapest Google One plan—and someone else to bother with questions about their Gmail accounts—might be worth it. On top of that, Google is also dropping in special “perks” for Google One subscribers—whether you’re a $2/month customer or $300/month customer—that include hotel discounts and Google Play credits. Google representatives said that Google One customers will enjoy more more perks going forward, but didn’t have specifics to share. Google One versus everyone else Google’s tweaks drop the price of its 2TB plan to the former price of its 1TB plan—effectively doubling your storage for free, especially since the 1TB plan is now gone. Google’s new $3-for-200GB plan is actually a slightly better deal, on a dollar-per-gigabyte basis, than its $2-for-100GB plan, but its $10-for-2TB plan now has the best price-to-storage ratio of all. And here’s how Google One stacks up against the competition— Amazon, Apple, Box, Dropbox, Mega, and OneDrive / Office 365 : Free 😐Amazon: 5GB 😐Apple: 5GB 🔥Box: 10GB (250MB limit on all individual files) ❌Dropbox: 2GB 🔥Google: 15GB 🤔Mega: 50GB (1GB transfer limit for every six hours) 😐OneDrive: 5GB Winner: Google (transfer limits are annoying) Best plan &lt;$5 monthly 😐Amazon: 100GB for $1/mo 🔥Apple: 200GB for $3/mo ❌Box: DNE ❌Dropbox: DNE 🔥Google: 200GB ($3/mo) ❌Mega: DNE (200GB for 5€/mo, so your final price in USD, as of when we wrote this, is just around $6/mo) ❌OneDrive: 50GB ($2/mo) Winner: Apple or Google (tie) Best plan &lt;$10 monthly 🔥Amazon: 2TB for $10/mo 🔥Apple: 2TB for $10/mo ❌Box: 100GB for $10/mo ❌Dropbox: 1TB for $10/mo 🔥Google: 2TB for $10/mo ❌Mega: 200GB for 5€/mo (around $6/mo) 😐OneDrive/Office 365: 1TB for $7/mo Winner: Amazon, Apple, or Google (tie) Best plan &lt;$20 monthly 🔥Amazon: 4TB for $20/mo 🔥Apple: 2TB for $10/mo ❌Box: 100GB for $10/mo 😐Dropbox: 2TB for $16.50/mo 🔥Google: 2TB for $10/mo ❌Mega: 1TB 10€/mo (around $12/mo, and a 4TB plan is around $22/mo) 😐OneDrive/Office 365: 1TB for $7/mo Winner: Amazon (with Apple and Google a close second)","Google launched its new “Google One” cloud storage plans back in May and started rolling its paid storage customers over to the new service. Today, Google is opening up Google One for new customers. Here’s what you get and how Google’s new cloud storage offer…",2018-08-15T17:39:00Z,"{'id': None, 'name': 'Lifehacker.com'}","Google One Is Now Open for Everyone, But Is It a Good Deal?",https://lifehacker.com/google-one-is-now-open-for-everyone-but-is-it-a-good-d-1826049257,"https://i.kinja-img.com/gawker-media/image/upload/s--cNbMqYC8--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/u0fexuikon5zqckgtdwm.jpg",,Lifehacker.com,cloud,1
Ron Miller,"It was a stormy Monday for cloud stocks today with the general trend pointing way down. Okta stock took the biggest beating, down over 15 percent to $48.67 and even mighty Salesforce had its worst day since 2016, according to CNBC, down 8.7 percent to $121.01. Wall Street was apparently disenchanted with just about every cloud company, large and small and in between. Nobody it seemed was spared investor’s wrath. Box was down 6.93 percent to $16.66 Workday was down 7.57 percent to $124.07 Twilio was down 13.76 percent to $76.90 Amazon (which of course includes AWS) was down 5.09 percent to $1,512.29 That’s just a smattering of the cloud stocks, and it didn’t take any cherry picking to give you the idea. It was a bad day, but there really is no rhyme or reason for this drubbing. The cloud is in high growth mode. Amazon just reported cloud revenue accounted for $6.68 billion up 46 percent in its most recent earnings report. That’s a run rate of almost $25 billion just for the cloud business. Salesforce? In August, it reported $3.28 billion for the quarter. That’s 27% year-over-year and puts them on a run rate of over $13 billion. It wasn’t that long ago the company stormed through a $10 billion revenue goal and set its sights on 20. As for Okta, it’s reporting in a couple of weeks, but in its most recent report, it announced $94.6 million, which accounted for 57% year-over-year growth with subscription revenue growing 59% year-over-year. None of these sound like companies in trouble, do they? John Dinsdale, chief analyst and research director at Synergy Research, a firm that tracks cloud market share said it was a case of today’s short-term snapshot of the market simply not matching the forecasted growth of the cloud in the coming years. “Well, there is logic and then there is the stock market; and often the two have little in common,” Dinsdale told TechCrunch. “In terms of ongoing market growth and future prospects, absolutely nothing has changed. The market forecasts remain extremely healthy. Indeed, if anything our next forecast update will likely result in us nudging up our forecast growth rates a little,” he said. In general, the cloud market continues to grow, according to Synergy’s latest reports (which is inline with other market projections). “The Q3 growth rate of 45% compares with a full-year 2017 growth rate of 44% and a 2016 growth rate of 50%. Public IaaS and PaaS services account for the bulk of the market and those grew by 51% in Q3,” Synergy wrote in their October 25th report. Perhaps this is just a case of one bad day as the stock market continues its overall volatility of the last couple of months, but whatever the reasons, cloud stocks really took it on the chin today.","It was a stormy Monday for cloud stocks today with the general trend pointing way down. Okta stock took the biggest beating, down over 15 percent to $48.67 and even mighty Salesforce had its worst day since 2016, according to CNBC, down 8.7 percent to $121.01…",2018-11-19T23:05:10Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Cloud stocks take a beating,http://techcrunch.com/2018/11/19/cloud-stocks-take-a-beating/,https://techcrunch.com/wp-content/uploads/2018/11/GettyImages-57422133.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Last July, at its Cloud Next conference, Google announced the Cloud Services Platform, its first real foray into bringing its own cloud services into the enterprise data center as a managed service. Today, the Cloud Services Platform (CSP) is launching into beta.
It’s important to note that the CSP isn’t — at least for the time being — Google’s way of bringing all of its cloud-based developer services to the on-premises data center. In other words, this is a very different project from something like Microsoft’s Azure Stack. Instead, the focus is on the Google Kubernetes Engine, which allows enterprises to then run their applications in both their own data centers and on virtually any cloud platform that supports containers.As Google Cloud engineering director Chen Goldberg told me, the idea here it to help enterprises innovate and modernize. “Clearly, everybody is very excited about cloud computing, on-demand compute and managed services, but customers have recognized that the move is not that easy,” she said and noted that the vast majority of enterprises are adopting a hybrid approach. And while containers are obviously still a very new technology, she feels good about this bet on the technology because most enterprises are already adopting containers and Kubernetes — and they are doing so at exactly the same time as they are adopting cloud and especially hybrid clouds.
It’s important to note that CSP is a managed platform. Google handles all of the heavy lifting like upgrades and security patches. And for enterprises that need an easy way to install some of the most popular applications, the platform also supports Kubernetes applications from the GCP Marketplace.
As for the tech itself, Goldberg stressed that this isn’t just about Kubernetes. The service also uses Istio, for example, the increasingly popular service mesh that makes it easier for enterprises to secure and control the flow of traffic and API calls between its applications.
With today’s release, Google is also launching its new CSP Config Management tool to help users create multi-cluster policies and set up and enforce access controls, resource quotas and more. CSP also integrates with Google’s Stackdriver Monitoring service and continuous delivery platforms.
“On-prem is not easy,” Goldberg said, and given that this is the first time the company is really supporting software in a data center that is not its own, that’s probably an understatement. But Google also decided that it didn’t want to force users into a specific set of hardware specifications like Azure Stack does, for example. Instead, CSP sits on top of VMware’s vSphere server virtualization platform, which most enterprises already use in their data centers anyway. That surely simplifies things, given that this is a very well-understood platform.","Last July, at its Cloud Next conference, Google announced the Cloud Services Platform, its first real foray into bringing its own cloud services into the enterprise data center as a managed service. Today, the Cloud Services Platform (CSP) is launching into b…",2019-02-20T17:00:41Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google’s managed hybrid cloud platform is now in beta,http://techcrunch.com/2019/02/20/googles-managed-hybrid-cloud-platform-is-now-in-beta/,https://techcrunch.com/wp-content/uploads/2019/02/GettyImages-910201616.jpg?w=618,techcrunch,TechCrunch,cloud,1
Ron Miller,"As Google Cloud Next opened today in San Francisco, Accenture announced its intent to acquire Cirruseo, a French cloud consulting firm that specializes in Google Cloud intelligence services. The companies did not share the terms of the deal.
Accenture says that Cirruseos strength and deep experience in Google’s cloud-based artificial intelligence solutions should help as Accenture expands its own AI practice. Google TensorFlow and other intelligence solutions are a popular approach to AI and machine learning, and the purchase should help give Accenture a leg up in this area, especially in the French market.
The addition of Cirruseo would be a significant step forward in our growth strategy in France, bringing a strong team of Google Cloud specialists to Accenture, Olivier Girard, Accentures geographic unit managing director for France and Benelux said in a statement.
With the acquisition, should it pass French regulatory muster, the company would add a team of 100 specialists trained in Google Cloud and G Suite to the an existing team of 2600 Google specialists worldwide.
The company sees this as a way to enhance its artificial intelligence and machine learning expertise in general, while giving it a much strong market placement in France in particular and the EU in general.
As the company stated there are some hurdles before the deal becomes official. “The acquisition requires prior consultation with the relevant works councils and would be subject to customary closing conditions,” Accenture indicated in a statement. Should all that come to pass, then Cirruseo will become part of Accenture.","As Google Cloud Next opened today in San Francisco, Accenture announced its intent to acquire Cirruseo, a French cloud consulting firm that specializes in Google Cloud intelligence services. The companies did not share the terms of the deal. Accenture says th…",2019-04-09T18:50:11Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Accenture announces intent to buy French cloud consulting firm,http://techcrunch.com/2019/04/09/accenture-announces-intent-to-buy-french-cloud-consulting-firm/,https://techcrunch.com/wp-content/uploads/2019/04/GettyImages-1127512492.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google today announced the launch of four new certifications and training programs for cloud developers and engineers: Professional Cloud Developer, Professional Cloud Network Engineer (beta) and Professional Cloud Security Engineer (beta), as well as a new G Suite certification.
The G Suite certification stands out a bit because its cheaper ($75) and far less technical than its counterparts in today’s release, but as Google notes, the overall idea here to address the ‘cloud skills crisis.’ The reason for the G Suite course, Google says, is that the “key to a successful cloud transformation is developing skills throughout the organization.” To ensure this, the G Suite exam tests your knowledge in key features of Gmail, Docs, Sheets, Drive and other G Suite tools. If you’re highly technical, that may seem unnecessary, but for many, making the move from Office to G Suite is surely quite a challenge.
The other exams, like the Cloud Developer certifications, test your ability to design, build, test, manage and secure applications on the Google Cloud Platform . They tend to cost around $200 and come in the form of multiple choice exams. To prepare for them, you can study with the help of both on-demand and instructor-led courses from Coursera and other Google partners.
Google notes that IT managers have a hard time finding candidates with the right skills, so it’s trying to address this with these new certification programs and the accompanying training tools.
The new certifications join Google’s existing ones, which include the Professional Cloud Architect, Professional Data Engineer, and Associate Cloud Engineer certifications.","Google today announced the launch of four new certifications and training programs for cloud developers and engineers: Professional Cloud Developer, Professional Cloud Network Engineer (beta) and Professional Cloud Security Engineer (beta), as well as a new G…",2019-01-24T17:00:34Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google launches new certification courses for cloud developers and engineers,http://techcrunch.com/2019/01/24/google-launches-new-certification-courses-for-cloud-developers-and-engineers/,https://techcrunch.com/wp-content/uploads/2019/01/IMG_20181211_160812.jpg?w=533,techcrunch,TechCrunch,cloud,1
Ron Miller,"A couple of years ago, Dropbox shocked a lot of people when it decided to mostly drop the public cloud, and built its own datacenters. More recently, Atlassian did the opposite, closing most of its datacenters and moving to the cloud. Companies make these choices for a variety of reasons. When Atlassian CTO Sri Viswanath came on board in 2016, he made the decision to move the company’s biggest applications to AWS. In part, this is a story of technical debt — that’s the concept that over time your applications become encumbered by layers of crusty code, making it harder to update and ever harder to maintain. For Atlassian, which was founded in 2002, that bill came due in 2016 when Viswanath came to work for the company. Atlassian already knew they needed to update the code to move into the future. One of the reasons they brought Viswanath on board was to lead that charge, but the thinking was already in place even before he got there. A small team was formed back in 2015 to work out the vision and the architecture for the new cloud-based approach, but they wanted to have their first CTO in place to carry it through to fruition. Shifting to microservices He put the plan into motion, giving it the internal code name Vertigo — maybe because the thought of moving most of their software stack to the public cloud made the engineering team dizzy to even consider. The goal of the project was to rearchitect the software, starting with their biggest products Jira and Confluence, in a such a way that it would lay the foundation for the company for the next decade — no pressure or anything. Photo: WILLIAM WEST/AFP/Getty Images They spent a good part of 2016 rewriting the software and getting it set up on AWS. They concentrated on turning their 15-year old code into microservices, which in the end resulted in a smaller code base. He said the technical debt issues were very real, but they had to be careful not to reinvent the wheel, just change what needed to be changed whenever possible. “The code base was pretty large and we had to go in and do two things. We wanted to build it for multi-tenant architecture and we wanted to create microservices,” he said. “If there was a service that could be pulled out and made self-contained we did that, but we also created new services as part of the process.” Migrating customers on the fly Last year was the migration year, and it was indeed a full year-long project to migrate every last customer over to the new system. It started in January and ended in December and involved moving tens of thousands of customers. Photo: KTSDesign/Science Photo Library First of all, they automated whatever they could and they also were very deliberate in terms of the migration order, being conscious of migrations that might be more difficult. “We were thoughtful in what order to migrate. We didn’t want to do easiest first and hardest at the end. We didn’t want to do just the harder ones and not make progress. We had to blend [our approaches] to fix bugs and issues throughout the project,” he said. Viswanath stated that the overarching goal was to move the customers without a major incident. “If you talk to anyone who does migration, that’s a big thing. Everyone has scars doing migrations. We were conscious to do this pretty carefully.” Surprisingly, although it wasn’t perfect, they did manage to complete the entire exercise without a major outage, a point of which the team is justifiably proud. That doesn’t mean that it was always smooth or easy. “It sounds super easy: ‘we were thoughtful and we migrated,’ but there was warfare every day. When you migrate, you hit a wall and react. It was a daily thing for us throughout the year,” he explained. It took a total team effort involving engineering, product and support. That included having a customer support person involved in the daily scrum meetings so they could get a feel for any issues customers were having and fix them as quickly as possible. What they gained As in any cloud project, there are some general benefits to moving an application to the cloud around flexibility, agility and resource elasticity, but there was more than that when it came to this specific project. Photo: Ade Akinrujomu/Getty Images First of all it has allowed faster deployment with multiple deployments at the same time, due in large part to the copious use of microservices. That means they can add new features much faster. During the migration year, they held off on new features for the most part because they wanted to keep things as static as possible for the shift over, but with the new system in place they can move much more quickly to add new features. They get much better performance and if they hit a performance bottleneck, they can just add more resources because it’s the cloud. What’s more, they were able to have a local presence in the EU and that improves performance by having the applications closer to the end users located there. Finally, they actually found the cloud to be a more economical option, something that not every company that moves to the cloud finds. By closing the datacenters and reducing the capital costs associated with buying hardware and hiring IT personnel to maintain it, they were able to reduce costs. Managing the people parts It was a long drawn out project, and as such, they really needed to think about the human aspect of it too. They would swap people in and out to make sure the engineers stayed fresh and didn’t burn out helping with the transition. One thing that helped was the company culture in general, which Viswanath candidly describes as one with open communication and a general “no bullshit” policy. “We maintained open communication, even when things weren’t going well. People would raise their hand if they couldn’t keep up and we would get them help,” he said. He admitted that there was some anxiety within the company and for him personally implementing a project of this scale, but they knew they needed to do it for the future of the organization. “There was definitely nervousness on what if this project doesn’t go well. It seemed the obvious right direction and we had to do it. The risk was what if we screwed up in execution and we didn’t realize benefits we set out to do.” In the end, it was a lot of work, but it worked out just fine and they have the system in place for the future. “Now we are set up for the next 10 years,” he said.","A couple of years ago, Dropbox shocked a lot of people when it decided to mostly drop the public cloud, and built its own datacenters. More recently, Atlassian did the opposite, closing most of its datacenters and moving to the cloud. Companies make these cho…",2018-04-02T11:51:11Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Atlassian’s two-year cloud journey,http://techcrunch.com/2018/04/02/atlassians-two-year-cloud-journey/,https://techcrunch.com/wp-content/uploads/2018/04/gettyimages-825887190.jpg?w=537,techcrunch,TechCrunch,cloud,1
Ron Miller,"Cloud providers love to brag about their customer wins and Amazon got a big one today when it announced that Comcast had chosen AWS as its preferred cloud provider. What does that mean exactly? Presumably Comcast is going to put the majority of its cloud workloads on AWS. In a time when Comcast is fighting the cable cutting trend, being able to produce cloud services and apps that differentiate it from a pure content provider could matter. Those services include features like voice search through the Comcast remote or the Xfinity TV remote app, which lets you see what’s currently on and allows you to choose programs to record on your DVR. They have also created XFi, an app that lets customers have more control over their Comcast WiFi networks. Comcast has been using a number of AWS services like compute, storage and analytics to help drive these apps.The company hopes that by building on the relationship, it can use the agility of the cloud to continue to fight cord-cutting disruption. These days, all you need from Comcast and other cable providers is the internet access. With a streaming device like Apple TV, Roku, Amazon Fire TV or Google Chromecast, you can get a myriad of choices, usually for a lower cost than cable TV subscriptions Cord cutting is happening faster than anyone expected. According to research from eMarketer, over 22 million people dropped their cable subscriptions last year, up over 33 percent from 2016. Not all of them were Comcast customers of course, but the trend has to have the company concerned. With so many content options even Comcast, which also owns NBC Universal, faces a crunch. One way to fight that disruptive force is offering services on top of the content to differentiate itself from other offerings like Netflix and Hulu, not mention more pure cable alternatives like Hulu Live TV, Playstation Vue and YouTube TV. AWS is the number one cloud infrastructure provider by far. Comcast is one of the leading cable TV providers with 22.5 million subscriber. AT&amp;T had just over 25 million subscribers. These numbers are from last spring.",Cloud providers love to brag about their customer wins and Amazon got a big one today when it announced that Comcast had chosen AWS as its preferred cloud provider. What does that mean exactly? Presumably Comcast is going to put the majority of its cloud work…,2018-01-16T14:00:33Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Comcast partners with AWS as it struggles to fight cord-cutting disruption,http://techcrunch.com/2018/01/16/comcast-partners-with-aws-as-it-struggles-to-fight-cord-cutting-disruption/,https://tctechcrunch2011.files.wordpress.com/2018/01/img_68651.jpg,techcrunch,TechCrunch,cloud,1
David Murphy,"The subject of file backups and online storage came up the other day at a Lifehacker staff meeting, and resident door-holder Nick Douglas chimed in that his solution for backing up his laptop was easy: He never keeps any important files on it. Everything—and he means everything—lives in the cloud. On paper, leaving everything on someone else’s server sounds like the perfect setup (assuming your files are encrypted, the server is secure, etc.) And there are a number of different services you can use to manage data of all kinds. This got us at Team Lifehacker curious: What setup do you use to store your precious data in the cloud? Let us know in the comments to this post, and we’ll publish the most eye-opening configurations in a follow-up article. Just to get your mind churning, here’s how Nick’s setup works. Using the cloud for everything: The Nick Douglas method Local storage? Pffft. When Nick is busy writing—and he’s always busy writing, it seems—he throws all of his documents into Dropbox. Everything. But he’s not just filling up a single folder full of mounds of digital paperwork. He organizes his documents into a number of different folders, and configures Dropbox to only synchronize certain folders to different computers he uses. All browsers: Google and Dropbox are now collaborating on a brand-new “Dropbox add-on for Gmail,”… Read more Read Moving on to photos, Nick dumps all of his photos to Apple’s iCloud. (I can only presume he pays for extra storage, since photos and device backups sure eat up that free 5GB pretty quickly.) He also backs up all of his photos onto his Dropbox, which happens whenever he pulls up the Dropbox app on his iPhone. And because you can never be too safe, he also backs up his photos to Google Photos (the free version). When he’s at home, he backs up his desktop computer’s contents to both a local hard drive and to the cloud, via Carbonite, but he is starting to have second thoughts about that. As I understood it, he’s most concerned about keeping all of his music intact, but he just signed up for Apple Music, which helps him keep everything organized and accessible from any device he uses. That’s not too extreme, right? A while back I decided my apartment looked like a teenager lived there. Video games were front and… Read more Read David Murphy’s lazy approach to the cloud. Hi. I try not to pay for too many monthly services, because I’m a mere journalist, so here’s my piecemeal approach to using the cloud. It’s a little like Nick’s, but not quite as thorough. For documents, I just say, “The hell with it.” I can’t recall the last time I opened up a word processor on my desktop or laptop, because there’s this wonderful thing called Google Docs or, in my specific case, typing directly into a content management system. But when I’ve had jobs where Kinja hasn’t been my everything, I still tend to use Google Docs if I can. In those rare cases where I’m forced to use Pages or Office, I treat the documents as data and lump them into whatever file backup solution I have going on at the time. More on that in a bit. I also use the free version of Dropbox to shoot files back and forth across my various computers, because dragging-and-dropping is easy, and to receive things others send me. That’s about it. For photos, I admit that I use both iCloud and Google Photos. On principle, I hate that I have to pay Apple $1 a month for a 50GB chunk of iCloud storage, but the convenience of being able to pull up all of my photos across my different Apple devices is too much to ignore. And now, because I said that, Apple will never expand its free storage to anything more. Great. Google launched its new “Google One” cloud storage plans back in May and started rolling its paid… Read more Read I use Google Photos as my primary photo backup service. I guess I could use it as my primary photo program instead of paying for iCloud—and I really should look into this, I should, but I haven’t yet. If anything, paying Google a mere $2 a month for 50GB (which gets you full-resolution storage on Google Photos) makes more sense, and I love Google Photos’ web format a lot more than Apple’s terrible web UI for iCloud. So, maybe this will be the month (or year) that I finally jump ship from iCloud. Maybe. I haven’t really thought about the music situation much lately. I used to pay for Apple Music, simply because it’s a lot more convenient to use Siri with CarPlay than to fumble around with Spotify. However, I’ve been on a big podcast kick lately—hi, Acquisitions Incorporated —so I’ve let my music subscriptions lapse. I kind of want to go old-school and just transfer albums’ worth of MP3s to my iPhone. For data, I regularly send over my most critical files (C:\Users) to my NAS box, which keeps all my data safe via a RAID 5 array. I combine that with a Backblaze backup, because you can never have too many backups. I also use my NAS to hold pretty much every other file I need to keep for an extended period of time. It’s kind of like my free cloud storage device, even though I’m terrified about the prospect of two of its four hard drives failing at the same time. You never know. I suppose I could back it up as well, but then I’ll have to pay for another online service, and I just... no.","The subject of file backups and online storage came up the other day at a Lifehacker staff meeting, and resident door-holder Nick Douglas chimed in that his solution for backing up his laptop was easy: He never keeps any important files on it. Everything—and …",2018-08-16T20:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",How Do You Back Up Your Data in the Cloud?,https://lifehacker.com/how-do-you-back-up-your-data-in-the-cloud-1828396275,"https://i.kinja-img.com/gawker-media/image/upload/s--myrxqSQX--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/hrfcrihtrfvdg51cdwsm.jpg",,Lifehacker.com,cloud,1
Jon Russell,"Alibaba’s cloud computing business is to deploy its big data services package for cities in Kuala Lumpur to help Malaysia’s government with the running of its capital city and potentially other parts of the country in the future. “City Brain,” an Alibaba Cloud service that uses big data and artificial intelligence on its cloud computing infrastructure, will be put to use in the city after an agreement with local council Dewan Bandaraya Kuala Lumpur (DBKL) was announced today. City Brain was first adopted by the government of Hangzhou, Alibaba’s home city, in 2016 to help run operations more efficiently. That’s quite a nebulous scope of work, but essentially the service pulls in all kinds of data — including video feeds, social media and traffic information — which is then processed to provide information that helps to manage daily activities. That could be responding to a traffic accident, or providing the data to redesign parts of the city to reduce vehicle congestion. Last August, Alibaba Cloud signed an agreement to bring City Brain to Macau last year, but this Malaysia deployment will mark the first move outside of Greater China. Initially, the system will be put to work on traffic, with the potential to help on town planning, incident response, and other emergency services such as calculating the optimal route to a scene. Alibaba said it plans to expand the scope of its influence to cover areas that will be of interest to enterprises, startups, entrepreneurs and academic and research institutions. Alibaba has invested significantly in both its cloud business and developing artificial intelligence technologies. We wrote about the lofty ambitions of Alibaba Cloud last year, and, in October, the Alibaba Group itself pledged to spend $15 billion on a global innovation program focused on cutting-edge tech. The DAMO Academy — which stands for “discovery, adventure, momentum and outlook” — will have seven offices worldwide which will develop areas such as data intelligence, the Internet of Things, financial tech, quantum computing and human-machine interaction. Alibaba plans to collaborate closely with the world of academia. The fruits of the program are likely to be seen in services like Cloud Brain. Alibaba maintains strong links with authorities in Malaysia. It picked Kuala Lumpur as the site for a cross-border commerce initiative that it says will help develop e-commerce across Southeast Asia. Featured Image: Garry Knight / Flickr UNDER A CC BY 2.0 LICENSE (IMAGE HAS BEEN MODIFIED)","Alibaba’s cloud computing business is to deploy its big data services package for cities in Kuala Lumpur to help Malaysia’s government with the running of its capital city and potentially other parts of the country in the future. “City Brain,” an Alibaba Clou…",2018-01-29T10:29:08Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Malaysia’s capital will adopt ‘smart city’ platform from Alibaba,http://techcrunch.com/2018/01/29/malaysia-alibaba-city-brain/,https://tctechcrunch2011.files.wordpress.com/2017/05/malaysia-flag.jpg,techcrunch,TechCrunch,cloud,1
Brendan Hesse,"How many people put a sticky note over their laptop webcams because theyre worried about someone spying on them? Rumors of hackers accessing webcams and USB microphones are ever-present in todays technological world, but its even worse when the manufacturer responsible for a connected camera is the one leaking videos of you.
As detailed in a recent report by The Intercept, Amazon Ring home security cameras stored unencrypted video recordings from both outdoor and indoor cameras alike. Ring also hired people to watch these potentially sensitive videos and tag the contents of them to help aid in the machine learning capabilities of its virtual neighborhood watch network, Neighbors.
Even more frustrating is the fact that these videos could have been shared and distributed to anyone with a simple copy and paste. And those with access to Rings database only needed to know a users email address to tap into a live feed of everything their cameras saw.
According to a statement provided by Ring to The Intercept, These videos are sourced exclusively from publicly shared Ring videos from the Neighbors app (in accordance with our terms of service), and from a small fraction of Ring users who have provided their explicit written consent to allow us to access and utilize their videos for such purposes.
Still, this is a great reminder that these kinds of security surveillance camerasmeant to keep our homes securecarry serious privacy implications. 
Theres little you can do to protect against outsiders accessing your cloud-connected camera feed in these cases. Short of removing cloud-based cameras and surveillance devices from your home, theres no sure-fire way to configure them to be inaccessible by the company that controls the cloud service. You simply have to trust that those safeguarding your data are doing the right thing, much as you would with Google, Facebook, Uber, and the lot.
If unencrypted data is being collected by a company, then the company theoretically has access to whatever said data contains. Even if a company prevents its staff from viewing its customers camera feeds, theres always a potential that hackersor the governmentcan find their way in.
Pretend mics are hot and cameras are rolling
If you still want to enjoy the benefits of a cloud-connected camera to keep your home safe while youre away, its best to treat all cameras, mics, and similar devices as if anyone could be watching or listening. Chances are that at some point someone probably will bewhether or not that be with malicious or lascivious intent. Keep security cameras and other recording devices away from locations where you want total privacy, like your bedroom, and dont do anything in front of them that you wouldnt want exposed on the internet someday.
While the same could be said for what you say in front of your smart speakers, or the webcam perched atop your computer monitor, its up to you to draw the line for where youre willing to trade convenience for privacy. I wouldnt be as concerned if someone knew what I yelled at Alexa, but I probably wont set up a Nest facing the bathroom.
When in doubt, tape it up
It might sound paranoid, but if youre ever suspicious of a device, you should turn it off, unplug it, or otherwise disable it. Put tape or sticky notes over your webcams. Shut down smart devices when you arent using them (or at least turn off their microphones). Unplug USB mics when you dont need them. Disconnect from unfamiliar wifi networks. Configure user settings to share as little data as possible. Dont store sensitive data on unprotected cloud drives. The more openings you create in your wall of privacy, the more potential there is for abuse.
Pay attention to user agreements and privacy policies
Not every camera in your home is compromised, and some companies go to great lengths to ensure peace of mind for their usersincluding encrypting your data, being overly transparent about how your data is stored or used, or not collecting data in the first place.
For example, Facebookdespite being notorious for using user data to sell adswas forthcoming about what its new Alexa-powered Portal video chat devices do with your video data. Sure, theres always a potential for fine print, PR spin, and/or loopholes when promises like that are made, but in the case of Amazons Ring cameras, no such promises were provided. You might have to dig to find what a company actually is or isnt doing with your data, but its worth investigating.
Host your own security camera setup
The appeal of products like Nest cameras or Amazon Ring is their convenience, affordability, and simplicity. Its easy to set them up, you can connect them to other devices in your home without fuss, and someone else stores all your data for you. If you dont mind a little legwork, you can set up your own system and ensure that nobody outside of you has access to your video or your feeds.
Before you go about doing so, make sure your prospective setup meets legal requirements. As for the network itself, youll need to build your own cameras and security devices using either Arduino or Raspberry Pi minicomputers, or purchase cameras that can dump their recordings to a home server or an SD card. Youll also need to wire up your house and install these devices yourself, as well as set up your own hard drive or server to store video recordings. Lastly, youll want to investigate open-source software that can help you outthough youll also want to vet what youre installing, or read comments from others, to ensure that the software itself doesnt contain some unpleasant backdoor to your system.","How many people put a sticky note over their laptop webcams because they’re worried about someone spying on them? Rumors of hackers accessing webcams and USB microphones are ever-present in today’s technological world, but it’s even worse when the manufacture…",2019-01-11T21:40:00Z,"{'id': None, 'name': 'Lifehacker.com'}",Protect Your Privacy From Your Own Cloud Security Cameras,https://lifehacker.com/protect-your-privacy-from-your-own-cloud-security-camer-1831684103,"https://i.kinja-img.com/gawker-media/image/upload/s--zQ2_Wjso--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/djyatmutl9qqurxk8bud.jpg",,Lifehacker.com,cloud,1
Romain Dillet,"It’s no secret, Apple has been relying on third-party cloud companies for iCloud. And CNBC spotted an interesting tidbit in Apple’s own documents. The company now relies on Amazon S3 and Google Cloud Platform’s storage product to store iCloud data. Back in 2016, CRN reported that Apple signed with Google for cloud storage. But Apple’s document represents the first official confirmation that this deal happened. You can find the information in Apple’s iOS 11 security guide that was published in January 2018. The company mentions that user files are divided into tiny chunks and encrypted. The encryption keys and metadata information are stored on Apple’s own servers. But the encrypted files are stored on third-party services. While users have no idea that Amazon and Google are managing their iCloud data, Amazon and Google can’t do anything with those files without the encryption keys. So it seems highly unlikely that Amazon and Google are looking at your data. “The encrypted chunks of the file are stored, without any user-identifying information, using third-party storage services, such as S3 and Google Cloud Platform,” you can read in the document. In the past, Apple has mentioned Microsoft Azure in its partners. The wording of the document isn’t really clear. Apple could be using more storage services without naming them directly. In all cases, this is a great example of asymmetric competition. While Apple and Google are fighting really hard to grab market share of the smartphone market, Apple is also Google’s client. Apple also competes with Amazon and Microsoft in other areas. So Apple would need to step up its cloud hosting game to cut ties with its competitors altogether. Featured Image: Erik Von Weber/Getty Images","It’s no secret, Apple has been relying on third-party cloud companies for iCloud. And CNBC spotted an interesting tidbit in Apple’s own documents. The company now relies on Amazon S3 and Google Cloud Platform’s storage product to store iCloud data. Back in 20…",2018-02-27T16:50:01Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Apple now relies on Google Cloud Platform and Amazon S3 for iCloud data,http://techcrunch.com/2018/02/27/apple-now-relies-on-google-cloud-platform-and-amazon-s3-for-icloud-data/,https://tctechcrunch2011.files.wordpress.com/2017/04/gettyimages-200400445-001.jpg,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"The cloud isn’t right for every business, be that because of latency constraints at the edge, regulatory requirements or because it’s simply cheaper to own and operate their own data centers for their specific workloads. Given this, it’s maybe no surprise that the vast majority of enterprises today use both public and private clouds in parallel. That’s something Microsoft has long been betting on as part of its strategy for its Azure cloud, and Google, too, is now taking a number of steps in this direction. With the open-source Kubernetes project, Google launched one of the fundamental building blocks that make running and managing applications in hybrid environments easier for large enterprises. What Google hadn’t done until today, though, is launch a comprehensive solution that includes all of the necessary parts for this kind of deployment. With its new Cloud Services Platform, though, the company is now offering businesses an integrated set of cloud services that can be deployed on both the Google Cloud Platform and in on-premise environments. As Google Cloud engineering director Chen Goldberg noted in a press briefing ahead of today’s announcement, many businesses also simply want to be able to manage their own workloads on-premise but still be able to access new machine learning tools in the cloud, for example. “Today, to achieve this, use cases involve a compromise between cost, consistency, control and flexibility,” she said. “And this all negatively impacts the desired result.” Goldberg stressed that the idea behind the Cloud Services Platform is to meet businesses where they are and then allow them to modernize their stack at their own pace. But she also noted that businesses want more than just the ability to move workloads between environments. “Portability isn’t enough,” she said. “Users want consistent experiences so that they can train their team once and run anywhere — and have a single playbook for all environments.” The two services at the core of this new offering are the Kubernetes container orchestration tool and Istio, a relatively new but quickly growing tool for connecting, managing and securing microservices. Istio is about to hit its 1.0 release. We’re not simply talking about a collection of open-source tools here. The core of the Cloud Services Platform, Goldberg noted, is “custom configured and battle-tested for enterprises by Google.” In addition, it is deeply integrated with other services in the Google Cloud, including the company’s machine learning tools. GKE On-Prem Among these new custom-configured tools are a number of new offerings, which are all part of the larger platform. Maybe the most interesting of these is GKE On-Prem. GKE, the Google Kubernetes Engine, is the core Google Cloud service for managing containers in the cloud. And now Google is essentially bringing this service to the enterprise data center, too. The service includes access to all of the usual features of GKE in the cloud, including the ability to register and manage clusters and monitor them with Stackdriver, as well as identity and access management. It also includes a direct line to the GCP Marketplace, which recently launched support for Kubernetes-based applications. Using the GCP Console, enterprises can manage both their on-premise and GKE clusters without having to switch between different environments. GKE on-prem connects seamlessly to a Google Cloud Platform environment and looks and behaves exactly like the cloud version. Enterprise users also can get access to professional services and enterprise-grade support for help with managing the service. “Google Cloud is the first and only major cloud vendor to deliver managed Kubernetes on-prem,” Goldberg argued. GKE Policy Management Related to this, Google also today announced GKE Policy Management, which is meant to provide Kubernetes administrators with a single tool for managing all of their security policies across clusters. It’s agnostic as to where the Kubernetes cluster is running, but you can use it to port your existing Google Cloud identity-based policies to these clusters. This new feature will soon launch in alpha. Managed Istio The other major new service Google is launching is Managed Istio (together with Apigee API Management for Istio) to help businesses manage and secure their microservices. The open source Istio service mesh gives admins and operators the tools to manage these services and, with this new managed offering, Google is taking the core of Istio and making it available as a managed service for GKE users. With this, users get access to Istio’s service discovery mechanisms and its traffic management tools for load balancing and routing traffic to containers and VMs, as well as its tools for getting telemetry back from the workloads that run on these clusters. In addition to these three main new services, Google is also launching a couple of auxiliary tools around GKE and the serverless computing paradigm today. The first of these is the GKE serverless add-on, which makes it easy to run serverless workloads on GKE with a single-step deploy process. This, Google says, will allow developers to go from source code to container “instantaneously.” This tool is currently available as a preview and Google is making parts of this technology available under the umbrella of its new native open source components. These are the same components that make the serverless add-on possible. And to wrap it all up, Google also today mentioned a new fully managed continuous integration and delivery service, Google Cloud Build, though the details around this service remain under wraps. So there you have it. By themselves, all of those announcements may seem a bit esoteric. As a whole, though, they show how Google’s bet on Kubernetes is starting to pay off. As businesses opt for containers to deploy and run their new workloads (and maybe even bring older applications into the cloud), GKE has put Google Cloud on the map to run them in a hosted environment. Now, it makes sense for Google to extend this to its users’ data centers, too. With managed Kubernetes from large and small companies like SUSE, Platform 9, containership is starting to become a big business. It’s no surprise the company that started it all wants to get a piece of this pie, too.","The cloud isn’t right for every business, be that because of latency constraints at the edge, regulatory requirements or because it’s simply cheaper to own and operate their own data centers for their specific workloads. Given this, it’s maybe no surprise tha…",2018-07-24T16:00:01Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud goes all-in on hybrid with its new Cloud Services Platform,http://techcrunch.com/2018/07/24/google-cloud-goes-all-in-on-hybrid-with-its-new-cloud-services-platform/,https://techcrunch.com/wp-content/uploads/2018/07/GettyImages-906499460.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google Cloud is getting a managed cron service for running batch jobs. Cloud Scheduler, as the new service is called, provides all the functionality of the kind of standard command-line cron service you probably love to hate, but with the reliability and ease of use of running a managed service in the cloud. The targets for Cloud Scheduler jobs can be any HTTP/S endpoints and Google’s own Cloud Pub/Sub topics and App Engine applications. Developers can manage these jobs through a UI in the Google Cloud Console, a command-line interface and through an API. “Job schedulers like cron are a mainstay of any developers arsenal, helping run scheduled tasks and automating system maintenance,” Google product manager Vinod Ramachandran notes in today’s announcement. “But job schedulers have the same challenges as other traditional IT services: the need to manage the underlying infrastructure, operational overhead of manually restarting failed jobs and lack of visibility into a jobs status.” As Ramachandran also notes, Cloud Scheduler, which is currently in beta, guarantees the delivery of a job to the target, which ensures that important jobs are indeed started and if you’re sending the job to AppEngine or Pub/Sub, those services will also return a success code — or an error code, if things go awry. The company stresses that Cloud Scheduler also makes it easy to automate retries when things go wrong. Google is obviously not the first company to hit upon this concept. There are a few startups that also offer a similar service, and Google’s competitors like Microsoft also offer comparable tools. Google provides developers with a free quota of three (3) jobs per month. Additional jobs cost $0.10 per month.","Google Cloud is getting a managed cron service for running batch jobs. Cloud Scheduler, as the new service is called, provides all the functionality of the kind of standard command-line cron service you probably love to hate, but with the reliability and ease…",2018-11-06T17:54:24Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","Google launches Cloud Scheduler, a managed cron service",http://techcrunch.com/2018/11/06/google-launches-cloud-scheduler-a-managed-cron-service/,https://techcrunch.com/wp-content/uploads/2018/11/GettyImages-764854125.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google is hosting its annual Cloud Next conference in San Francisco this week. With 20,000 developers in attendance, Cloud Next has become the cloud-centric counterpart to Google I/O. A few years ago, when the event only had about 2,000 attendees and Google still hosted it on a rickety pier, Diane Greene had just taken over as the CEO of Google’s cloud businesses and Google had fallen a bit behind in this space, just as Amazon and Microsoft were charging forward. Since then, Google has squarely focused on bringing business users to its cloud, both to its cloud computing services and to G Suite. Ahead of this year’s Cloud Next, I sat down with Diane Greene to talk about the current state of Google Cloud and what to expect in the near future. As Greene noted, a lot of businesses first approached cloud computing as an infrastructure play — as a way to get some cost savings and access to elastic resources. “Now, it’s just becoming so much more. People realize it’s a more secure place to be, but really, I feel like in its essence it’s all about super-charging your information to make your company much more successful.” It’s the cloud, after all, where enterprises get access to globally distributed databases like Cloud Spinner and machine learning tools like AutoML (and their equivalent tools from other vendors). When she moved to Google Cloud, Greene argued, Google was missing many of the table stakes that large enterprises needed. “We didn’t have all the audit logs. We didn’t have all the fine-grained security controls. We didn’t have the peer-to-peer networking. We didn’t have all the compliance and certification,” she told me. People told her it would take Google 10 years to be ready for enterprise customers. “That’s how long it took Microsoft. And I was like, no, it’s not 10 years.” The team took that as a challenge and now, two years later, Greene argues that Google Cloud is definitely ready for the enterprise (and she’s tired of people calling it a ‘distant third’ to AWS and Azure). Today, when she thinks about her organization’s mission, she sees it as a variation on Google’s own motto. “Google’s mission is to organize the world’s information,” she said. “Google Cloud’s mission then is to supercharge our customers’ information.” When it comes to convincing large enterprises to bet on a given vendor, though, technology is one thing, but a few years ago, Google also didn’t have the sales teams in place to sell to these companies. That had to change, too, and Greene argues that the company’s new approach is working as well. And Google needed the right partners, too, which it has now found with companies like SAP, which has certified Google’s Cloud for its Hana in-memory database, and the likes of Cisco. A few months ago, Greene told CNBC she thought that people were underestimating the scale of Google’s cloud businesses. And she thinks that’s still the case today, too. “They definitely are underestimating us. And to some extent, maybe that hurt us. But we love our pipeline and all our engagements that we have going on,” she told me. Getting large businesses on board is one thing, but Greene also argued that today is probably the best time ever to be an enterprise developer. “I’ve never seen companies so aggressively pursuing the latest technology and willing to adopt this disruptive technology because they see the advantage that can give them and they see that they won’t be competitive if the people they compete with adopt it first,” Greene told me. “And because of this, I think innovation in the enterprise is happening right now, even faster than it is in consumer, which is somewhat of a reversal.” As for the companies that are choosing Google Cloud today, Greene sees three distinct categories. There are those that were born in the cloud. Think Twitter, Spotify and Snap, which are all placing significant bets on Google Cloud. Not shy to compare Google’s technology prowess to its competitors, Green noted that “they are with Google Cloud because they know that we’re the best cloud from a technology standpoint.” But these days, a lot of large companies that preceded the internet but were still pretty data-centric are also moving to the cloud. Examples there, as far as Google Cloud customers go, include Schlumberger, HSBC and Disney. And it’s those companies that Google is really going after at this year’s Next with the launch of the Google Services Platform for businesses that want or need to take a hybrid approach to their cloud adoption plans. “They see that the future is in the cloud. They see that’s where the best technology is going to be. They see that through using the technology of the cloud they can redeploy their people to be more focused on their business needs,” Greene explained. Throughout our conversation, Greene stressed that a lot of these companies are coming to Google because of its machine learning tools and its support for Kubernetes. “We’re bringing the cloud to them,” Greene said about these companies that want to go hybrid. “We are taking Kubernetes and Istio, the monitoring and securing of the container workflows and we’re making it work on-prem and within all the different clouds and supporting it across all that. And that way, you can stay in your data center and have this Kubernetes environment and then you can spill over into the cloud and there’s no lock-in.” But there’s also a third category, the old brick-and-mortar businesses like Home Depot that often don’t have any existing large centralized systems but that now have to go through their own digital transformation, too, to remain competitive. While it’s fun to talk about up-and-coming technologies like Kubernetes and containers, though, Greene noted the vast majority of users still come to Google Cloud because of its compute services and data management and analytics tools like BigQuery. Of course there’s lot of momentum behind the Google Kubernetes Engine, too, as well as the company’s machine learning tools, but enterprises are only now starting to think about these tools. But Greene also stressed that a lot of customers are looking for security, not just in the cloud computing side of Google Cloud but also when it comes to choosing the G Suite set of productivity tools. “Companies are getting hacked and Google, knock on wood, is not getting hacked,” she noted. “We are so much more secure than any company could ever contemplate.” But while that’s definitely true, Google has also faced an interesting challenge here because of its consumer businesses. Greene noted that it sometimes takes people a while to understand that what Google does with consumer data is vastly different from what it does with data that sits in Google Cloud. Google, after all, does mine a good amount of its free users’ data to serve them more relevant ads. “We’ve been keeping billions of people’s data private for almost 20 years and that’s a lot of hard work, but a cloud customer’s data is completely private to them and we do have to continually educate people about that.” So while Google got a bit of a late start in getting enterprises to adopt its Cloud, Greene now believes that it’s on the right track. “And the other thing is, we’re playing the long game,” she noted. “This thing is early. Some people estimate that only 10 percent of workloads are in the big public clouds. And if it’s not in a public cloud, it is going to be in a public cloud.”","Google is hosting its annual Cloud Next conference in San Francisco this week. With 20,000 developers in attendance, Cloud Next has become the cloud-centric counterpart to Google I/O. A few years ago, when the event only had about 2,000 attendees and Google s…",2018-07-24T13:00:10Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud CEO Diane Greene: “We’re playing the long game here”,http://techcrunch.com/2018/07/24/google-cloud-wants-enterprises-to-think-beyond-infrastructure/,https://techcrunch.com/wp-content/uploads/2018/07/GettyImages-604493160.jpg?w=601,techcrunch,TechCrunch,cloud,1
Ron Miller,"CloudHealth, a startup that enables customers to manage a multi-cloud environment, announced today it was adding support for Google Cloud Platform. With today’s addition, CloudHealth now supports AWS, Azure, VMware and Google, giving customers a fairly comprehensive view of their cloud usage. Company co-founder and CTO Joe Kinsella says the company has been seeing inbound interest for Google Cloud support dating back to 2014, but up until now there hasn’t been enough interest to warrant a startup investing the resources necessary to support another platform. He says that has changed over the last 12-18 months as they’ve seen an increase in requests and decided to take the plunge. Google Cloud cost summary page in CloudHealth. Screenshot: CloudHealth “I think a lot of the initiatives that have been driven since Diane Greene joined Google [at the end of 2015] and began really driving towards the enterprise are bearing fruit. And as a result, we’re starting to see a really substantial uptick in interest,” he said. As for why Google is gaining traction, Kinsella believes they have found ways to differentiate themselves in some key areas. “Its two biggest differentiated services are in machine learning services and the App Engine service. I also think that they have generated a lot of innovation across Infrastructure as a Service and Platform as a Service, and they built really reliable, durable, flexible, highly configurable services,” he said. Dave Bartoletti, an analyst with Forrester Research, who specializes in the public cloud says he has also seen increasing interest in Google Cloud. “Google’s developer experience (e.g., role/account management, CI/CD toolchains, and language support) now rivals AWS and Microsoft. Very strong identity and access management, security, database, and AI/ML services are drawing increasing numbers of traditional enterprise customers,” Bartoletti told TechCrunch. CloudHealth is a cloud-based subscription service. Customers sign up and enter their cloud credentials and they get an integrated view of their cloud activity in a single interface. Kinsella says their solution provides several primary benefits including visibility, governance, compliance and cost control. Cross cloud usage view in CloudHealth. Screenshot: CloudHealth The company’s primary competitor is customers trying to build a tool to monitor multi-cloud activity themselves, something that Bartoletti also sees. “Cloud cost monitoring and optimization tools help clients pay only for what they use, pay as little as possible for what they use, and develop best practices for workload sizing and automated operations to continue to save money over time, without needing to build a large cost management practice in house,” he said. The company, which has over 300 employees, is based in downtown Boston with multiple offices around the world. It was founded in 2012 and has raised over $87 million with its most recent Series D round generating $46 million from the likes of Kleiner Perkins, Scale Venture Partners, Meritech Capital Partners, Sapphire Ventures and.406 Ventures.","CloudHealth, a startup that enables customers to manage a multi-cloud environment, announced today it was adding support for Google Cloud Platform. With today’s addition, CloudHealth now supports AWS, Azure, VMware and Google, giving customers a fairly compre…",2018-07-19T18:00:10Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",CloudHealth adds support for Google Cloud amidst growing demand,http://techcrunch.com/2018/07/19/cloudhealth-adds-support-for-google-cloud-amidst-growing-demand/,https://techcrunch.com/wp-content/uploads/2018/07/GettyImages-740521139.jpg?w=600,techcrunch,TechCrunch,cloud,1
Ron Miller,"Cloud Spanner, Google’s globally distributed cloud database got an update today that includes multi-region support, meaning the database can be replicated across regions for lower latency and better performance. It also got an updated Service Level Agreement (SLA) that should please customers. The latter states Cloud Spanner databases will have 99.999% (five nines) availability, a level of downtime that translates into less than five minutes per year, according to Cloud Spanner product manager, Deepti Srivastava. “We have engineered the system to be highly available and efficient and we expect service to be able to provide that,” she said. It’s worth noting that before Google packaged Cloud Spanner as a service, it used it in-house to run products like AdWords. From Google’s perspective, if AdWords goes down, it’s not making money, so it was engineered to stay running. Today, many of its popular services are running on Cloud Spanner. “It’s been battle tested with mission-critical application that Google runs,” Srivastava explained. But the product lacked support across multiple regions, meaning you could only house a Cloud Spanner database within a single location. That changed today with the announcement of multi-region support, which means that companies can put the database closer to users. That should result in a more responsive experience with lower latency. When Google announced the Beta of Cloud Spanner earlier this year, it sounded almost magical. It is a database that gives developers the transactional consistency of a SQL database with the scalability of the the NoSQL variety. It is a rare combination and companies like Evernote and Marketo are using Cloud Spanner today. The company claims you can be up and running in 4-clicks, but in reality if you are migrating an existing database to Cloud Spanner, it could be more complex. Srivastava says it really depends on the type of system. Obviously, those companies starting with a brand new application are going to get going faster than those rearchitecting an existing database system to work on Cloud Spanner, she said. Featured Image: Getty Images","Cloud Spanner, Google’s globally distributed cloud database got an update today that includes multi-region support, meaning the database can be replicated across regions for lower latency and better performance. It also got an updated Service Level Agreement …",2017-11-14T17:05:10Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud Spanner update includes SLA that promises less than five minutes of downtime per year,https://techcrunch.com/2017/11/14/google-cloud-spanner-update-include-sla-that-promises-less-than-five-minutes-of-downtime-per-year/,https://tctechcrunch2011.files.wordpress.com/2017/11/gettyimages-498430927.jpg,techcrunch,TechCrunch,cloud,1
Catherine Shu,"In an effort to attract more government clients, Microsoft said it will integrate Azure Stack, its hybrid cloud platform, with Azure Government in the middle of this year. Azure Stack will let government agencies run Azure’s cloud computing platform on their own private servers while retaining access to Microsoft’s cloud services. This makes it easier for clients to protect sensitive data, including classified information, that need to be kept in on-premise servers, and be compliant with data regulations. Microsoft Azure competes closely for public sector customers with Amazon Web Services, which had a head start because it signed a $600 million deal in 2013 to serve the Central Intelligence Agency, which John Edwards, the CIA’s chief information officer, described last year as the “best decision we ever made” because it allows the agency to work more efficiently and cut costs associated with maintaining legacy software. Microsoft, however, is intent on catching up with Amazon and both companies have been busy adding services designed for government clients. For example, last fall Amazon announced that it now offers a “secret region” designed especially for the needs U.S. intelligence agencies. Amazon’s reveal of its AWS Secret Region came on the heels of Microsoft’s own announcement of Azure Government Secret, which also supports classified workloads. Featured Image: Bloomberg /Getty Images","In an effort to attract more government clients, Microsoft said it will integrate Azure Stack, its hybrid cloud platform, with Azure Government in the middle of this year. Azure Stack will let government agencies run Azure’s cloud computing platform on their …",2018-03-06T07:19:51Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Microsoft will integrate Azure Stack with Azure Government this year,http://techcrunch.com/2018/03/05/microsoft-will-integrate-azure-stack-with-azure-government-this-year/,https://tctechcrunch2011.files.wordpress.com/2018/03/gettyimages-452289968.jpg,techcrunch,TechCrunch,cloud,1
Gabe Gurwin,"If you’re having problems with your PlayStation 4 and need to get it fixed—or need a replacement—you don’t need to lose your game save data. Using cloud backups, you can upload all of your information beforehand, allowing you to quickly download it again and continue playing once you get your system back. The process is a little tricky, however. And unlike the rival Xbox One, it isn’t automatic. There are a few steps you have to set up first, but once you’ve gotten the hang of it, saving games to the cloud will become second nature. Here’s how to upload save data to the cloud on PlayStation 4. Subscribe to PlayStation Plus Here’s the bad news: Cloud saving on PlayStation 4 requires a PlayStation Plus subscription. There’s no getting around that. If you aren’t a member of the service, which is also needed for playing most games online, you can try it free for 14 days, after which you’ll have to start paying around $60 per year. How to Upload Your Data Manually If you’ve only been playing one or two games for an extended period of time and want to make sure your save data is backed up, you can manually upload it to the cloud. Go to your system’s settings menu and select “Application Saved Data Management,” and then select “Saved Data in System Storage” and “Upload to Online Storage.” On the next screen, you’ll see a list of your games arranged from recent to oldest. Select the game in question and on the next page hit the “Upload” button in the bottom-right. It’s as easy as that. No matter your game console, you need to add a passcode and two-factor authentication. You can do… Read more Read How to Upload Your Data Automatically Head back to the “Application Saved Data Management” screen and select “Auto-Upload” at the bottom and enable it. As long as your power settings have “Stay Connected to the Internet” enabled, your system will upload all relevant data whenever it’s turned on or in its low-power “Rest Mode.” (Make sure your system is set to maintain its network connection whenever it drops into Rest Mode, which you can set within the “Power Save” menu in your system’s Settings.) To put the system in rest mode instead of turning it off, just hold down the home button on your controller, select the power button icon, and select “Enter Rest Mode.” The light on the front should turn orange instead of white. Occasionally, the system can fail to properly upload your data in rest mode. You can check to see if this has happened by selecting the notifications icon on the home screen the next time you start up your system. If that’s the case, make sure to manually upload the data to the cloud. If you still encounter issues uploading data, you may have hit the data limit. PlayStation Plus gives you 10GB of cloud storage space, which should be plenty for most gamers, but you might have to cull some older files in order for new saves to upload. To see how much space your save games take up, select a game (as if you were manually uploading your save data). You’ll see the total available space in the bottom-right corner. If you need to delete anything to save space, go back to “Application Saved Data Management,” select “Saved Data in Online Storage” and “Delete.” The nostalgia effect is powerful. If you’re still clinging to your old-school Nintendo… Read more Read How to Download Data From the Cloud The process of downloading save data from the cloud is almost identical to uploading it. Head back to “Application Saved Data Management” and select “Saved Data in Online Storage” this time. On the next screen, select “Download to System Storage.” From here, you can pick save data for any of your games and download it to your system. If you’re on a brand new console, the process of redownloading all your data should only take a few minutes.","If you’re having problems with your PlayStation 4 and need to get it fixed—or need a replacement—you don’t need to lose your game save data. Using cloud backups, you can upload all of your information beforehand, allowing you to quickly download it again and …",2018-09-12T13:30:00Z,"{'id': None, 'name': 'Lifehacker.com'}",How to Upload PS4 Save Data to the Cloud,https://lifehacker.com/how-to-upload-ps4-save-data-to-the-cloud-1828967753,"https://i.kinja-img.com/gawker-media/image/upload/s--8uo-2zfh--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/cu95spdunfd1ecmwiiho.jpg",,Lifehacker.com,cloud,1
Shannon Liao,"Microsoft is moving Kinect to the cloud, the company’s CEO Satya Nadella announced today during the company’s annual Build keynote. “Kinect, when we first launched it in 2010, was a speech-first, gaze-first, vision-first device. It was used in gaming, and then later on it came to the PC and it was used in many applications: medical, industrial, robotics, education,” said Nadella. “We’ve been inspired by what developers have done, and since Kinect we’ve made a tremendous amount of progress when it comes to some of the foundational technologies in HoloLens and so we’re taking those advances and packaging them up as Project Kinect for Azure.” It’s big news after the depth camera and microphone accessory that originally debuted on the Xbox 360 was basically declared dead last October, when Microsoft stopped manufacturing it. Alex Kipman, a technical fellow at Microsoft, explained in a LinkedIn blog post that Project Kinect for Azure would combine the depth-sensor with Azure AI services that could help developers make devices that will be more precise “with less power consumption.” Kipman also notes that AI deep learning on depth images could lead to “cheaper-to-deploy AI algorithms” that require smaller networks to operate. Introducing Project Kinect for Azure, the most powerful sensor kit with spatial human and object understanding. #MSBuild pic.twitter.com/k1UxoGhtXs — Microsoft (@Microsoft) May 7, 2018","Microsoft is moving Kinect to the cloud, the company’s CEO Satya Nadella announced today during the company’s annual Build keynote. “Kinect, when we first launched it in 2010, was a speech-first, gaze-first, vision-first device. It was used in gaming, and the…",2018-05-07T16:48:56Z,"{'id': 'the-verge', 'name': 'The Verge'}",Microsoft is moving Kinect to the cloud,https://www.theverge.com/2018/5/7/17318690/microsoft-kinect-cloud-announced-build-2018,https://cdn.vox-cdn.com/thumbor/9Stds1ZsWiDMldjaIjcjHEg1D5E=/0x0:2040x1068/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/10793665/twarren_build2018_twarren_build2018_28.JPG,the-verge,The Verge,cloud,1
Ingrid Lunden,"More consolidation is afoot in the world of cloud-based voice services. Today, Vonage — once a VoIP pioneer that today offers cloud-based unified communications and other IP services in the business market — announced that it would acquire NewVoiceMedia, a UK startup that builds cloud-based contact center solutions, for $350 million in cash. Vonage says the price represents 3.8 times NewVoiceMedia’s projected 2019 revenue. But it isn’t a stellar exit for the startup, which has been around since 1998 and was last valued at upwards of $311 million, according to Pitchbook data. That was over two years ago, and we’d heard the valuation was actually closer to $500 million at the time. Its investors included Bessemer Venture Partners, Technology Crossover Ventures, Salesforce Ventures and more. On the other hand, the deal will help Vonage increase the services it provides, and thus the margins it makes, in a wider suite of value-added IP services — which today include office phone systems, marketing automation and an existing call center solution, as well as MPLS and other IP services. Specifically, it gives it the platform to integrate also more deeply with other software providers like Salesforce, an important part of how Vonage sells its services to would be customers. “We are thrilled to announce the acquisition of NewVoiceMedia, which represents a major step forward in the realization of our strategic vision to deliver a differentiated, fully-programmable communications solution that drives more meaningful customer interactions and better outcomes for businesses,” said Vonage CEO Alan Masarek, in a statement. “This acquisition accelerates Vonage’s growth strategy and leadership position in cloud communications, strengthens our presence with global mid-market and enterprise clients, and deepens our integrations and key go-to-market relationships with CRM providers, especially Salesforce.com.” Vonage is today a $3.2 billion company traded publicly on the NYSE, and its stock is up slightly in trading today. Vonage claims NewVoiceMedia is the largest privately-owned, pure-play, cloud contact center company globally, with some 700 customers mostly in the mid- to large-enterprise range, including Adobe, Siemens, Time Inc., FundingCircle, and Rapid7. Dennis Fois, the CEO of NewVoiceMedia, will stay on and continue to lead NewVoiceMedia business, which has 400 employees today. “Vonage and NewVoiceMedia share a powerful vision to serve businesses with cloud communications that connect employees and enable personalized conversations with their customers and prospects,” Fois said in a statement. “Together, we can help businesses create richer experiences through a state-of-the-art, global, programmable cloud communication platform.” Vonage said it would finance this deal through a combination of existing revolver capacity, cash on hand and cash on the balance sheet of NewVoiceMedia. The transaction is expected to close in the fourth quarter of 2018, subject to standard regulatory review and customary closing conditions.","More consolidation is afoot in the world of cloud-based voice services. Today, Vonage — once a VoIP pioneer that today offers cloud-based unified communications and other IP services in the business market — announced that it would acquire NewVoiceMedia, a UK…",2018-09-20T13:48:37Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Vonage acquires cloud-based contact center startup NewVoiceMedia for $350M in cash,http://techcrunch.com/2018/09/20/vonage-acquires-cloud-based-contact-center-startup-newvoicemedia-for-350m-in-cash/,https://techcrunch.com/wp-content/uploads/2015/07/shutterstock_102266374.jpg?w=600,techcrunch,TechCrunch,cloud,1
Ron Miller,"Google announced today it was going to acquire Israeli cloud migration startup, Velostrata. The companies did not share the purchase price. Velostrata helps companies migrate from on-premises datacenters to the cloud, a common requirement today as companies try to shift more workloads to the cloud. It’s not always a simple matter though to transfer those legacy applications, and that’s where Velostrata could help Google Cloud customers. As I wrote in 2014 about their debut, the startup figured out a way to decouple storage and compute and that had wide usage and appeal. “The company has a sophisticated hybrid cloud solution that decouples storage from compute resources, leaving the storage in place on-premises while running a virtual machine in the cloud,” I wrote at the time. But more than that, in a hybrid world where customer applications and data can live in the public cloud or on prem (or a combination), Velostrata gives them control to move and adapt the workloads as needed and prepare it for delivery on cloud virtual machines. “This means [customers] can easily and quickly migrate virtual machine-based workloads like large databases, enterprise applications, DevOps, and large batch processing to and from the cloud,” Eyal Manor VP of engineering at Google Cloud wrote in the blog post announcing the acquisition. This of course takes Velostrata from being a general purpose cloud migration tool to one tuned specifically for Google Cloud in the future, but one that gives Google a valuable tool in its battle to gain cloud marketshare. In the past, Google Cloud head Diane Greene has talked about the business opportunities they have seen in simply “lifting and shifting” data loads to the cloud. This acquisition gives them a key service to help customers who want to do that with the Google Cloud. Velostrata was founded in 2014. It has raised over $31 million from investors including Intel Capital and Norwest Venture partners.","Google announced today it was going to acquire Israeli cloud migration startup, Velostrata. The companies did not share the purchase price. Velostrata helps companies migrate from on-premises datacenters to the cloud, a common requirement today as companies t…",2018-05-09T18:42:18Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google to acquire cloud migration startup Velostrata,http://techcrunch.com/2018/05/09/google-to-acquire-cloud-migration-startup-velostrata/,https://techcrunch.com/wp-content/uploads/2018/05/gettyimages-121178648.jpg?w=542,techcrunch,TechCrunch,cloud,1
Ron Miller,"When Alibaba reported its earnings yesterday, the cloud data got a bit buried in other stories, but it’s worth pointing out that its cloud business grew 93 percent in the most recent quarter to $710 million. That’s down a smidgen from the gaudy triple digit growth of last report, but their market share has doubled in just two years, and they are growing fast. As John Dinsdale, principal analyst at Synergy Research, a firm that keeps a close eye on the cloud market points out, the dip in growth is all about the law of large numbers. Alibaba couldn’t sustain triple digit growth for long. “Microsoft Azure and Google Cloud Platform have recently seen similar reductions in growth rates, and if you go back far enough in time, AWS did too. The key thing is that the market for cloud infrastructure services is now very big, yet is still growing by 50% per year — and the leading players are either maintaining or growing their market share,” he said. Back in 2015, when the Chinese eCommerce giant launched a big cloud push as part of an effort to expand beyond its eCommerce roots, Alibaba Cloud’s president Simon Hu bragged to Reuters, “Our goal is to overtake Amazon in four years, whether that’s in customers, technology, or worldwide scale.” That is obviously not happening, but the company has managed to move the market share needle, doubling from just 2 percent of worldwide cloud infrastructure market share in 2016 to 4 percent today. That’s nothing to sneeze at, according to Dinsdale, but it’s also worth pointing out that most that business is in Asia, and of that, most of it is in its native China. Like all its cloud competitors, the company is concentrating on some key technologies to drive that growth including big data analytics, artificial intelligence, security and Internet-of-Things, all of which are resource intensive and help grow revenue quickly. To sustain its growth, however, Alibaba needs to begin to develop markets outside of China and Asia. Dinsdale thinks that could happen as Chinese customers expand internationally. He also recognizes the political realities that the company faces as it tries to move into western markets. “Alibaba has what it takes to seriously challenge the top four cloud providers — despite some inevitable political headwinds that it will face,” he said. While Alibaba might not reach the lofty heights of catching AWS any time soon, or probably ever, it has a good shot at IBM and Google Cloud Platform and for a company that just started taking the cloud market seriously in 2015, that’s amazing progress.","When Alibaba reported its earnings yesterday, the cloud data got a bit buried in other stories, but it’s worth pointing out that its cloud business grew 93 percent in the most recent quarter to $710 million. That’s down a smidgen from the gaudy triple digit g…",2018-08-24T16:28:29Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Alibaba continues to gain cloud momentum,http://techcrunch.com/2018/08/24/alibaba-continues-to-gain-cloud-momentum/,https://techcrunch.com/wp-content/uploads/2018/08/GettyImages-200279407-001.jpg?w=579,techcrunch,TechCrunch,cloud,1
Jon Fingas,"Instead of nice, neat outlines that represent the old concept of opening a local document, each icon has a distinctive shape that indicates its specific role. Excel has a rectangular icon to reflect spreadsheets, but you'll see a circular pie chart for PowerPoint, abstract people for Teams and similar changes for other apps. The emphasis on content over format mimics the ""collaborative nature"" of Office 365, design lead Jon Friedman said. Yes, they're purely superficial changes that don't translate to functional improvements. However, it's easy to see this as a symbolic milestone for Microsoft as a whole, not just Office. The 2013 icons came at the tail end of the Steve Ballmer era, when the company still revolved heavily around Windows and was only just ramping up its cloud services (Office 365 launched in 2011). Flash forward to 2018 and it's a different story. Conventional Office releases and Windows still exist, of course, but Microsoft is thriving in the cloud and considers Windows just one piece of a larger puzzle. There's a real chance you might never click these icons on a PC's desktop, and that speaks volumes about the tech giant's transition.","Microsoft's Office icons on Windows and the web have been conservative, to put it mildly. They've been functional things you click while you scramble to finish a business spreadsheet or school report. The company would like you to sit up and take notice this …",2018-11-29T21:03:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Microsoft's redesigned Office icons reflect its move to the cloud,https://www.engadget.com/2018/11/29/microsoft-redesigns-office-icons/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D1600%252C900%252C0%252C0%26quality%3D85%26format%3Djpg%26resize%3D1600%252C900%26image_uri%3Dhttps%253A%252F%252Fs.yimg.com%252Fos%252Fcreatr-uploaded-images%252F2018-11%252F7fffe8b0-f40d-11e8-bddf-ea0b006cfc51%26client%3Da1acac3e1b3290917d92%26signature%3D2c2db2aa083ade2af524a4e5b75f4633919c4919&client=amp-blogside-v2&signature=5e9b5b6a459b0ecaf35409a737c7511a26614c7f,engadget,Engadget,cloud,1
Ron Miller,"While it appears that overall economic activity could be slowing down, one area that continues to soar is the cloud business. Just this week, Amazon and Microsoft reported their cloud numbers as part of their overall earnings reports.
While Microsoft’s cloud growth was flat from the previous quarter, it still grew a healthy 76 percent to $9.4 billion or a $37.6 billion run rate. Meanwhile AWS, Amazon’s cloud division, grew 46 percent to $7.4 billion or a $29.6 billion run rate. That’s up from $5.11 billion from a year ago. As always, it’s important to remember that it isn’t necessarily an apples to apples comparison as each company counts what they call cloud revenue a little differently, but it gives you a sense of where this market is going.
Both businesses also face the law of large numbers in terms of growth, that is, the bigger you get, the harder it is to keep growing at a substantial rate. The two companies are doing quite well though considering how mature their offerings are.
Last year Synergy Research reported the overall cloud market worldwide grew 32 percent to $250 billion. In Synergy’s last report on cloud market share in October, it had Amazon well in the lead with around 35 percent and Microsoft around 15 percent. A Canalys report from the same time period had AWS with 32 percent and Microsoft with 17 percent, so close you could call it a tie for statistical purposes.
Alibaba, which just reported earnings was up 84 percent, but only have a small worldwide market share. IBM, which bought Red Hat for $34 billion last year, hoping to grab a bigger piece of the hybrid cloud market, reported cloud revenue was up only 12 percent for 2018 in its earnings report last week, which seem pretty paltry compared to the rest of the market. It’s worth noting that the Red Hat sale won’t close until later this year. Google will be reporting at the beginning of next week, but has not been breaking out cloud revenue recently. It will be interesting to see if that changes.
Most experts agree that we are just beginning to scratch the surface of cloud adoption and that the vast majority of workloads are still locked in private data centers around the world. That means even if there is a broader economic downturn in the future, the cloud could be somewhat insulated because companies are already in process of moving parts of their businesses to the cloud.
As these companies grow, it requires increasing numbers of data centers to deal with all this new business, and a Canalys report found that Microsoft and Amazon have been busy in this regard. Amazon currently has 60 cloud locations worldwide with another 12 under construction. Canalys reports that the company’s CapEx spending (which includes non-data center spend) reached $26 billion, up a modest 7 percent. Meanwhile Microsoft, which is chasing AWS, had much more aggressive infrastructure spending with expenditures up 64 percent to $14 billion.
You can expect that unless something drastic happens, the market pie will continue to expand, but the numbers probably won’t change dramatically as these two market leaders have hardened their market positions and it will become increasingly difficult for competitors to catch them.","While it appears that overall economic activity could be slowing down, one area that continues to soar is the cloud business. Just this week, Amazon and Microsoft reported their cloud numbers as part of their overall earnings reports. While Microsoft’s cloud …",2019-02-01T17:34:38Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",AWS and Microsoft reap most of the benefits of expanding cloud market,http://techcrunch.com/2019/02/01/aws-and-microsoft-reap-most-of-the-benefits-of-expanding-cloud-market/,https://techcrunch.com/wp-content/uploads/2019/02/GettyImages-697904566.jpg?w=600,techcrunch,TechCrunch,cloud,1
Ron Miller,"Two of the biggest trends in applications development in recent years have been the rise of serverless and containerization. Today at Google Cloud Next, the company announced a new product called Cloud Run that is designed to bring the two together. At the same time, the company also announced Cloud Run for GKE, which is specifically designed to run on the Google’s version of Kubernetes.
Oren Teich, director of product management for serverless, says these products came out of discussions with customers. As he points out, developers like the flexibility and agility they get using serverless architecture, but have been looking for more than just compute resources. They want to get access to the full stack, and to that end the company is announcing Cloud Run.
“Cloud Run is introducing a brand new product that takes Docker containers and instantly gives you a URL. This is completely unique in the industry. We’re taking care of everything from the top end of SSL provisioning and routing, all the way down to actually running the container for you. You pay only by the hundred milliseconds of what you need to use, and its end-to-end managed,” Teich explained.
As for the GKE tool, it provides the same kinds of benefits, except for developers running their containers on Google’s GKE version of Kubernetes. Keep in mind, developers could be using any version of Kubernetes their organizations happen to have chosen, so it’s not a given that they will be using Google’s flavor of Kubernetes.
“What this means is that a developer can take the exact same experience, the exact same code they’ve written — and they have G Cloud command line, the same UI and our console and they can just with one-click target the destination they want,” he said.
All of this is made possible through yet another open source project the company introduced last year called Knative. “Cloud Run is based on Knative, an open API and runtime environment that lets you run your serverless workloads anywhere you choose fully managed on Google Cloud Platform, on your GKE cluster or on your own self-managed Kubernetes cluster,” Teich and Eyal Manor, VP of engineering wrote in a blog post introducing Cloud Run.
Serverless, as you probably know by now, is a bit of a misnomer. It’s not really taking away servers, but it is eliminating the need for developers to worry about them. Instead of loading their application on a particular virtual machine,  the cloud provider, in this case, Google, provisions the exact level of resources required to run an operation. Once that’s done, these resources go away, so you only pay for what you use at any given moment.","Two of the biggest trends in applications development in recent years have been the rise of serverless and containerization. Today at Google Cloud Next, the company announced a new product called Cloud Run that is designed to bring the two together. At the sa…",2019-04-09T16:00:07Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud Run brings serverless and containers together,http://techcrunch.com/2019/04/09/google-cloud-run-brings-serverless-and-containers-together/,https://techcrunch.com/wp-content/uploads/2019/04/IMG_20190409_075753.jpg?w=533,techcrunch,TechCrunch,cloud,1
Romain Dillet,"If you cant stop dreaming about NoSQL databases, Googles Cloud Next conference is the closest thing to heaven that youll find today. At 9 AM PT, 12 PM ET, 5 PM GMT, some of the brightest minds in cloud computing are going to introduce the upcoming features of Google Cloud.
Along with Amazon Web Services and Microsoft Azure, Google is building the infrastructure of the web. Countless startups use Google Cloud as their only hosting provider. And there are now launching more and more specialized and niche services. So its going to be interesting to see what Google has in store to beat their competitors on the cloud front.
Well have a team on the ground covering all the announcements and explaining what it means.","If you can’t stop dreaming about NoSQL databases, Google’s Cloud Next conference is the closest thing to heaven that you’ll find today. At 9 AM PT, 12 PM ET, 5 PM GMT, some of the brightest minds in cloud computing are going to introduce the upcoming features…",2019-04-09T07:01:45Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Watch Google Cloud Next developer conference live right here,http://techcrunch.com/2019/04/09/google-cloud-next-live-stream-livestream-watch-its-always-cloudy-in-california/,https://techcrunch.com/wp-content/uploads/2018/07/GettyImages-908368194.jpg?w=750,techcrunch,TechCrunch,cloud,1
Ron Miller,"Densify, a Toronto company that helps customers optimize their cloud resources to control usage and spending, announced a new tool today specifically designed to optimize container usage in the cloud.
Company CEO Gerry Smith, says that as containerization proliferates, it’s getting more difficult to track and control cloud infrastructure resource usage as software development and deployment happens with increasing speed.
“The whole basis upon which people buy and use cloud and container resources has become wildly expensive because of the lack of a resource management system,” Smith said.
The Densify solution looks at the consumption and for ways to cut costs and usage. “We have analytics in the cloud, any of various common cloud services that you can connect to, and then we use machine learning to analyze the resources and your cloud and container consumption,” he said.
Densify continuously make recommendations on how to make better use of resources and to find the cheapest computing, whether that’s reserved instances, spot instances or other discounted cloud resources.
What’s more, it can help you identify whether you are providing too few resources to accommodate the number of containers you are deploying, as well as too many.
This may sound a bit like what Spotinst and Cloudyn, the company Microsoft bought a couple of years ago, do in terms of helping control costs in the cloud, but Smith says, for his company it’s more about understanding the resources than pure cost.
“We look at ourselves as a resource management platform. So what we do is characterize the applications, demands of CPU and all the other resources, and use machine learning to predict what it’s going to need at any any given minute, at any given day of a week of the year, so that we can then better predictively match the right supply,” Smith explained.
It’s providing information about each container at a highly detailed level including “whats running, what resources are being allocated, and the true utilization of an organizations Kubernetes environment at a cluster, namespace and container level,” according to the company. All of this information should help DevOps teams better understand the resources required by their container deployments.
The company has actually been around since 2006 under the name Cirba. In its early guise it helped companies manage VMware installations. In 2016, it pivoted to cloud resource management and changed the company name to Densify. It has raised around $60 million since inception, with about half of that coming after the company changed to Densify in 2016.
The company is based in Toronto, but has offices in London and Melbourne as well","Densify, a Toronto company that helps customers optimize their cloud resources to control usage and spending, announced a new tool today specifically designed to optimize container usage in the cloud. Company CEO Gerry Smith, says that as containerization pro…",2019-04-02T15:52:47Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Densify announces new tool to optimize container management in the cloud,http://techcrunch.com/2019/04/02/densify-announces-new-tool-to-optimize-container-management-in-the-cloud/,https://techcrunch.com/wp-content/uploads/2019/04/GettyImages-1030878158-1.jpg?w=600,techcrunch,TechCrunch,cloud,1
Mariella Moon,"The publication found the note on the Pokémon Let's Go Eevee &amp; Pikachu pages, as well as on the pages for Splatoon 2, Dark Souls Remastered, Dead Cells, FIFA 19 and NBA 2K19. In a statement sent to GameInformer, Nintendo confirmed that those titles really won't be able to access cloud saves. It said that while most Switch games will support the feature, some won't to ensure that players won't abuse it to regain traded items or to go back to a higher online multiplayer ranking: ""The vast majority of Nintendo Switch games will support Save Data Cloud backup. However, in certain games this feature would make it possible to, for example, regain items that had been traded to other players, or revert to a higher online multiplayer ranking that had been lost. To ensure fair play, Save Data Cloud backup may not be enabled for such games. To ensure that Save Data Cloud backups cannot be used to unfairly affect online multiplayer rankings, the feature will not be enabled in Splatoon 2."" It's unfortunate that you can't always access cloud saves when you're paying for the service, especially since the function is available for those games on the PlayStation Network and Xbox Live. While Nintendo said that ""the vast majority"" of games will support it, you may want to make it a habit to check first before assuming that you can back up your Switch game to the cloud.","There are a lot of reasons why cloud-based backup is one of Nintendo Switch Online's most awaited features. It can make sure you don't lose your progress if your game suddenly gets corrupted, if your console gets stolen or if you decide to buy a new Switch. S…",2018-09-09T02:31:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Nintendo Switch cloud saves won't be available for some games,https://www.engadget.com/2018/09/08/nintendo-switch-cloud-save-not-available/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D1600%252C1092%252C0%252C0%26quality%3D85%26format%3Djpg%26resize%3D1600%252C1092%26image_uri%3Dhttp%253A%252F%252Fo.aolcdn.com%252Fhss%252Fstorage%252Fmidas%252F3ba71026f959ee7d884b30469c545eb8%252F205865221%252Fswitch-ed.jpg%26client%3Da1acac3e1b3290917d92%26signature%3D55ecbeb1028ae0bbcd5d21cb21d5a92f76d7cf85&client=amp-blogside-v2&signature=b74a9c98a294dc6c5a4c975cc9f1fd722081024e,engadget,Engadget,cloud,1
David Murphy,"Odds are good you’re using at least one cloud service to store most of your important data, but it can be tough to stay under most services’ free limits if you have a ton of stuff. Google just announced big changes to its paid storage plans for Google Drive, rebranding them “Google One,” and making a few of them cheaper and bigger. But do these changes give you a better deal than the other major storage services you can use? Let’s explore. Google One: Way better than older Google Drive storage plans As Google upgrades all current Drive users to Google One, its older storage plans will go away at some point over the next few months. (And, no, Google isn’t rebranding the entire “Google Drive” service to “One.” There’s Google Drive, the storage and backup service, and the supplemental “Google One” subscriptions you’ll soon be able to purchase for it.) Here are the different amounts of storage you can get for Google Drive today: 15GB: Free. Go crazy. 100GB: $2/mo 1TB: $10/mo 2TB: $20/mo 10TB: $100/mo 20TB: $200/mo 30TB: $300/mo And when Google One debuts later this year— sign up for updates here —here’s how the pricing plans will change: 15GB: Still free. Keep going crazy. 100GB: $2/mo 200GB: $3/mo ( new! ) 1TB: $10/mo ( gone! ) 2TB: $20/mo $10/mo 10TB: $100/mo 20TB: $200/mo 30TB: $300/mo Google One versus everyone else Google’s tweaks drop the price of its 2TB plan to the former price of its 1TB plan—effectively doubling your storage for free, especially since the 1TB plan is now gone. Google’s new $3-for-200GB plan is actually a slightly better deal, on a dollar-per-gigabyte basis, than its $2-for-100GB plan, but its $10-for-2TB plan now has the best price-to-storage ratio of all. Now let’s take a look at how Google One stacks up against the competition. I’ll focus on the storage tiers you’re most likely to buy (and the tiers services have in common, or close to it), as I doubt most people are plunking down a hundo each month for ten terabytes. Here’s what you can get on Amazon, Apple, Box, Dropbox, Google, Mega, and OneDrive : Free 😐Amazon: 5GB 😐Apple: 5GB 🔥Box: 10GB (250MB limit on all individual files) ❌Dropbox: 2GB 🔥Google: 15GB 🤔Mega: 50GB (1GB transfer limit for every six hours) 😐OneDrive: 5GB Winner: Google (transfer limits are annoying) Best plan &lt;$5 monthly 😐Amazon: 100GB for $1/mo 🔥Apple: 200GB for $3/mo ❌Box: DNE ❌Dropbox: DNE 🔥Google: 200GB ($3/mo) ❌Mega: DNE (200GB for 5€/mo, so your final price in USD, as of when we wrote this, is just around $6/mo) ❌OneDrive: 50GB ($2/mo) Winner: Apple or Google (tie) Best plan &lt;$10 monthly 🔥Amazon: 2TB for $10/mo 🔥Apple: 2TB for $10/mo ❌Box: 100GB for $10/mo ❌Dropbox: 1TB for $10/mo 🔥Google: 2TB for $10/mo ❌Mega: 200GB for 5€/mo (around $6/mo) 😐OneDrive: 1TB for $7/mo Winner: Amazon, Apple, or Google (tie) Best plan &lt;$20 monthly 🔥Amazon: 4TB for $20/mo 🔥Apple: 2TB for $10/mo ❌Box: 100GB for $10/mo ❌Dropbox: 1TB for $10/mo 🔥Google: 2TB for $10/mo ❌Mega: 1TB 10€/mo (around $12/mo, and a 4TB plan is around $22/mo) 😐OneDrive: 1TB for $7/mo Winner: Amazon (with Apple and Google a close second) Google sweetens the deal Even though Google One is pretty competitive, storage-wise, against everybody else, ponying up for a Google One plan will net you a few extra features that might tip the scales in Google’s favor. For starters, you’ll get access to live experts—real people, not Google’s clever AIs—who can help you with any of Google’s services via chat, email, or a phone call. You’ll also be able to share your storage plan with up to five other family members, in case you all want to contribute to one of Google’s larger offerings and divide the storage pool among yourselves. Google One subscribers will also receive other benefits at some future point, which could possibly include discounts on other Google services— including hotels you find via a Google search.","Odds are good you’re using at least one cloud service to store most of your important data, but it can be tough to stay under most services’ free limits if you have a ton of stuff. Read more...",2018-05-16T16:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",Is Google One Cloud Storage a Good Deal?,https://lifehacker.com/is-google-one-cloud-storage-a-good-deal-1826049257,"https://i.kinja-img.com/gawker-media/image/upload/s--cNbMqYC8--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/u0fexuikon5zqckgtdwm.jpg",,Lifehacker.com,cloud,1
Andy Greenberg,"Photos present a practical privacy dilemma: Keep them stored on your phone, and they'll hog your storage and risk being lost forever the next time your phone falls into a toilet. Stash them in the cloud, and they're in the hands of Google, Apple, or anyone who can compel those companies to hand over your most intimate pictures. A forthcoming app called Pixek wants to offer a better option. Pixek plans to upload your camera roll, while still letting you keep your selfies and sensitive photo evidence secret. It does so by sending photos to its own servers, while end-to-end encrypting them with a key stored only on the user's phone. That means it's designed to ensure that no one other than that user can ever decrypt those pics, not even Pixek itself. And yet thanks to some semi-magical crypto tricks, Pixek still allows you to search those photos by keyword, performing image recognition on your photos before they're uploaded, and then scrambling them with a unique form of encryption that makes their contents searchable without ever exposing those contents to Pixek itself. ""My sense is that photos are this special case, where people have to use the cloud because the sentimental value is too high to risk losing them and the storage costs are too large. And they give up privacy because of it,"" says Pixek developer and Brown University cryptographer Seny Kamara, who presented an alpha version of the app at the Real World Crypto conference earlier this month. But Pixek, as he describes it, offers another alternative, a full camera-to-cloud encrypted storage system. ""You take the pictures on your phone with the app, they're encrypted on the device and backed up to our servers. The keys stay on your device, and we can't see anything."" 'I don’t see any inherent reason why Apple wouldn’t be able to deploy something like this.' Pixek developer Seny Kamara While the app is only being distributed in alpha on Android for now —with a public beta in the coming months and an iOS version to follow—Kamara says Pixek also aims to demonstrate that the form of encryption it uses is more broadly practical; that could even work for large-scale cloud platforms, even while keeping features like machine-learning-based recognition of image content and search intact. ""I don’t see any inherent reason why Apple wouldn’t be able to deploy something like this,"" Kamara says. To enable its encrypted search feature, Pixek uses so-called ""structured encryption,"" a form of searchable encryption that researchers have been refining for more than a decade but which rarely winds up in commercial software. When someone uses Pixek to take a photo, the software performs machine learning analysis on their device to recognize objects and elements of photos, then adds tags to the image for each one. It then encrypts the image along with its tags, using a unique key stored only the user's phone. Next, Pixek's server adds the encrypted, tagged photo to a cloud-based data structure with some very specific properties: Kamara describes it as a kind of ""maze."" No one, not even someone controlling the server, can map out which encrypted keywords are connected to which encrypted image. But when the user searches for a term—like ""dog"" or ""beach""—that word is encrypted with their secret key to produce a special ""token"" that unlocks encrypted components of the database structure. ""Using that token, the server can navigate a part of the maze, and unlock pointers to whatever it’s supposed to return back,"" Kamara says. In other words, the server can use that encrypted search token to find the right encrypted photo of a dog or breach. But because the server can't navigate its own data structure without those tokens, it can't read those search terms without possessing the phone's secret key. That encryption scheme may be convoluted, but Kamara says it makes Pixek immune to privacy pitfalls that have rocked other cloud photo storage services. Last fall Apple made headlines when it added keyword-based searching to iCloud photo storage, and users who typed in ""bra"" suddenly discovered that Apple could identify photos that included cleavage. Though Apple performs its image recognition locally on users' devices, not in the cloud, iPhone owners were nonetheless dismayed by the reminder that iCloud servers could ""see"" all their most revealing selfies. A few years earlier, in 2014, hackers posted hundreds of nude photos of celebrities online after using phishing attacks to breached their iCloud accounts. That kind of phishing attack would be significantly more difficult for photos stored on Pixek, Kamara says, since only the phone with the user's secret key can decrypt the images. And if the user loses their phone? They can recover their key with a series of security questions and an emailed code. (That backup measure means anyone using Pixek would be wise to link it only to an email address protected with strong two-factor authentication.) Pixek shows a potential for encrypted search that goes well beyond photo storage, says Nigel Smart, cryptographer at the Belgian University KU Leuven. The technique means that any cloud-based service could potentially encrypt its data without making it unsearchable. 'People today know what end-to-end encryption is, now. They're starting to have an expectation that their apps are end-to-end encrypted.' Seny Kamara But Smart also points to Pixek's limitations. It doesn't currently let users share photos via the cloud. Its search is relatively simple, only working when users enter a single, exact search term. And it can't do the kind of sophisticated, cloud-based machine learning that Google Photos and others do for powerful image categorization. ""The app demonstrates cool technology, but it’s not going to replace Flickr or Google,"" says Smart. Kamara believes, however, that a service like Apple's iCloud, which performs its machine learning only on photos before they're uploaded, could still use a Pixek-like system. And he says there are in fact technical measures to allow the more nuanced searches and photo sharing that Pixek lacks, and that he hopes to add them in the future. And after witnessing the adoption of end-to-end encryption in apps like Signal, WhatsApp, Facebook Messenger, and Skype, Kamara thinks users will embrace a similarly protected system for protecting their images. ""People today know what end-to-end encryption is, now. They're starting to have an expectation that their apps are end-to-end encrypted,"" Kamara says. ""At some point people will expect that their photos will be end-to-end encrypted, too.""","Pixek, an end-to-end encrypted photo app, could point to the future of searchable cloud data storage.",2018-01-22T12:00:00Z,"{'id': 'wired', 'name': 'Wired'}",Pixek App Encrypts Your Photos From Camera to Cloud,https://www.wired.com/story/pixek-app-encrypts-photos-from-camera-to-cloud/,https://media.wired.com/photos/5a6292a0e32b507022796259/191:100/pass/PixekCloudLock-FeatureArt-648594762.jpg,wired,Wired,cloud,1
Sarah Perez,"UltraViolet, an older “cloud movie locker” service, is shutting down. The service, which allowed consumers to unlock a digital copy of their DVDs and Blu-Rays, was something of a transitional step between the age of physical media and today’s streaming video landscape. Over time, it’s become less necessary for consumers, as movie marketplaces and subscription services now offer extensive libraries of movies for streaming, rental and purchase – all in digital formats.
The shutdown was first reported by Variety.
Today, UltraViolet claims to have over 30 million users, who are able to stream more than 300 million movies and shows from their cloud libraries. But arguably, “UltraViolet” never became a household name.
The service was not well-received at launch. When the Hollywood and tech execs first came up with the idea, many people at the time thought it was just another form of DRM to keep people from sharing their movies – the way that was possible with physical disks.
After a few years, however, UltraViolet loosened its grip a bit. Walmart’s Vudu began offering a way for people to selectively share their UltraViolet movies with friends back in 2014, for example..
But that may have already been too little, too late. People were increasingly more interested streaming Netflix on their Roku – not buying DVDs, converting them to digital, then loaning them out. (Besides, if you really wanted your friend to watch one of your Vudu movies, it was just easier to share your login.)
UltraViolet’s other issue was Disney. While UltraViolet was backed by all the major Hollywood studios, it didn’t have Disney on board. And Disney later decided to launch its own cloud locker, Disney Movies Anywhere. With its launch, several studios left UltraViolet for Disney’s service, Variety’s report also noted. And last year, 20th Century Fox, Universal Pictures, and Lionsgate stopped distributing new release movies on Ultraviolet.
Disney’s service – now just called Movies Anywhere and operated with studio partners including Universal, WB, Sony Pictures and 20th Century Fox – is more popular. Within one app, all the movies you purchased across retailers are centralized.
This, combined with a shift to streaming and subscription video, didn’t bode well for UltraViolet’s future.
The service will shut down on July 31, 2019.
Users are advised to link their UltraViolet accounts to at least one retailer before that date. They should not actually cancel or unlink their UltraViolet accounts before then, as they’d lose their entire movie collection, in that case.","UltraViolet, an older “cloud movie locker” service, is shutting down. The service, which allowed consumers to unlock a digital copy of their DVDs and Blu-Rays, was something of a transitional step between the age of physical media and today’s streaming video …",2019-01-31T19:42:27Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Cloud movie locker UltraViolet is finally closing,http://techcrunch.com/2019/01/31/cloud-movie-locker-ultraviolet-is-finally-closing/,https://techcrunch.com/wp-content/uploads/2019/01/ultraviolet.jpg?w=711,techcrunch,TechCrunch,cloud,1
Romain Dillet,"Cloud-hosting company Scaleway is upgrading its entry-level instances today. These instances are now all equipped with DDR4 ECC RAM, NVMe SSD storage and AMD EPYC CPUs. As INpact Hardware noted, the company is betting on AMD across the board.
The cheapest instance now costs 2.99 per month ($3.38). For that price, you get 2 CPU cores, 2GB of RAM, 20GB of storage and 100Mbps of bandwidth. Theres no direct equivalent in the previous lineup as its a new price point for the company.
Before today, you could get a server with 1GB of RAM for 1.99 per month, or 2GB of RAM for 3.99 per month, but with more storage and bandwidth too. It remains a solid deal for personal use cases, experiments and servers that dont get a lot of traffic.
The three new instances in the DEV1 category cost 7.99 per month ($9) for 3 cores, 4GB of RAM, 40GB of storage and 200Mbps of bandwidth, 15.99 ($18) for 4 cores, 8GB of RAM, 80GB of storage, 300Mbps of bandwidth, and 23.99 ($27) for 4 cores, 12GB of RAM, 120GB of storage and 400Mbps of bandwidth.
If you want more cores and more memory, you need to look at the high-performance instances. If youre out of storage, you dont necessarily need to upgrade to a more expensive instance. You can buy block storage per 50GB increments for 1 per month.
With todays update, Scaleway has now completed its lineup refresh. Older instances will be slowly phased out. Now lets see if the company plans to open new data centers around the world with these new servers.","Cloud-hosting company Scaleway is upgrading its entry-level instances today. These instances are now all equipped with DDR4 ECC RAM, NVMe SSD storage and AMD EPYC CPUs. As INpact Hardware noted, the company is betting on AMD across the board. The cheapest ins…",2019-03-28T19:26:11Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Scaleway refreshes entry-level cloud instances,http://techcrunch.com/2019/03/28/scaleway-refreshes-entry-level-cloud-instances/,https://techcrunch.com/wp-content/uploads/2017/02/gettyimages-493420504.jpg?w=616,techcrunch,TechCrunch,cloud,1
Antonio García Martínez,"Find yourself a map of the world's major shipping routes (or just click here ) and compare it to a map of the world's undersea internet cables (…and here ): They are strikingly similar, a thick band of traffic across the Atlantic between Europe and North America; a sizable swath from Asia to the US Pacific Coast; a longer haul from Europe to Asia across Africa; and a few thinner connections between the northern and southern hemispheres. Both sets of routes have their origins in a 400-year-old world trading system that dates back to the Age of Sail, yet while the movement of goods over oceans is highly regulated by centuries of law and custom, the movement of data via fiber-optic cables is very much not. Data can now be stored in whatever corner of the world best serves the interests of cost and convenience, and it can be retrieved at the literal speed of light or secreted away in the dark. This digital version of mare liberum (“free seas,” the guiding legal notion coined by 17th-century Dutch jurist Hugo Grotius) is only now being subject to real legal scrutiny, and it’s a once-in-an-era opportunity to define how we regulate the global trade of information and intellectual property. Unfortunately, bureaucrats are busy writing data localization laws that stem from the age of ink-stained vellum paper rather than our current Dropbox reality. Consider this instructive legal quiz: If US authorities have a warrant to obtain data from a US company about a US citizen, but that data happens to live abroad because of whatever cloud caching particulars, must the US company hand over the data to US authorities? Antonio García Martínez ( @antoniogm ) is an Ideas contributor for WIRED. Before turning to writing, he dropped out of a doctoral program in physics to work on Goldman Sachs’ credit trading desk, then joined the Silicon Valley startup world, where he founded his own startup (acquired by Twitter in 2011), and finally joined Facebook’s early monetization team, where he headed their targeting efforts. His 2016 memoir, Chaos Monkeys, was a New York Times best seller and NPR Best Book of the Year, and his writing has appeared in Vanity Fair, The Guardian, and The Washington Post. He splits his time between a sailboat on the SF Bay and a yurt in Washington’s San Juan Islands. The answer, incredibly, isn’t at all clear. In the US, right now, there are two completely contradictory court judgements in effect. In 2016, the Second Circuit Court of Appeals judged that Microsoft need not comply with a US warrant requesting data stored overseas (even though the parties in question are subject to US law). Then last year, in another unrelated case, a federal judge in Washington, D.C. completely contravened the earlier ruling, ordering Google to turn over user data to federal investigators, even though the data was stored overseas. The story only gets more complex as US and European governments try to carve out digital free-trade zones in the face of competing local and state laws. For example, the Trump administration, as part of its NAFTA renegotiation bid, wants total data openness among the US, Canada, and Mexico. However, such a deal would violate laws in the Canadian provinces of British Columbia and Nova Scotia that mandate local storage for any personal data. Similarly, the European Union is pitching a Digital Single Market to its members that would unify all European data, whether government or commercial, and allow its use and physical storage anywhere within the EU. However, many European countries possess detailed legislation around where public records are stored. France (of course) has gone so far as to build a local cloud and call it le cloud souverein, which French businesses are encouraged to adopt over any regional or global cloud services. Quel shitshow, and it begs the question: Can a global cloud system peacefully coexist with national sovereignty? All this data is ultimately the magnetization state of a thin patch of cobalt alloy on a hard drive, and where that hard drive lives should be immaterial, in both senses of the word. In the same way the designer of a mobile app interface doesn't much think of how the app's data will be saved and retrieved from memory—that being the job of the language compiler and operating system—the managers of our legal and economic systems shouldn't care too much where a document or piece of user data happens to physically live. To use the dismissive charge leveled by software engineers when someone reaches way down the programming language or technological stack into the murky depths where they’re not welcome nor knowledgeable, our politicians and regulators are “violating the abstraction layer” by trying to dictate where data is physically stored. For the purposes of obtaining that data through a search warrant, how that data is stored should be about as relevant as the ink color a contractual signatory used to seal their business deal. This is not to say that “information wants to be free,” nor is it the recipe for anarchy it might seem. Personal privacy concerns and intellectual property rights must remain paramount, and the leviathan of the state, with its coercive power, should still reign supreme inside its own borders. Tangible goods (and digital bits) can transit the deep, blue sea, but they need to be subject to the laws of the land when they cross into the land. (Even Facebook, which formerly countenanced no local restrictions, is now bending over backwards to comply with local German election laws or Israeli hate speech legislation.) It’s the tradeoff between free movement and national sovereignty, as always, that’s tricky. Even a suddenly-contrite Facebook will have trouble managing the data laws of almost two hundred nation-states. Instead of this ugly patchwork of local, national, and regional laws, regulators need to come together to create new trade agreements (and strengthen existing ones) that treat data like so much oil or grain or any other tradable good. In facilitating and safeguarding the lawful exchange of data between independent countries, whether in the name of commerce or the rule of law, these digital trade agreements would help lay the groundwork for the global, digital infrastructure that our future selves deserve. Before long, talking about data somehow possessing a physical location, out of reach to some and too easily hidden by others, will become one of those quaint throwbacks we’ll completely forget. Data Dilemmas Photograph by WIRED/Getty Images",WIRED’s new columnist Antonio García Martínez on the battle between global cloud services and national borders.,2018-02-01T14:19:33Z,"{'id': 'wired', 'name': 'Wired'}",The Era of Data Without Borders Is Under Threat,https://www.wired.com/story/overseas-data-regulation/,https://media.wired.com/photos/5a727bacedc9bb2cddd1500a/191:100/pass/Shipping.jpg,wired,Wired,cloud,1
Swapna Krishna,"Today, Google announced that it will be adding three undersea cables, as well as five new Cloud Platform regions, to its infrastructure in 2018. The Netherlands and Montreal regions will open in Q1 2018, while Los Angeles, Hong King and Finland will follow. The three cables will connect Chile to Los Angeles, the U.S. to Denmark and Ireland and Hong Kong to Guam, and are called, respectively, Curie, Havfrue and HK-G.","Today, Google announced that it will be adding three undersea cables, as well as five new Cloud Platform regions, to its infrastructure in 2018. The Netherlands and Montreal regions will open in Q1 2018, while Los Angeles, Hong King and Finland will follow. T…",2018-01-16T16:20:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Google’s cloud is spreading through new undersea cables,https://www.engadget.com/2018/01/16/google-adding-undersea-cables-cloud-regions/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D6565%252C4377%252C0%252C0%26quality%3D85%26format%3Djpg%26resize%3D1600%252C1067%26image_uri%3Dhttp%253A%252F%252Fo.aolcdn.com%252Fhss%252Fstorage%252Fmidas%252F4b9a34dccc1f2e0c939716e15ed51df6%252F205547752%252FRTS1AT63.jpeg%26client%3Da1acac3e1b3290917d92%26signature%3D3d42a3c7154a962bd01331bea95ef7fd262841c1&client=cbc79c14efcebee57402&signature=393ff4cc63e787dc1c15d62d56228cf535291267,engadget,Engadget,cloud,1
Jon Fingas,"There may be some rough spots. AT&amp;T noted that the new app may take longer to load on Roku Express sticks and certain current-gen TCL TVs, and the issue is significant enough that it's planning to provide these users with a ""special offer"" in the near future. The load times should improve with subsequent updates, so don't despair if you're using one of those devices.","It didn't take long for AT&T to fulfill its promise of spreading DirecTV Now's cloud DVR to more devices. The TV provider has launched its reworked app on Roku-based players and smart TVs, including beta access to the cloud DVR feature as well as access to lo…",2018-06-05T00:49:00Z,"{'id': 'engadget', 'name': 'Engadget'}",DirecTV Now's cloud DVR comes to Roku devices,https://www.engadget.com/2018/06/04/directv-now-cloud-dvr-comes-to-roku-devices/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fs.aolcdn.com%2Fhss%2Fstorage%2Fmidas%2F48357bcb39ea7aaba843b7b453496b1d%2F206426691%2Fdirectv-now-roku-player-cloud-dvr.jpg&client=cbc79c14efcebee57402&signature=5d31cd787b32ba5cad1030f009b1e5934b111fda,engadget,Engadget,cloud,1
Nick Summers,"Plus members will get 10 hours of Network Personal Video Recorder (nPVR) storage as standard. For an extra £5 per month, you can boost that allowance to 150 hours. Unfortunately, it only works with paid TV channels — so anything you would normally find on Freeview, like Dave and BBC Four, will be off limits. TVPlayer says ""additional channels will be added in the coming weeks,"" however, followed by temporary downloads ""in the coming months."" TVPlayer is available on a bunch of different platforms, including iOS, Android, Roku and Apple TV. Cloud-based recordings could have some utility, then, if you travel a lot and want to stream on the go. Ultimately, though, TVPlayer still feels like a niche proposition. Streaming is hugely popular, but the industry has moved toward on-demand libraries, rather than linear broadcast channels. TV-style recordings are a useful perk — especially for shows like Match of the Day, which take an age to come onto BBC iPlayer — but you have to carefully manage your storage and remember to press the big red button. For a similar price, you could get a Now TV Entertainment Pass, which offers similar live streaming (without the free TV channels, admittedly) and a fairly extensive VOD catalogue.","If you've forgotten about TVPlayer, don't worry — so had we. The company, which offers a live TV streaming service in the UK, piqued our curiosity in 2015 with TVPlayer Plus. The £5 per month package gave subscribers access to 25 premium channels including Na…",2017-12-15T14:10:00Z,"{'id': 'engadget', 'name': 'Engadget'}",TVPlayer subscription lets you record live TV to the cloud,https://www.engadget.com/2017/12/15/tvplayer-plus-cloud-live-tv-recordings/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fs.aolcdn.com%2Fhss%2Fstorage%2Fmidas%2F9d129b0d7e4ce44b5245344e7ee7791%2F205956230%2Fgenius1.jpg&client=cbc79c14efcebee57402&signature=c00afd3603a750417266b9560a54c2475a33c48f,engadget,Engadget,cloud,1
Frederic Lardinois,"Atlassian previewed the next generation of its hosted Jira Software project tracking tool earlier this year. Today, it’s available to all Jira users. To build the new Jira, Atlassian redesigned both the back-end stack and rethought the user experience from the ground up. That’s not an easy change, given how important Jira has become for virtually every company that develops software — and given that it is Atlassian’s flagship product. And with this launch, Atlassian is now focusing on its hosted version of Jira (which is hosted on AWS) and prioritizing that over the self-hosted server version. So the new version of Jira that’s launching to all users today doesn’t just have a new, cleaner look, but more importantly, new functionality that allows for a more flexible workflow that’s less dependent on admins and gives more autonomy to teams (assuming the admins don’t turn those features off). Because changes to such a popular tool are always going to upset at least some users, it’s worth noting at the outset that the old classic view isn’t going away. “Its important to note that the next-gen experience will not replace our classic experience, which millions of users are happily using,” Jake Brereton, head of marketing for Jira Software Cloud, told me. “The next-gen experience and the associated project type will be available in addition to the classic projects that users have always had access to. We have no plans to remove or sunset any of the classic functionality in Jira Cloud.” The core tenet of the redesign is that software development in 2018 is very different from the way developers worked in 2002, when Jira first launched. Interestingly enough, the acquisition of Trello also helped guide the overall design of the new Jira. “One of the key things that guided our strategy is really bringing the simplicity of Trello and the power of Jira together,” Sean Regan, Atlassian’s head of growth for Software Teams, told me. “One of the reasons for that is that modern software development teams aren’t just developers down the hall taking requirements. In the best companies, they’re embedded with the business, where you have analysts, marketing, designers, product developers, product managers — all working together as a squad or a triad. So JIRA, it has to be simple enough for those teams to function but it has to be powerful enough to run a complex software development process.” Unsurprisingly, the influence of Trello is most apparent in the Jira boards, where you can now drag and drop cards, add new columns with a few clicks and easily filter cards based on your current needs (without having to learn Jira’s powerful but arcane query language). Gone are the days where you had to dig into the configuration to make even the simplest of changes to a board. As Regan noted, when Jira was first built, it was built with a single team in mind. Today, there’s a mix of teams from different departments that use it. So while a singular permissions model for all of Jira worked for one team, it doesn’t make sense anymore when the whole company uses the product. In the new Jira then, the permissions model is project-based. “So if we wanted to start a team right now and build a product, we could design our board, customize our own issues, build our own workflows — and we could do it without having to find the IT guy down the hall,” he noted. One feature the team seems to be especially proud of is roadmaps. That’s a new feature in Jira that makes it easier for teams to see the big picture. Like with boards, it’s easy enough to change the roadmap by just dragging the different larger chunks of work (or “epics,” in Agile parlance) to a new date. “It’ s a really simple roadmap,” Brereton explained. “ It’s that way by design. But the problem we’re really trying to solve here is, is to bring in any stakeholder in the business and give them one view where they can come in at any time and know that what they’re looking at is up to date. Because it’s tied to your real work, you know that what we’re looking at is up to date, which seems like a small thing, but it’s a huge thing in terms of changing the way these teams work for the positive. “ The Atlassian team also redesigned what’s maybe the most-viewed page of the service: the Jira issue. Now, issues can have attachments of any file type, for example, making it easier to work with screenshots or files from designers. Jira now also features a number of new APIs for integrations with Bitbucket and GitHub (which launched earlier this month), as well as InVision, Slack, Gmail and Facebook for Work. With this update, Atlassian is also increasing the user limit to 5,000 seats, and Jira now features compliance with three different ISO certifications and SOC 2 Type II.","Atlassian previewed the next generation of its hosted Jira Software project tracking tool earlier this year. Today, it’s available to all Jira users. To build the new Jira, Atlassian redesigned both the back-end stack and rethought the user experience from th…",2018-10-18T15:00:03Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Atlassian launches the new Jira Software Cloud,http://techcrunch.com/2018/10/18/atlassian-launches-the-new-jira-software-cloud/,https://techcrunch.com/wp-content/uploads/2018/10/Life_Photo_2.jpg?w=660,techcrunch,TechCrunch,cloud,1
Mariella Moon,"Only a small number of people use the Watch Later bookmarking feature, as well, so it had to go. Cloud Sync seems to be the most barely used among the three, though, because Plex is even jokingly(?) asking people to DM the company of a photo of their Cloud Sync library and today's newspaper as proof that they're using it. As TechCrunch explains, the feature allowed you to sync your content from your local library to a cloud storage provider, so you can access it whenever the Plex Media Server isn't available. Obviously, users didn't find it that useful. ""The Plex ecosystem is quite large, and over the years, we've sometimes added things that might have made sense at the time, but didn't age well,"" the company explained in its post. It added that it didn't approach the process lightly and that it looked at usage numbers and the costs it would take to maintain the features in a way that will keep the people using them satisfied. In other words, it just wasn't worth it to keep them running any longer.","Plex is clearly doing some major spring cleaning, because it has just killed a handful more barely used features after shutting down its personal cloud streaming service in early September. The company has announced that it's sunsetting plugins, Cloud Sync an…",2018-09-27T05:28:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Plex is sunsetting Cloud Sync and other features,https://www.engadget.com/2018/09/27/plex-sunsetting-cloud-sync/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fs.aolcdn.com%2Fhss%2Fstorage%2Fmidas%2Fca04f7ecf0edb05b2dbdc237b956279c%2F206694734%2Fplex.jpeg&client=amp-blogside-v2&signature=8148a36e5e398eee07ac7ca480971fc9a2d092f5,engadget,Engadget,cloud,1
Florence Ion,"Googles cloud-based Chrome OS has morphed into an impressive computing platform now that its compatible with Android apps and, more recently, Linux apps. But what about Windows apps, you might wonder? What happened to all the rumors of Windows compatibility on the horizon? Theyre still floating around, but you dont have to wait for them to become true to start running Windows apps on your Chromebook. All you need is the Crossover app and a little time for experimentation. Crossover is a free app (for now) that runs Windows programs inside a virtual machine. Its easy to use, and though theres a full-fledged version readily available for Linux thats also compatible with Chrome OS, the Android version of Crossover is much more user-friendly. If youre curious about the novelty of running Windows apps on your Chromebook, or perhaps youre desperately trying to run an oft-used favorite app, keep reading. What is Crossover? Crossover is a beta app available in the Google Play Store. Its free, even though theres a giant Trial Period banner emblazoned on the apps main page. ( Codeweavers says youll be able to pay for the full Android version soon.) The Android/Chrome OS version of the app is still very experimental, but it utilizes the same underlying technology as the macOS and Linux versions. Since Crossover has had some time to incubate on other platforms, its Android version is pretty robust. Each Windows app launch via Crossover can run on a different version of Windows if needed, and you can even add on essential components alike font libraries. One major caveat to keep in mind is that Crossover can only run 32-bit applications due to limitations in Chrome OSs Android environment. (Crossover runs 64-bit applications on macOS and Linux.) How to install Crossover Before you can install Crossover from the Google Play Store, youll want to make sure your Chromebook is capable of handling Android apps and is fast enough to run a virtual machine. Codeweavers suggests using the app on an Intel-based Chromebook. To ensure your device can run Android apps downloaded from the Play Store, head to your Chromebooks settings menu. If all is in order, youll see a Google Play option with an external link to Chrome OSs Android settings. If youre on an older version of Chrome OS, you may have to enable developer mode to access Android apps. PCs are great, but often they feel like theyre made for another species besides humanity. Each Read more Read How to install a Windows app using Crossover There are two ways to install Windows apps using Crossover. The first is to install an executable file via Crossovers Known Applications database. You can access this by using the search bar on the main page of the app. Type in the name of the program youre hoping to install and Crossover will populate the screen with matching results. Some of the apps you can look for include Microsoft Office 2006 and 2010, older versions of Adobe Photoshop, FileZilla, and 7-Zip. (You may need to purchase a key to use some of these apps. If you cant find what youre looking for, try installing a Windows app using the Install Unlisted Application option instead, which allows you to point directly to an installer file. After you type in the name of the app into the search bar, tap on the option in the bottom-right corner. On the next window, select Search for Installer to navigate to the setup file. Follow the installation prompts as they appear. Once you install a Windows program using either method, youll be able to launch the app from your Chromebooks application drawer or Crossovers main landing page. Theres a third method for running Windows apps, but you should only try it if youre unsuccessful with the first two. It involves creating a shell desktop of sorts so you can manually run an installer file. Like before, tap on Install Unlisted Application after entering a program name, then select the option to Create an empty environment. This will install the components you need to run a Windows app. A dialog window will pop up as soon as the virtual environment is ready. Tap on the option to Launch File Explorer from the drop-down menu, then navigate your devices file system to select the.exe file you want to fire up. You will also want to use this method if you need to run an app on top of a different version of Windows. To access this setting, click on the app from the main Crossover page, then select Wine Configuration from the drop-down menu. An Applications tab will appear with an option towards the bottom to change the version of Windows for that app. You can choose a version of Windows as far back as Windows 3.0, or you can join the rest of us the current age with Windows 10. By default, Crossover will set the compatibility for each app based on the information in its database. How to use Windows apps on Chrome OS As mentioned, you can launch an app from your Chromebooks application drawer or Crossovers main landing page. If an app crashes (it happens), Crossover will pop up a notification with an option for a force-close, which helps alleviate your devices memory cache. Using Windows apps through Crossover is relatively straightforward, but dont set your expectations too high. Not all Windows apps work with this particular version of Crossover. I tried installing the Windows version of Spotify and Adobe Creative Cloud, and neither were successful. Then I installed Steam, which let me message my pals with its chat app, though it often crashed because it couldnt load some promotional pop-ups. It feels like Microsoft is ramping up its major system updates for Windows, with one now coming Read more Read Some apps wont even allow you to install the software because its a virtual machine, and while Codeweavers offers direct access to legacy programs like Microsoft Office 2010, you still need a valid key to unlock the experience. Even if you do manage to get an application like that running, you might have to make up for any missing components by searching far and wide on the internet. I did have some success running Windows apps on a Google Pixelbook. 7-Zip flawlessly extracted ZIP files to the Downloads folder, and I got a kick out of running an old classic, Ski Free. FileZilla allowed me to access my FTP server, while Gimp was a hoot to use with the Pixelbooks touchscreen. I managed to get some lesser-known apps to run, too, like Bulk Rename Utility, which sports a severely outdated interface but is efficient at renaming hundreds of files in one batch. These examples are why youd go through the trouble of running Windows apps on your Chromebook: to help increase your productivity on Chrome OS. After all, there isnt always an Android app or Chrome extension that can get the job done. And while Crossover isnt the exact equivalent to running full-fledged Windows apps on Chrome OS, at least it helps bridge a few gaps as Google continues to establish its cloud-based operating system.","Google’s cloud-based Chrome OS has morphed into an impressive computing platform now that it’s compatible with Android apps and, more recently, Linux apps. Read more...",2018-10-17T18:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",How to Run a Windows App on a Chromebook,https://lifehacker.com/how-to-run-a-windows-app-on-a-chromebook-1829716989,"https://i.kinja-img.com/gawker-media/image/upload/s--PdtpreWo--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/wummeubqxpcv5palur9c.jpg",,Lifehacker.com,cloud,1
Jon Fingas,"Reportedly, the console would have a ""limited amount of compute"" inside to tackle not just basics like controller input and visual output, but game elements like collision detection. Although the system might cost more than it did if Microsoft offloaded everything to the cloud, it'd offer a better experience while still remaining more affordable. And no, there wouldn't be games limited to the conventional machine -- they'd have to be compatible with both. So long as the leak is accurate, the cloud version of Scarlett is poised to launch in 2020 alongside its traditional counterpart. There aren't concrete specs at this point, but that may be expected at this early stage when Microsoft hasn't necessarily locked down every feature. The bigger question is whether or not gamers will bite. This appears to be a bet that gaming will go the way of Netflix, with a huge library of content available from many of the devices you own. The company might be right about the always-connected nature of modern consoles, but that doesn't mean gamers will embrace streaming with open arms. They may stick to local copies of games if they have concerns about quality and cost or, just want more control over when and how they play.","Microsoft hasn't said much about its next Xbox consoles other than to acknowledge than they're in development. Details appear to be trickling out, though, and the biggest news may be what isn't included. Thurrott tipsters have maintained that the next Xbox co…",2018-07-23T23:49:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Microsoft's next Xbox could have a cloud-only counterpart,https://www.engadget.com/2018/07/23/xbox-scarlett-cloud-console/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D1600%252C949%252C0%252C0%26quality%3D85%26format%3Djpg%26resize%3D1600%252C949%26image_uri%3Dhttp%253A%252F%252Fo.aolcdn.com%252Fhss%252Fstorage%252Fmidas%252F9ece7fdad1e7025dec06ac9bf98688d0%252F205826075%252FXbox%252BOne%252BX%252Breview%252Bgallery%252B5.jpg%26client%3Da1acac3e1b3290917d92%26signature%3D9913883753141e7df322616bfe0bc41c6ecd80c8&client=cbc79c14efcebee57402&signature=3524d318f5ab1d956f1b7a678def4f33e053e5c1,engadget,Engadget,cloud,1
Tom Simonite,"Google parent Alphabet generated 86 percent of its revenue from advertising last year. On Friday the woman leading its best shot at building a second big revenue stream said she is moving on. Diane Greene, a storied cloud computing entrepreneur and executive, has been leading Googles cloud computing division since early 2016. Snagging her was seen as a good omen for the companys attempt to compete with Amazon and Microsoft in the booming business of providing computing infrastructure for other companies. Greenes announcement Friday morning that she will move on in January is being read as the latest evidence that the project hasnt gone as well as planned. Greene said that she wants to spend more time mentoring and investing in women technology entrepreneurs. She will retain her Alphabet board seat. Google Clouds new boss in January will be Thomas Kurian, who until September was a top executive at Oracle. Googles cloud division made strides under Greenes leadership, but Amazon and Microsoft have advanced their own more mature cloud businesses. Amazon essentially invented the business of renting out computers and storage by the minute or megabyte, and has cemented its dominance. The company has roughly a third of the global cloud market, according to Canalys, and that business contributes more to its bottom line than sales on Amazon.com. Microsoft has roughly half Amazons market share, but its growing more quickly than Google, which Canalys says has only 8 percent. The search companys prospects now seem more challenging than ever. When Diane Greene came in they had a really solid chance of being the number-two provider, says Maribel Lopez, of Lopez Research. Microsoft has really closed the gap and are the number two provider for most enterprise customers by a significant margin. Greene, a mechanical engineer by training, started her career designing oil rigs. Industry rules forbade her from visiting them because of her gender, and she pivoted into computing. Greene sold her first startup and its video streaming technology to Microsoft for $75 million in 1997, and then cofounded cloud computing giant VMware, where she was CEO for 10 years. Hiring Greene was seen as a major fillip for Google. She was the kind of steady enterprise helmsperson a company known for expansive data collection and quirky consumer services needed to lure customers in the buttoned-up cloud market. Greene positioned Googles cloud division to be more independent from its parent, and lured customers such as Twitter, Target, and HSBC. She made expertise in artificial intelligence the platforms calling card, building on projects like creating custom chips for machine learning. While the AI-centric strategy played to Googles strengths, it didnt help much with more boring workloads such as storage and website hosting that are the bulk of the cloud market and power Amazons dominance. Theyve been making the right moves and saying the right things but it just hasnt shown through in performance financially, says Meaghan McGrath, who tracks Google and other cloud providers at Technology Business Research. She says Google is still hamstrung by a perception that it doesnt really know how to work with corporate IT departmentsan area where Microsoft has a long track record. Greenes replacement in January brings skills well-suited to that problem. Kurian was president of product development at Oracle and a leader of its cloud strategy. McGrath says he has a reputation for being expert at the kind of business positioning that Google needs to make its cloud operations more palatable to the big buyers in cloud computing. More Great WIRED Stories",Google trails Amazon and Microsoft in the business of renting computing power to other businesses.,2018-11-16T21:59:50Z,"{'id': 'wired', 'name': 'Wired'}",What Diane Greene's Departure Means for Google Cloud,https://www.wired.com/story/what-diane-greenes-departure-means-google-cloud/,https://media.wired.com/photos/5bef130eef5d1f76593721be/191:100/pass/Diane-Greene-h_14779949.jpg,wired,Wired,cloud,1
Jon Fingas,"While many of these mining attempts rely on a public mining pool, the perpetrators here installed mining pool software an d pointed a script to reach an 'unlisted' destination. The move made it harder to simply block the cryptojacking based on internet addresses. The intruders also masked the address of their mining pool server through CloudFlare, and minimized processor use to avoid giving away its presence. RedLock said it notified Tesla right away when it discovered the breach, and that the automaker has already patched the flaw. It's not clear at this point what private data was involved, although this doesn't necessarily mean customer data. We've asked Tesla for comment on the incident and will let you know if it can share more. There doesn't appear to have been much damage at first glance, but the intrusion continues a recent trend of companies and even militaries leaving sensitive info relatively unprotected. RedLock pointed out that there have been ""hundreds"" of instances like this at other companies. While the solutions in these cases are sometimes straightforward, that they're necessary at all suggests it'll take a while before companies are diligent about preventing slip-ups like this.","Tesla isn't immune to the plague of cryptocurrency mining hijacks, it seems. Security researchers at RedLock have reported that intruders gained access to Tesla's Kubernetes console (where it deploys and manages containerized apps) without needing a password,…",2018-02-20T13:59:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Intruders 'borrowed' Tesla's public cloud for cryptocurrency mining,https://www.engadget.com/2018/02/20/tesla-cryptojacking-report/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D4000%252C2667%252C0%252C0%26quality%3D85%26format%3Djpg%26resize%3D1600%252C1067%26image_uri%3Dhttp%253A%252F%252Fo.aolcdn.com%252Fhss%252Fstorage%252Fmidas%252Fca9d68a878f081be3b65b2aecd175aab%252F206141250%252Ftesla-inc-model-3-vehicle-stands-on-display-during-automobility-ahead-picture-id883314020%26client%3Da1acac3e1b3290917d92%26signature%3Dd2df47194b53839482f37b13e28ad4aa5d88d716&client=cbc79c14efcebee57402&signature=4163cdffe484408a1d1217d7b630abfdc5711429,engadget,Engadget,cloud,1
Christopher Jones,"Streaming music doesn’t have to mean compromised sound. These hi-fi amps can help you find cloud-connected aural ecstasy. 1. Naim Audio Uniti Star Best for : Streamcurious audiophiles With a built-in CD player that rips tracks to a local drive, the Uniti Star eases the pain of parting with your CDs. Naim’s app summons your newly captured tunes and streams hi-res songs from cloud services. The hardware is pricey, but you get premium guts like a 70-watt-per-channel amp and a huge, velvet-­smooth volume knob. $5,995 Buy Now 2. Bluesound Powernode 2 Best for: Proud digital natives Bluesound’s more modestly priced streamer can access oodles of cloud music services and radio stations—including hi-res offerings—or play a local library stacked with FLACs. Basic panel controls are supplemented by the excellent BluOS Controller app. The integrated 60-watt-per-channel amp can power any speakers, from tiny to towering. $799 Buy Now Styling by Reina Takahashi This article appears in the August issue. Subscribe now. More Great WIRED Stories",Streaming music doesn’t have to mean compromised sound. Here are two picks to help you find cloud-connected aural ecstasy.,2018-09-03T13:00:00Z,"{'id': 'wired', 'name': 'Wired'}","2 Streaming Amps for Audiophiles: Naim Uniti, Bluesound",https://www.wired.com/story/head-to-head-audiophile-streaming-amps/,https://media.wired.com/photos/5b467741c6b9e35c563298be/191:100/pass/GL_H2HStreamers_web.jpg,wired,Wired,cloud,1
WIRED Staff,"Here on the WIRED Gear team, one of the questions that we field the most often is: Which smart speaker should I choose? There are a lot of different factors to take in, like how important is sound quality? And do you want a touchscreen? But when it comes to price, it's really, really hard to beat Amazon's offerings. Right now, it is currently offering deals on a lot of great devices that we've used and loved over the past few years. You can currently buy all of these devices, though a few will ship on March 30 or later.
(Note: When you buy something using the retail links in our stories, we may earn a small affiliate commission. Read more about how this works.)
Prime Members Get Up To $50 off Fire Tablets
We've already noted that the prices on Amazon's tablets already seem too good to be true in our Best Fire Tablets guide. They aren't perfect tablets, but they're incredibly cheap and do the job. If you're a Prime member, you can currently knock more off the top. The Fire HD 10 (read our review) is our favorite Fire tablet for watching videos, and it's currently only $100 for Prime members. The Fire HD 8 is also great for traveling, and well below $100. We also recommend picking up one of Amazon's official magnetic traveling cases with the money you save. The discounts will end March 30.
The Amazon Fire HD 10 costs $100 ($50 off).
The Amazon Fire HD 8 costs $50 ($30 off).
More Amazon Deals Happening Now
Echo Dot (3rd Gen.) costs $40 ($10 off): With its rounded edges and soft fabric coating, the newest Echo Dot is much more aesthetically appealing than its previous iteration. It sounds better, too. It's currently 20 percent off its original list price. This discount ends March 29.
Echo Show (2nd Gen.) costs $190 ($40 off): As with the Echo Dot, the second iteration of the Echo Show has improved by leaps and bounds. The 10-inch display now covers the front of the device, and fabric-covered speakers on the rear now look and sound better. The Show might be a great choice if you use a tablet for cooking or video calling other Echo Shows. It's one of the better Smart Displays you can own. This discount ends March 29.
Buy Two Amazon Cloud Cams for $200 ($40 off): Amazon's Cloud Cams aren't weatherproof, so it's probably best to keep them inside. But otherwise, they're a pretty simple and easy-to-use baby or pet monitor. You can also add voice control if you connect them to an Alexa-enabled device with a screen, like the Echo Show or Spot.
Two Amazon Fire Tablet Kids Editions for $195 ($65 off): One of the things that we liked best about the Fire tablets for kids was the two-year worry-free guarantee. If your child breaks it, you can send it in for a replacement with no questions asked. But what if you have multiple children? Or a child who is especially destructive? In that case, the 25 percent discount on two or more tablets may be worth a look.
More Great WIRED Stories","Amazon is currently offering Cloud Cams, Fire Tablets, and Echo Speakers at a steep discount.",2019-03-26T21:06:02Z,"{'id': 'wired', 'name': 'Wired'}",Amazon Has Great Deals on Its Tablets and Echo Speakers Now,https://www.wired.com/story/amazon-prime-fire-echo-cloud-cam-deals/,https://media.wired.com/photos/5a0f0f868ec26226c230f378/191:100/pass/02-best-fire-tablet-house-hd10.jpg,wired,Wired,cloud,1
Frederic Lardinois,"When you think of cloud computing, chances are you are thinking about massive server farms that let you edit documents in the cloud and update your CRM system, but thankfully, there’s a playful side to the cloud as well. All of those multiplayer games, after all, have to run somewhere, too. Often, gaming companies write their own systems for running these servers, but Google and Ubisoft today announced a new project that provides an open source alternative to managing and hosting multiplayer game servers. Agones, as the project is called because that’s the Greek word for ‘contest,’ uses the Google-incubated Kubernetes container project as it core tool for orchestrating and scaling a fleet of multiplayer game servers. When you play your favorite multiplayer game, it’s these kind of game servers that assure that users can see each other as they traverse an island full of 99 other suicidal maniacs, for example — and they also often run the software necessary to identify cheaters. Containers are actually ideal for this kind of scenario because game sessions tend to last for relatively short periods of time and containers can be deployed and shut down quickly. “Our goal is to continually find new ways to provide the highest-quality, most seamless services to our players so that they can focus on their games,” writes Ubisoft development director Carl Dionne today. “Agones helps by providing us with the flexibility to run dedicated game servers in optimal datacenters, and by giving our teams more control over the resources they need. This collaboration makes it possible to combine Google ’s expertise in deploying Kubernetes at scale with our deep knowledge of game development pipelines and technologies” Agones essentially extends Kubernetes with the kind of tools necessary to run a game server. These include a custom Kubernetes Controller and custom resource definitions for the game server. The team notes that developers can easily integrate their custom matchmaking services for pairing gamers with each other with the standard Kubernetes APIs to start up a game server. While Google would surely want developers to host their games on the Google Kubernetes Engine, Agones itself is cloud agnostic and can run on virtually any cloud or on premises. Today’s release is very much an early effort. A v2 roadmap is already in the works, though, and the team says it’s also working on new feature like game server Fleets and support for Windows, game server stats and node autoscaling.","When you think of cloud computing, chances are you are thinking about massive server farms that let you edit documents in the cloud and update your CRM system, but thankfully, there’s a playful side to the cloud as well. All of those multiplayer games, after …",2018-03-13T13:00:55Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","Google partners with Ubisoft to launch Agones, an open source game server hosting system",http://techcrunch.com/2018/03/13/google-partners-with-ubisoft-to-launch-agones-an-open-source-game-server-hosting-system/,https://s0.wp.com/wp-content/themes/vip/techcrunch-2013/assets/images/techcrunch.opengraph.default.png,techcrunch,TechCrunch,cloud,1
Paris Martineau,"The Pentagon didnt mince words in March when introducing the Joint Enterprise Defense Initiative, or JEDI Cloud program: This program is truly about increasing the lethality of our department and providing the best resources to our men and women in uniform, the Defense Departments Chief Management Officer John H. Gibson II told industry leaders and academics at a public event. JEDI aims to bring DoDs computing systems into the 21st century by moving them into the cloud. The Pentagon says that will help it inject artificial intelligence into its data analysis and equip warfighters with real-time data during missions, among other benefits. For the winning bidder, the contract will be lucrative: Bloomberg Government estimates it will be worth $10 billion over the next decade. Or at least thats how it should have worked. But little about the JEDI Cloud contract is standard. With dramatic pre-bid accusations, ever-changing deadlines, and a last-minute dropout due to ethical concerns, the story of the JEDI contract reads more like a bizarre corporate thriller than a simple cloud-computing deal. Case in point: Late Monday evening, Google announced it no longer plans to submit a bid by Friday, the deadline for interested applicants. Google said it couldnt be assured that the work to fulfill the JEDI contract would align with [its] AI Principles, among other things. Its a reasonable assessment, given that developing the JEDI program would contradict with Googles recently codified stance against developing or deploying artificial intelligence technologies that cause or are likely to cause overall harm, as the digital backbone of the Pentagon will certainly do in one way or another. However, it was also a bit of a gimme, as Google probably wouldnt have won the contract in the first place. Thats because Amazon has been considered the clear frontrunner for the JEDI Cloud deal since the details of the contract were first released. Accentuating concerns about the fairness of the bidding process, IBM Wednesday protested the contracts ground rules to the US Government Accountability Office. IBM said the stipulations would prove both counterintuitive and dangerous, and that the contract was written to clearly favor one well-known cloud service provider. JEDI turns its back on the preferences of Congress and the administration, is a bad use of taxpayer dollars and was written with just one company in mind, wrote IBM general manager Sam Gordy in a statement. Another potential bidder, Oracle, had previously lodged a similar protest. The Pentagons 1,375-page winner-take-all request for proposal for JEDI is a web of restrictions and requirements that some critics allege leaves few viable candidates beyond Amazon. To qualify to submit a bid, potential applicants must have the infrastructure to transition approximately 3.4 million Pentagon users and 4 million devices to the cloud, generate at least $2 billion a year in revenue from cloud services (a tough order, given that Googles cloud businesses only recently broke the $1 billion a quarter mark), prove they can win certification to handle the government's most classified information (a status only Amazon currently holds), derive most of their revenue from sources other than federal contracts (disqualifying many traditional defense contractors), and ensure their data centers are placed 150 miles apart, among many other stipulations. Viewed in their entirety, the rules suggest bids would be limited to a handful of tech giants that have made names for themselves in the cloud computing industry over the past decade, of which Amazon is king. This already short list is only made shorter by the Pentagons insistence on awarding the JEDI contract to a single company; most contracts of its size are split among several vendors. In their protests, both Oracle and IBM argued that the DoDs insistence on awarding the multi-billion contract to a one bidder would stifle innovation and pose a huge security risk by giving would-be attackers a single large entity to target. Their protests forced the Pentagon to justify its approach to Congress, and contributed to repeated delays in the bid-submission date. Google agrees. Google Cloud believes that a multi-cloud approach is in the best interest of government agencies, because it allows them to choose the right cloud for the right workload, said a Google spokesperson in a statement to WIRED. Had the JEDI contract been open to multiple vendors, Google said, we would have submitted a compelling solution for portions of it. That seems at odds with its statement Monday about pulling out because of its policy on the ethical use of AI. The company did not respond to a request for additional comment. The Pentagon argues that hiring multiple companies would be more complicated, costly, and less secure. But few industry experts and companies outside of the DoD and Amazon agree. Using multiple vendors is considered more reliable, as it reduces the risk of a total failure if one has a freak outage or vulnerability. It also would allow the DoD to make use of the expertise of several providers, rather than relying on a single entity to know everything. Kris Lahiri, chief security officer of Egnyte, an enterprise file sharing and cloud computing storage company, finds the Pentagons insistence on a winner-take-all approach worrisome. It feels too risky to bet it all on the capabilities of a single provider, says Lahiri. For that reason, he says a lot of larger tech companies eventually end up relying on more than one provider as their needs become more complicated. Lahiri says that the governments decision to put a huge swath of the DoDs operations in the hands of just one bidder feels awkward, as its not as if the Pentagon couldnt afford to divvy things up in the name of increased security. Of course, Amazon isnt just any cloud provider. The company is the only web services provider authorized to handle extremely sensitive, classified government data with a well documented history of performing similar tasks. Back in 2013, the then-underdog Amazon Web Services beat out IBM for a $600 million contract to develop a similar cloud system for the CIA. However, Amazons experience in the intelligence cloud computing market has critics worried the government might be putting all its eggs in one basket. If Amazon wins the JEDI contract, it would be the sole guard of a worrying amount of highly sensitive information about the government, says Lahiri. Meaning any sort of issue affecting Amazon could have a cascading effect throughout the intelligence community. Which I feel is very risky, he added. Amazon did not respond to WIREDs request for comment. Recent reports by Bloomberg alleging that Chinese agents compromised the supply chain of a technology supplier to the Pentagon and private industry have increased pressure on the Pentagon to prioritize security when selecting a winner. According to Bloomberg, the illicit hardware additions compromised the security of Department of Defense data centers, the CIAs drone operations, and the onboard networks of Navy warships as well as hardware by Apple and Amazon. Amazon, Apple, and the supplier, Super Micro Computer, have all denied the allegations. More Great WIRED Stories",Rivals complain that specs for a potential $10 billion contract favor Amazon.,2018-10-10T22:21:56Z,"{'id': 'wired', 'name': 'Wired'}",IBM Joins Fight Over Pentagon Cloud Contract Favoring Amazon,https://www.wired.com/story/how-pentagons-move-to-cloud-landed-in-mud/,https://media.wired.com/photos/5bbd0047235b00769c2683a5/191:100/pass/BezosJEDI-1036099110.jpg,wired,Wired,cloud,1
Paris Martineau,"In 1994, soon after Jeff Bezos incorporated what would become Amazon, the entrepreneur briefly contemplated changing the companys name. The nascent firm had been dubbed Cadabra, but Bezos wanted a less playful, more accurate alternative: Relentless. (Relentless.com redirects to Amazon.com to this day.) Twenty-four years later, perhaps no adjective better describes Bezos empire than the name he once wanted to give it.
The company is known as the everything store, but in its dogged pursuit of growth, Amazon has come to dominate more than just ecommerce. Its now the largest provider of cloud computing services and a maker of home security systems. Amazon is a fashion designer, advertising business, television and movie producer, book publisher, and the owner of a sprawling platform for crowdsourced micro-labor tasks. The company now occupies roughly as much space worldwide as 38 Pentagons. It has grown so large that Amazons many subsidiaries are difficult to trackso we catalogued them all for you. This is our exhaustive map of the Kingdom of Amazon.
You might be wondering, why Amazon? After all, other tech firms, including Google and Facebook, have also expanded outside their core businesses in recent years. But few other companies can claim leadership in sectors as disparate as videogame streaming, online fabric sales, and facial recognition. Amazon also employs far more people than its competitors. Roughly 613,000 people work at Amazon, more than twice as many as work at Alphabet (94,000), Facebook (33,000), and Microsoft (135,000) combined. Most of those workers labor in one of Amazons more than 100 North American logistics centers, or at one of over 450 Whole Foods stores.
Amazon employees are paid far less than other tech workers. In its annual filing with the Securities and Exchange Commission in February, Amazon said its median worker earned $28,446 in 2017 (it says that number jumps to $34,123 for full-time US workers). Facebooks median salary in 2017, by contrast, was over $240,000.
A bit of context: It helps to know how Amazon makes money. While its retail business is the most visible to consumers, the cloud computing arm, Amazon Web Services, is the cash cow. AWS has significantly higher profit margins than other parts of the company. In the third quarter, Amazon generated $3.7 billion in operating income (before taxes). More than half of the total, $2.1 billon, came from AWS, on just 12 percent of Amazons total revenue. Amazon can use its cloud cash to subsidize the goods it ships to customers, helping to undercut retail competitors who dont have similar adjunct revenue streams.
Books
Amazon began as an online bookseller in 1994, and although it quickly expanded into other ventures, it still owns and operates multiple publishers and online bookselling subsidiaries. Nowadays, most of these fall under the umbrella of Amazon Publishing, which is both a publisher and the owner of imprints for specific genres, languages and locales.
Amazon imprint Thomas &amp; Mercer publishes mysteries, thrillers, and true crime novels; Little A handles literary fiction and nonfiction; AmazonCrossing is responsible for translated texts; 47North does science fiction and fantasy; Skyscape is for teen and young adult books; theres Two Lions for childrens books; Jet City Comics for, well, comics; Montlake Romance handlesyou guessed itromance; Waterfall Press publishes Christian fiction; Grand Harbor Press is responsible for a category Amazon describes only as inspirational; Lake Union Publishing handles book club fiction; Amazon Original Fiction publishes short stories and fiction; AmazonEncore is for rediscovered works; and TOPPLE Books spotlights works selected by Jill Soloway. Amazon also has acquired Avalon Books, The Book Depository, and AbeBooks.
In 2005, Amazon acquired BookSurge, an on-demand self-publishing service, and CustomFlix, an on-demand video publishing service, which was later renamed CreateSpace. Two years later it bought independent audiobook producer, Brilliance Audio, and launched its own e-book publisher, Kindle Direct Publishing, concurrently with the first Amazon Kindle e-reader. Soon after, the company paid $300 million to acquire audiobook seller Audible. It also owns ACX, an audiobook publishing company.
In 2009, Amazon merged BookSurge and CreateSpace to provide more on-demand options for publishers; the merged company did business under the name CreateSpace, but was officially named On-Demand Publishing. Four years later, Amazon purchased the book-review site GoodReads. In 2014, the company acquired digital comics distribution platform ComiXology. The following year, it launched Amazon Rapids, a subscription-based app that presents short childrens stories in the form of fake text messages. In 2018, CreateSpace was merged with Kindle Direct Publishing, which now handles all e-book and paperback publishing services, while all media services were transferred to another new company, called Amazon Media on Demand, which is responsible for manufacturing and shipping disc content. Amazon also operates a digital Kindle Store, where customers can purchase ebooks and other content for the Kindle, and more than a dozen physical Amazon Books stores.
Media
In 1998, four years after its founding, Amazon bought IMDb (Internet Movie Database) and expanded into music, offering users more than 125,000 titles at launch on CDs and DVDs. The following year, Amazon acquired Alexa Internet, a web-traffic-analysis company not to be confused with the other, more popular Alexa that came later. It wasnt until 2007 that Amazon launched its streaming service, which was originally called Amazon MP3 and later changed to Amazon Music. In 2006, the company launched Amazon Unbox, a service for purchasing and downloading videos, which was later changed to Amazon Video on Demand, then Amazon Instant Video, and finally Prime Video (which is also, confusingly known as Amazon Video). Prime Video showcases content by Amazon Studios, which began in 2010 as a script development entity but now produces and distributes television series and films. (Last year Amazon bought the TV rights to make a Lord of the Rings spinoff for an estimated $250 million.) Through IMDb, Amazon purchased Withoutabox, which streamlined the submission and selection process for film festivals (and which Amazon is in the process of closing), as well as Box Office Mojo, which algorithmically tracks box office revenue, in 2008.
In early 2014, Amazon acquired American videogame developer Double Helix Games and renamed it Amazon Game Studios. Shortly after, Amazon bought popular live-streaming platform Twitch for $970 million, and Curse, a gaming information and communication platform with a robust community of users. Shortly after the acquisition, all Curse accounts were transferred to Twitch, boosting the platforms user base. In 2015, Amazon launched Amazon Tickets, which sells tickets to concerts and other live events in the UK.
Amazon also owns sites that provide educational resources, including Amazon Inspire and TenMarks.com (Amazon is winding down the latter). It also has Whispercast, a service designed to help educators share audiobooks.
Lastly, for some reason, Amazon also owns DPReview, a digital camera website.
Retail
Over 6 million independent merchants pay to sell goods through Amazons ecommerce marketplace and many also shell out additional fees for services like shipping and warehousing. Amazon also sells its own products through dozens of house brands, including Mountain Falls (primarily personal care products), Rivet (furniture), and Daily Ritual (womens clothing).
Merchants also may pay to place ads on Amazon through Amazon Advertising; the company is now the third-largest digital-advertising platform, behind Google and Facebook, with an estimated 4.2 percent market share.
Need some cash to start your Amazon selling business? Amazon Lending, an invitation-only program launched in 2011, has doled out billions in loans to businesses that may have difficulty obtaining credit elsewhere.
Nestled within Amazon.com are businesses such as $119 per-yearAmazon Prime, which began in 2005 as a subscription service offering free two-day shippingbut quickly ballooned into something much larger. In addition to Prime Video and Prime Music, Amazon launched a photo-storage service called Amazon Photos in 2014, giving users access to Amazon Drive a cloud-based file-storage service. Other Prime products include: Prime Reading, a rotating ebook loan service unrelated to Amazons other Kindle offerings; Prime Pantry, which ships non-perishable grocery items for an additional fee; Amazon Fresh, a grocery delivery and pickup service; Prime Now, a one-to-two hour direct delivery service for Prime members in certain cities; and Amazon Restaurants, which offers food delivery, among others. While Prime Now offers one-hour delivery for an additional fee, most of these services are included with a Prime membership.
There's also Amazon Warehouse, for deals on used products, Amazon Renewed, for refurbished products with a warranty, and Amazon Second Chance, also for second-hand goods. Lastly, there's Subscribe with Amazon, which lets customers sign up for subscription services like monthly boxes of snacks. Need someone to paint a wall or clean your carpet? There's Amazon Home Services, a marketplace for hiring home-repair and cleaning professionals.
To hire and manage the contract workers making deliveries, Amazon created Amazon Flex. The company also has its own payment processor, Amazon Pay, which was launched in 2007. Earlier this year, Amazon acquired the popular Indian payment platform Tapzo for $40 million, and then immediately said it would shutter Tapzo and shift users to Amazon Pay.
Aside from Amazon.com, the company also owns several other ecommerce websites, including Zappos (shoes) Shopbop (high-end womens clothing), East Dane (mens clothing), 6pm (discount clothing) and Fabric.com (you guessed it: fabric). Also in 2010, Amazon purchasedWoot!, a site for daily ecommerce deals. Last year, Amazon bought Dubai-based ecommerce platform Souq.com for $580 million; Souq then boughtWing.ae, a startup that builds next-day delivery networks for ecommerce sites. In addition, Amazon also owned Junglee, an Indian ecommerce site.
In one of its highest-profile acquisitions, Amazon last year purchasedWhole Foods, the high-end grocery store chain with hundreds of locations. Earlier in 2018, it also bought a 49 percent stake in More, one of Indias largest grocery chains. Amazon simultaneously operates its own chain of partially automated grocery stores, known as Amazon Go, which use ceilings full of cameras to offer customers a checkout-free experience. It also operates three 4 Star Stores, where customers can purchase products rated 4-stars and above on Amazon Marketplace, and a fleet of Treasure Trucks scattered around the country, doling out everything from steak to Philips Hue lights in a bizarre spin on the traditional food truck model.
Aside from traditional ecommerce, Amazon also owns Amazon Mechanical Turk, a site where organizations can hire individuals for piecemeal tasks, such as labeling data for machine learning algorithms. Started in 2005, Mechanical Turk is favored by academic researchers for collecting survey and experimental data.
Amazon Web Services
In 2003, Amazon launched its web hosting business, Amazon Web Services. The unit had begun several years earlier as Merchant.com, which helped other retailers such as Target and Borders build their own online shopping sites using Amazons e-commerce tools. In 2006, the company launched Amazon S3 a simple cloud storage service and hosting provider that as of 2013 stored more than two trillion digital objects, as well as Amazon Elastic Compute Cloud (better known as EC2), and Amazon Simple Queue Service. Reddit, Tumblr, Netflix, Pinterest, and Dropbox have all used Amazon S3 as their primary host or storage provider at one point over the past decade. (This article is also powered by Amazon, as WIREDs website runs on AWS.)
AWS offers so many cloud computing products and services that it would be cumbersome to name them all. In 2011, Amazon introduced AWS GovCloud, aimed at government agencies. Four years later, it launched AWS IoT, a platform for connecting and managing the plethora of connected devices known as the Internet of Things. Shortly after, the company won a $600 million contract to build AWS Secret Region, a cloud storage service for the CIA.
In 2015, Amazon purchased Shoefitr, a startup, that uses 3-D technology to help customers determine their shoe size while shopping online, and Safaba Translation Systems, a machine-translation startup. In 2017, the company acquired 3-D body scanning and modeling company Body Labs, and game developer platform GameSparks. Research conducted by these two latter companies was used to flesh out Amazons expansion into augmented and virtual reality, which is primarily covered by Amazon Sumerian, an AWS service. Around the same time, Amazon also acquired AI-security startup Harvest.ai and Sqrrl, a cybersecurity startup that was spun out of the NSA.
AWS also offers controversial facial-recognition software known as Amazon Rekognition, which is used by some law enforcement agencies and has also been pitched to Immigrations and Customs Enforcement. The service has drawn criticism for being inaccurate, particularly when used to identify people of color. In a test, the ACLU found that it incorrectly matched 28 members of Congress with people who had been arrested for a crime.
Energy and Transportation
To power all those data centers, Amazon has contracted with multiple renewable energy companies to create more than a dozen wind and solar energy farms in Indiana, Virginia, Ohio, and North Carolina. In 2017, it finished construction on its largest wind farm yet, the Amazon Wind Farm Texas, an achievement that Bezos celebrated by smashing a bottle of champagne on top of one of the farms 300-plus-foot tall wind turbines in an ultra-dramatic video:
Amazon also owns a fleet of Prime Air Cargo Planes and is facilitating ocean freight shipments. The company has been working on an army of Prime Air Drones since 2013. The project is still in its early stages, though the company first delivered a package to an English customer via drone in 2016.
Hardware
In 2004, Amazon opened Lab126, a computer hardware research and development unit. The Sunnyvale, California-based laboratory later created some of Amazons most successful products, including the Kindle in 2007 (and its many updated versions), the Kindle Fire Tablet in 2011, the Amazon Fire TV and Fire TV Stick in 2014, the Amazon Echo in 2015, and the smaller Amazon Echo Dot in 2016. Another Alexa-equipped device, the Echo Look, is a camera contraption that provides fashion advice. Lab126 was also responsible for the Amazon Fire Phone, which was a commercial failure.
In 2012, Amazon acquired robotics firm Kiva Systems, for $775 million, which it later renamed Amazon Robotics. After the acquisition, Amazon ended Kivas contracts with other companies like Staples and Crate and Barrel, leaving Amazon warehouses as the sole benefactor of the technology.
This video of Kiva robots in action is from 2013, shortly after Amazon bought the company.
In 2017, Amazon launched Amazon Key, a service that allows Amazon workers to deliver items inside a users home by making use of the Amazon Cloud Cam security camera, a compatible smart lock, and the Amazon Key app. Soon after, Amazon acquired Blink Home, a home automation company that makes security cameras and a video doorbell, as well asRing, best known for its smart doorbell, which includes a video camera, motion sensors, and other remote controls. Amazon also expanded Amazon Key delivery to the trunks of users cars, for some reason, with a service oh-so-creatively called Amazon Key In-Car.
Healthcare
In 2014, Amazon started a secret internal lab dedicated to developing healthcare technology that goes by at least three different names, depending on who you ask: 1492, The Amazon Grand Challenge, and Project X. As of late, the project has reportedly partnered with the Fred Hutchinson Cancer Research Center in Seattle to explore using machine learning to prevent or cure cancer, and is pitching health insurance companies on a new product called Hera, which mines patient medical records to flag incorrect codes and potential misdiagnoses, and help hospitals bill patients. Amazon also sells medical supplies to hospitals through a healthcare offshoot of its business-to-business marketplace, Amazon Business.
LEARN MORE
The WIRED Guide to Robots
Amazon has been hiring high-profile doctors, primary care specialists, and healthcare law experts. In the first quarter of 2018, Amazon hired more than 20 people with healthcare experience, including employees poached from CVS Health and UnitedHealth Group. In January, Amazon partnered with JPMorgan Chase and Berkshire Hathaway to create a new, still nameless company ostensibly designed to improve healthcare and cut costs. In August, CNBC reported that Amazon plans to open primary care clinics at its headquarters in Seattle. In June, Amazon bought online pharmacy PillPack, a startup that ships medication directly to customers, for $1 billion.
Bezos
Amazon CEO Jeff Bezos owns an equally ridiculous array of companies and ventures outside of Amazon. Theres Nash Holdings, which acquired the Washington Postfor $250 million in 2013. Bezos also owns Bezos Expeditions, which manages his venture capital investments. The entity is responsible for spaceflight services companyBlue Origin, numerous charitable organizations, a project to recover the Apollo F-1 Engine from the depths of the ocean, and the 10,000 Year Clock, under construction inside a mountain in Texas and designed to last for 10,000 years.
Do you know a part of Amazon we didn't list? Let the authors know at: paris_martineau@wired.com and louise_matsakis@wired.com.
More Great WIRED Stories","The ecommerce company is also a cloud computing provider, TV producer, fashion designer, wind-farm backer, and organizer of crowdsourced micro-labor tasks.",2018-12-23T12:00:00Z,"{'id': 'wired', 'name': 'Wired'}",Why It's Hard to Escape Amazon's Long Reach,https://www.wired.com/story/why-hard-escape-amazons-long-reach/,https://media.wired.com/photos/5c1c2bdd59e96b0db4565b1b/191:100/pass/Bezos-Empire-h_15073081.jpg,wired,Wired,cloud,1
Jacob Kleinman,"Storing all your photos in the cloud can be great, until it isn’t. Sometimes the storage service you use might jack up its prices or a few of your photos could just inexplicably disappear. Even worse, your private pictures might just end up on a stranger’s smartphone. It sounds crazy, but that’s exactly what happened to one Oregon man. Photos of him and his fianceé (including some nude pics) that were stored on Verizon ’s cloud service somehow showed up on another Verizon phone. Thankfully, in this case the phone’s owner knew the couple and warned them about it, leading to a $4 million lawsuit against the carrier. If you’re using Verizon Cloud to store your photos and you’re worried the same thing might happen to you here’s what you need to know, and what you can do about it. What’s The Issue? It’s unclear what specifically caused this one particular incident. In a forum discussing the story, one popular theory is that Verizon links its cloud service accounts to each smartphone’s International Mobile Equipment Identity number. The problem is that a device’s IMEI doesn’t change when you swap out the SIM card, so a refurbished phone could potentially have access to some else’s cloud storage. Regardless of the specific cause, a little research quickly reveals that Verizon Cloud isn’t exactly popular among its users. Just type the words “verizon cloud photos” into Google and the autofill suggestions speak for themselves. How to Get Your Photos Out of Verizon Cloud It’s possible to transfer your photos and videos from Verizon Cloud to a different photo storage service, but the company doesn’t exactly make it easy. You’ll have to log into your cloud storage account, select the media files, download them to your device, and then upload them to another service. The exact process varies a bit depending on your device. It’s easiest on a PC, where Verizon actually lets you highlight all your photos at once and then download them. If you only have a Mac, you’ll have to click on each individual file you want to download, according to Verizon’s instructions. The process on a smartphone is similar, but Verizon’s mobile interface somehow makes it a little bit trickier. Once you’d downloaded all your photos and videos, you can delete them from Verizon Cloud pretty easily. According to the company’s instructions, all you have to do is log into Verizon’s website, select the files you’re getting rid of (or just select them all), and then send them to the trash. With that done, the odds of your private pics randomly showing up on a stranger’s phone should be a lot lower. Where to Store Your Photos Instead If you just ditched Verizon Cloud, you’ll probably need a new cloud service to store all your pictures. Personally, I’m a big fan of Google Photos, which works seamlessly and offers 15GB of storage for free. If you’re not a fan of Google, Amazon’s service comes with 5GB of free space for Prime subscribers, and Microsoft also offers 5GB of storage for free with paid tiers above that. There are plenty of options to choose from while you wait for Verizon to solve it’s privacy issues, though once you experience the convenience of Google Photos you may never want to go back.","Storing all your photos in the cloud can be great, until it isn’t. Sometimes the storage service you use might jack up its prices or a few of your photos could just inexplicably disappear. Even worse, your private pictures might just end up on a stranger’s sm…",2018-02-16T20:45:00Z,"{'id': None, 'name': 'Lifehacker.com'}",How to Get Your Photos Out of Verizon's Leaky Cloud Service,https://lifehacker.com/how-to-get-your-photos-out-of-verizons-leaky-cloud-serv-1823081982,"https://i.kinja-img.com/gawker-media/image/upload/s--B23qj3PI--/c_fill,fl_progressive,g_center,h_450,q_80,w_800/afn53phimgyryowp5mha.png",,Lifehacker.com,cloud,1
David Murphy,"This weeks question for Lifehackers tech-advice column comes from Nate. While his query is a little business-themed, its still applicable to anyone who doesnt like paying a ton of money for cloud storageso, everyone. The Lifehacker reader asks: Being a small business owner, I run into the issue all the time of clients and their cloud storage systems. Google Drive, Dropbox, Box, etc. It creates quite the headache to be able to find everything, and it also becomes pretty expensive to maintain a subscription to all of these different platforms. How would you suggest consolidating everything into one main platform, while still making it convenient for my clients? Before I get to the answer, a quick note. Were reaching the end of the year for Lifehackers tech advice column, Tech 911, and the mailbag is looking like the inverse of Santas sackwhich is to say, please send me any and all tech-related questions you have, especially if they relate to things youve received (or are about to buy for yourself) this holiday season. I am here to help you! Let me help you. The Doctors Advice: This might sound like a silly comment, but here goes: If the clients are the ones with all the different online storage systems, why are you paying for different subscriptions? Its not like youre buying an unlimited account with Google, for example, that allows you to store anything you want on another companys Google Drive. Said company runs and pays for the storage, and creates user accounts for others to access it as it sees fit. Of course, a company might be doing just that and passing the buck along to you starting at $10 in my examplewhich would better explain your letter. If your clients all have different cloud services, and theyre charging you for a separate user account on each, that adds up. If you dont have an offsite cloud storage plan (and you really should), Googles newest update to Read more Read Assuming you can convince your clients to adopt a new solution for file managementwhich might involve taking deliverables outside of their control (their cloud storage) and keeping it on a solution you set upthen I agree, it makes the most sense to consolidate everything youre doing to a single service, rather than having to deal with a bunch of services simultaneously. If youre comfortable dealing with a more manual approach, you can always pick up a great NAS boxa network-attached storage deviceand use that to host your data. It shouldnt be that tricky to set up user accounts for your clients and launch some kind of online portal they can use to view, send, and download files. (Or you can grant them access to a simple FTP server, if you want.) You could also setup your own dedicated server, but thats a bit more complex than a NAS box. Google launched its new Google One cloud storage plans back in May and started rolling its paid Read more Read If youd rather not mess with any of that and just use a cloud servicewhich will probably guarantee you a lot more uptime, since youll have those rarer moments when your office internet connection goes down, the NAS box has to update, etc.you have an overwhelming amount of options to choose from. Ive previously covered some of the common consumer cloud options, but cloud storage for small business (and its costs) is a different beast. Googles G Suite could work, which starts at $5 per user for a shared pool of 30GB of cloud storage, or $10 per user for unlimited storage (if you have more than five users; 1TB per user if you have fewer than five). If you want others to be able to upload and download files seamlessly in your cloud, youll need to create accounts for themotherwise, theyll just be able to download files. This could get costly, and not really solve the spending issue you identified in your letter. You might also want to check out Dropboxs offerings. For its cheapest business plan$450 annuallyyou get up to three user accounts and 3TB of shared storage. You can reuse these user licenses as you see fit (as clients come and go), and you can grant your clients the ability to access shared folders and download anything in them. (And they can send new things your way via the File Request feature, or sign up for a free Dropbox account themselves, so long as they dont blow past the 2GB free limit.) Heck, you might even be able to just use a cheaper Dropbox personal account, if you dont need those extra users. And while youre at it, dont forget to check out DBinbox, which you can use to make a super-easy upload here form for your clients. Im not as familiar with Microsofts SharePoint Online offering ($10/user per month for unlimited storage), but you should be able to share folders with external clients and they can then download or upload whatever they want. I believe theyll need to make a Microsoft account to do this, but that appears to be the only major restriction? Unless Im totally misinterpreting this, you could get away with a single SharePoint Online account and grant access to as many external users as you want. Dear Lifehacker, Anytime I see an offer for free cloud storage, I'm all over it. I have over Read more Read You could also just pass files back and forth via a service like MASV, if youre sending archives of drafts and finished work to one another. Its less useful if each client just wants to have a folder they can reference thats full of everything youve worked on together: old and new projects, invoices, artwork, documents, spreadsheets, et cetera. Similarly, theres Hightail also worth considering, with the same kind of limitations. Those are a few of the options I can think of. Unfortunately, cloud storage can be pricey no matter how you go about it. And I find that cheaper solutions tend to create more headachesor, worse, can be a lot slower than an established player like Google, or Dropbox, et cetera. Nevertheless, hopefully one of these works for you. Write back and let me know what you picked (or if you need a bit more guidance!)","This week’s question for Lifehacker’s tech-advice column comes from Nate. While his query is a little business-themed, it’s still applicable to anyone who doesn’t like paying a ton of money for cloud storage—so, everyone. Read more...",2018-12-14T14:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",Which Cloud Storage Service Should I Use to Share and Receive Files Externally?,https://lifehacker.com/which-cloud-storage-service-should-i-use-to-share-and-r-1831072814,"https://i.kinja-img.com/gawker-media/image/upload/s--khwCNwvb--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/i5aas1ryvjnidnhsomvn.jpg",,Lifehacker.com,cloud,1
Shannon Liao,"Personal media player Plex told its users today that it’s shutting down its Plex Cloud service at the end of November, as spotted by Variety. The Plex Cloud service has had multiple technical issues and has been in maintenance mode for a few months. The company said in a statement to Variety that “after a lot of investigation and thought, we haven’t found a solution capable of delivering a truly first class Plex experience to Plex Cloud users at a reasonable cost.” We’ve reached out to Plex for comment. Unable to handle multiple cloud storage intergrations Plex first launched its cloud-based media server in 2016 as a way for users to easily access extra storage. Initially, users had to subscribe to Amazon Drive, which cost $59.99 a year for unlimited storage at the time, and get a Plex Pass in order to use Plex Cloud. Then, Plex added support for Dropbox, Google, and Microsoft’s OneDrive cloud storage. (Coincidentally, Amazon ended its unlimited storage plan last year.) Ultimately, Plex was unable to handle these integrations, running into a lot of technical issues that forced it to disable support for Amazon Drive and then stop making new cloud servers in February. The news is a blow to premium subscribers who might have relied on Plex Cloud to store and view their personal media.","Media player app Plex told its users today that it’s shutting down its Plex Cloud service at the end of November, as spotted by Variety. The Plex Cloud service has had multiple technical issues and has been in maintenance mode for a few months.",2018-09-11T19:56:42Z,"{'id': 'the-verge', 'name': 'The Verge'}",Plex shutters its cloud service after months of technical issues,https://www.theverge.com/2018/9/11/17847288/plex-cloud-service-ends-technical-issues,https://cdn.vox-cdn.com/thumbor/pcYqCvpcZVcQm4YWFq3lp4k7VHU=/0x88:2246x1264/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/7165657/plex-cloud-server.0.jpg,the-verge,The Verge,cloud,1
Ingrid Lunden,"The growth of cloud services — with on-demand access to IT services over the Internet — has become one of the biggest evolutions in enterprise technology, but with it, so has the threat of security breaches and other cybercriminal activity. Now it appears that one of the leading companies in cloud services is looking for more ways to double down and fight the latter. Amazon’s AWS has been working on a range of new cryptographic and AI-based tools to help manage the security around cloud-based enterprise services, and it currently has over 130 vacancies for engineers with cryptography skills to help build and run it all. One significant part of the work has been within a division of AWS called the Automated Reasoning Group, which focuses on identifying security issues and developing new tools to fix them for AWS and its customers based on automated reasoning, a branch of artificial intelligence that covers both computer science and mathematical logic and is aimed at helping computers automatically reason completely or nearly completely. In recent times, Amazon has registered two new trademarks, Quivela and SideTrail, both of which have connections to ARG. Classified in its patent application as “c omputer software for cryptographic protocol specification and verification,” Quivela also has a Github repository within AWS Labs’ profile that describes it as a “prototype tool for proving the security of cryptographic protocols,” developed by the AWS Automated Reasoning Group. (The ARG also has as part of its mission to share code and ideas with the community.) SideTrail is not on Github, but Byron Cook, an academic who is the founder and director of the AWS Automated Reasoning Group, has co-authored a research paper called “SideTrail: Verifying the Time Balancing of Cryptosystems.” However, the link to the paper, describing what this is about, is no longer working. The trademark application for SideTrail includes a long list of potential applications (as trademark applications often do). The general idea is cryptography-based security services. Among them: “Computer software, namely, software for monitoring, identifying, tracking, logging, analyzing, verifying, and profiling the health and security of cryptosystems; network encryption software; computer network security software,” “Providing access to hosted operating systems and computer applications through the Internet,” and a smattering of consulting potential: “Consultation in the field of cloud computing; research and development in the field of security and encryption for cryptosystems; research and development in the field of software; research and development in the field of information technology; computer systems analysis.” Added to this, in July, a customer of AWS started testing out two other new cryptographic tools developed by the ARG also for improving an organization’s cybersecurity. Tiros and Zelkova, as the two tools are called, are math-based techniques that variously evaluate access control schemes, security configurations and feedback based on different setups to help troubleshoot and prove the effectiveness of security systems across storage (S3) buckets. Amazon has not trademarked Tiros and Zelkova. A Zelkova trademark, for financial services, appears to be registered as an LLC called “Zelkova Acquisition” in Las Vegas, while there is no active trademark listed for Tiros. Amazon declined to respond to our questions about the trademarks. A selection of people we contacted associated with the projects did not respond to requests for comment. More generally, cryptography is a central part of how IT services are secured: Amazon’s Automated Reasoning Group has been around since 2014 working in this area. But Amazon appears to be doing more now both to ramp up the tools it produces and consider how it can be applied across the wider business. A quick look on open vacancies at the company shows that there are currently 132 openings at Amazon for people with cryptography skills. “Cloud is the new computer, the Earth is the motherboard and data centers are the cards,” Cook said in a lecture he delivered recently describing AWS and the work that the ARG is doing to help AWS grow. “The challenge is that as [AWS] scales it needs to be ever more secure… How does AWS continue to scale quickly and securely? “AWS has made a big bet on our community,” he continued, as one answer to that question. That’s led to an expansion of the group’s activities in areas like formal verification and beyond, as a way of working with customers and encouraging them to move more data to the cloud. Amazon is also making some key acquisitions also to build up its cloud security footprint, such as Sqrrl and Harvest.ai, two AI-based security startups whose founding teams both happen to have worked at the NSA. Amazon’s AWS division pulled in over $6 billion in revenues last quarter with $1.6 billion in operating income, a healthy margin that underscores the shift that businesses and other organizations are making to cloud-based services. Security is an essential component of how that business will continue to grow for Amazon and the wider industry: more trust in the infrastructure, and more proofs that cloud architectures can work better than using and scaling the legacy systems that businesses use today, will bolster the business. And it’s also essential, given the rise of breaches and ever more sophisticated cyber crimes. Gartner estimates that cloud-based security services will be a $6.9 billion market this year, rising to nearly $9 billion by 2020. Automated tools that help human security specialists do their jobs better is an area that others like Microsoft are also eyeing up. Last year, it acquired Israeli security firm Hexadite, which offers remediation services to complement and bolster the work done by enterprise security specialists.","The growth of cloud services — with on-demand access to IT services over the Internet — has become one of the biggest evolutions in enterprise technology, but with it, so has the threat of security breaches and other cybercriminal activity. Now it appears tha…",2018-08-30T18:48:14Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Amazon is quietly doubling down on cryptographic security,http://techcrunch.com/2018/08/30/amazon-aws-cryptography-security/,https://techcrunch.com/wp-content/uploads/2018/06/GettyImages-622184706.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google today announced that it is providing the Cloud Native Computing Foundation (CNCF) with $9 million in Google Cloud credits to help further its work on the Kubernetes container orchestrator and that it is handing over operational control of the project to the community. These credits will be split over three years and are meant to cover the infrastructure costs of building, testing and distributing the Kubernetes software. Why does this matter? Until now, Google hosted virtually all the cloud resources that supported the project like its CI/CD testing infrastructure, container downloads and DNS services on its cloud. But Google is now taking a step back. With the Kubernetes community reaching a state of maturity, Google is transferring all of this to the community. Between the testing infrastructure and hosting container downloads, the Kubernetes project regularly runs over 150,000 containers on 5,000 virtual machines, so the cost of running these systems quickly adds up. The Kubernetes container registry served almost 130 million downloads since the launch of the project. It’s also worth noting that the CNCF now includes a wide range of members that typically compete with each other. We’re talking Alibaba Cloud, AWS, Microsoft Azure, Google Cloud, IBM Cloud, Oracle, SAP and VMware, for example. All of these profit from the work of the CNCF and the Kubernetes community. Google doesn’t say so outright, but it’s fair to assume that it wanted others to shoulder some of the burdens of running the Kubernetes infrastructure, too. Similarly, some of the members of the community surely didn’t want to be so closely tied to Google’s infrastructure either. “By sharing the operational responsibilities for Kubernetes with contributors to the project, we look forward to seeing the new ideas and efficiencies that all Kubernetes contributors bring to the project operations,” Google Kubernetes Engine product manager William Deniss writes in today’s announcement. He also notes that a number of Google’s will still be involved in running the Kubernetes infrastructure. “Google’s significant financial donation to the Kubernetes community will help ensure that the project’s constant pace of innovation and broad adoption continue unabated,” said Dan Kohn, the executive director of the CNCF. “We’re thrilled to see Google Cloud transfer management of the Kubernetes testing and infrastructure projects into contributors’ hands – making the project not just open source, but openly managed, by an open community.” It’s unclear whether the project plans to take some of the Google-hosted infrastructure and move it to another cloud, but it could definitely do so — and other cloud providers could step up and offer similar credits, too.",Google today announced that it is providing the Cloud Native Computing Foundation (CNCF) with $9 million in Google Cloud credits to help further its work on the Kubernetes container orchestrator and that it is handing over operational control of the project t…,2018-08-29T16:09:47Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google steps back from running the Kubernetes infrastructure,http://techcrunch.com/2018/08/29/google-steps-back-from-running-the-kubernetes-infrastructure/,https://techcrunch.com/wp-content/uploads/2018/05/gettyimages-845816950.jpg?w=712,techcrunch,TechCrunch,cloud,1
Ron Miller,"The interconnectedness of the cloud has allowed us to share content widely with people inside and outside the organization and across different applications, but that ability has created a problem of its own, a kind of digital fragmentation. How do you track how that piece of content is being used across a range of cloud services? It’s a problem Box wants to solve with its latest features, Activity Stream and Recommended Apps. The company made the announcements at BoxWorks, its annual customer conference being held this week in San Francisco, Activity Stream provides a way to track your content in real time as it moves through the organization, including who touches it and what applications it’s used in, acting as a kind of digital audit trail. One of the big problems with content in the cloud age is understanding what happened to it after you created it. Did it get used in Salesforce or ServiceNow or Slack? You can now follow the path of your content and see how people have shared it, and this could help remove some of the disconnect people feel in the digital world. As Jeetu Patel, Box’s Chief Product and Chief Strategy Officer points out, an average large company could have more than a thousand apps and there is no good way to connect the dots when it comes to tracking unstructured content and getting a unified view of the digital trail. “We integrate with over 1400 applications, and as we integrate with those applications, we thought if we could surface those events, it would be insanely useful to our users,” he said. Patel sees this as the beginning of an important construct, the notion of a content hub where you can see the entire transaction record associated with a piece of content. Activity Stream sidebar inside Box. Photo: Box But Box didn’t want to stop with just a laundry list of the connections. It also created deep links into the applications being used, so a user can click a link, open the application and view the content in the context of that other application. “It seems like Box was a logical place to get a bird’s eye view of how content is being used,” Patel said, explaining Box’s thinking in creating this feature. A related feature is a list of Recommended Apps. Based the Box Graph, and what Box knows about the user, the content they use, and how it’s interconnected with other cloud apps, it also displays a list of recommended apps right in the Box interface. This lets users access those applications in the context of their work, so for instance, they could share the content in Slack right from the document. Recommended Apps bar inside Box. Photo: Box For starters, Recommended Apps integrations include G Suite apps, Slack, Salesforce, DocuSign and Netsuite, but Patel says anyone who is integrated with the web app via the API will start showing up in Activity Stream. While the products were announced today, Box is still working out the kinks in terms of how this will work. They expect these features to be available early next year. If they can pull this off, it will go a long way toward solving the digital fragmentation problem and making Box the content center for organizations.","The interconnectedness of the cloud has allowed us to share content widely with people inside and outside the organization and across different applications, but that ability has created a problem of its own, a kind of digital fragmentation. How do you track …",2018-08-29T16:00:00Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Box builds a digital hub to help fight content fragmentation,http://techcrunch.com/2018/08/29/box-builds-a-digital-hub-to-help-end-content-fragmentation/,https://techcrunch.com/wp-content/uploads/2018/07/Aaron-Levie-Startup-School.png?w=568,techcrunch,TechCrunch,cloud,1
Ron Miller,"As the world shifts to a cloud native approach, the way you secure applications as they get deployed is changing too. Twistlock, a company built from the ground up to secure cloud native environments, announced a $33 million Series C round today led by Iconiq Capital. Previous investors YL Ventures, TenEleven, Rally Ventures, Polaris Partners and Dell Technologies Capital also participated in the round. The company reports it has received a total of $63 million in venture investment to date. Twistlock is solving a hard problem around securing containers and serverless, which are by their nature ephemeral. They can live for fractions of seconds making it hard track problems when they happen. According to company CEO and co-founder Ben Bernstein, his company came out of the gate building a security product designed to protect a cloud-native environment with the understanding that while containers and serverless computing may be ephemeral, they are still exploitable. “It’s not about how long they live, but about the fact that the way they live is more predictable than a traditional computer, which could be running for a very long time and might have humans actually using it,” Bernstein said. Screenshot: Twistlock As companies move to a cloud native environment using Dockerized containers and managing them with Kubernetes and other tools, they create a highly automated system to deal with the deployment volume. While automation simplifies deployment, it can also leave companies vulnerable to host of issues. For example, if a malicious actor were to get control of the process via a code injection attack, they could cause a lot of problems without anyone knowing about it. Twistlock is built to help prevent that, while also helping customers recognize when an exploit happens and performing forensic analysis to figure out how it happened. It’s is not a traditional Software as a Service as we’ve come to think of it. Instead, it is a service that gets installed on whatever public or private cloud that the customer is using. So far, they count just over 200 customers including Walgreens and Aetna and a slew of other companies you would definitely recognize, but they couldn’t name publicly. The company, which was founded in 2015, is based in Portland, Oregon with their R&amp;D arm in Israel. They currently have 80 employees. Bernstein said from a competitive standpoint, the traditional security vendors are having trouble reacting to cloud native, and while he sees some startups working at it, he believes his company has the most mature offering, at least for now. “We don’t have a lot of competition right now, but as we start progressing we will see more,” he said. He plans to use the money they receive today to help expand their marketing and sales arm to continue growing their customer base, but also engineering to stay ahead of that competition as the cloud-native security market continues to develop.","As the world shifts to a cloud native approach, the way you secure applications as they get deployed is changing too. Twistlock, a company built from the ground up to secure cloud native environments, announced a $33 million Series C round today led by Iconiq…",2018-08-15T13:00:22Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Twistlock snares $33 million Series C investment to secure cloud native environments,http://techcrunch.com/2018/08/15/twistlock-snares-33-million-series-c-investment-to-secure-cloud-native-environments/,https://techcrunch.com/wp-content/uploads/2018/08/GettyImages-737369061.jpg?w=568,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Here’s some unusual news: AWS, Amazon’s cloud computing arm, today announced that it plans to bring its Relational Database Service (RDS) to VMware, no matter whether that’s VMware Cloud on AWS or a privately hosted VMware deployment in a corporate data center. While some of AWS’s competitors have long focused on these kinds of hybrid cloud deployments, AWS never really put the same kind of emphasis on this. Clearly, though, that’s starting to change — maybe in part because Microsoft and others are doing quite well in this space. “Managing the administrative and operational muck of databases is hard work, error-prone and resource intensive,” said AWS CEO Andy Jassy. “It’s why hundreds of thousands of customers trust Amazon RDS to manage their databases at scale. We’re excited to bring this same operationally battle-tested service to VMware customers’ on-premises and hybrid environments, which will not only make database management much easier for enterprises, but also make it simpler for these databases to transition to the cloud.” With Amazon RDS on VMware, enterprises will be able to use AWS’s technology to run and manage Microsoft SQL Server, Oracle, PostgreSQL, MySQL and MariaDB databases in their own data centers. The idea here, AWS says, is to make it easy for enterprises to set up and manage their databases wherever they want to host their data — and to then migrate it to AWS when they choose to do so. This new service is currently in private preview, so we don’t know all that much about how this will work in practice or what it will cost. AWS promises, however, that the experience will pretty much be the same as in the cloud and that RDS on VMware will handle all the updates and patches automatically. Today’s announcement comes about two years after the launch of VMware Cloud on AWS, which was pretty much the reverse of today’s announcement. With VMware Cloud on AWS, enterprises can take their existing VMware deployments and take them to AWS.","Here’s some unusual news: AWS, Amazon’s cloud computing arm, today announced that it plans to bring its Relational Database Service (RDS) to VMware, no matter whether that’s VMware Cloud on AWS or a privately hosted VMware deployment in a corporate data cente…",2018-08-27T17:35:15Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",VMware pulls AWS’s Relational Database Service into the data center,http://techcrunch.com/2018/08/27/vmware-pulls-awss-relational-database-service-into-the-data-center/,https://techcrunch.com/wp-content/uploads/2017/11/mvimg_20171129_165124.jpg?w=654,techcrunch,TechCrunch,cloud,1
Romain Dillet,"French startup Blade, the company behind Shadow, is updating its physical box that lets you connect to your cloud computer instance. Shadow Ghost is a tiny device that provides all the ports and wireless technologies that you need to plug to a TV or a monitor and start playing. Shadow has been building a cloud computing service for gamers. For $35 per month, you get a gaming PC in a data center near you. Shadow gives you 8 threads on an Intel Xeon 2620 processor, an Nvidia Quadro P5000 GPU that performs more or less as well as an Nvidia GeForce GTX 1080, 12GB of RAM and 256GB of storage. It’s a full Windows 10 instance and you can do whatever you want with it. The company started with a dedicated box from day one. The first Shadow box was an oddly-shaped black box with a few USB ports and DisplayPorts. This way, you could replace your PC at home with this box and use the same peripherals. When you turn it on, it feels like you’re booting up your gaming PC, but you’re actually just starting a computer with a low-powered CPU that connects to your gaming PC in the cloud. Over the past few months, Shadow has slowly decorrelated the service from the physical device in your home. When you subscribe, you don’t get a box by default. You can install the Shadow app on your existing computer, phone or tablet and start playing. If you still want the box to connect to your Shadow instance without an existing PC, you can rent it for $10 per month or purchase it for $140. It could be particularly useful for a TV for instance. Compared to the previous generation, Shadow Ghost is completely silent as the fan is gone — that was my main complaint with the first Shadow box. You won’t need as many dongles either as there’s an HDMI port by default (instead of a DisplayPort) and it supports both Wi-Fi and Bluetooth. It’s also much more energy efficient as it should consume three times less power than the existing Shadow device. Shadow Ghost will be available for the same price at some point during the last quarter of 2018. The service itself is currently available in France, Germany, the U.K., Belgium, Switzerland and Luxembourg. In the U.S., the company has a data center near San Francisco and another one on the East Coast.","French startup Blade, the company behind Shadow, is updating its physical box that lets you connect to your cloud computer instance. Shadow Ghost is a tiny device that provides all the ports and wireless technologies that you need to plug to a TV or a monitor…",2018-08-23T13:57:34Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Shadow announces a new box for its cloud gaming service,http://techcrunch.com/2018/08/23/shadow-announces-a-new-box-for-its-cloud-gaming-service/,https://techcrunch.com/wp-content/uploads/2018/08/c6034104ee78d02f9dc0ecb58ceef00a0344cd77_5.jpg?w=748,techcrunch,TechCrunch,cloud,1
Ron Miller,"Spotinst, the startup that helps companies purchase and manage excess cloud infrastructure capacity, announced a hefty $35 million Series B today led by Highland Capital. Existing investors Leaders Fund, Intel Capital and Vertex Ventures also participated. Today’s round brings the total investment to over $52 million. Cloud infrastructure vendors like Amazon Web Services, Microsoft Azure and Google Cloud Platform run massive data centers to have enough capacity at any given moment to respond to customer demand. That means there are always going to be some machines sitting idle. To make use of this excess capacity, the vendors offer deep discounts of up to 80 percent, but there’s a catch. If the vendor needs that virtual machine at any given moment, the discount customers are going to get kicked off. That leaves developers wary of putting anything critical on the discounted servers, no matter how much they are saving. That’s where Spotinst comes in. “With machine learning and artificial intelligence, Spotinst can predict trends of availability. We know how long an instance will live and we can smoothly move our customers from one instance to another, allowing them to run complex or mission critical applications,” Spotinst co-founder and CEO Amiram Shachar told TechCrunch. He sees the two trends of developers moving toward serverless and containerization really helping to drive his business growth. The company announced support for Lambda, AWS’s serverless product, last fall and they are also seeing a big rise in the use of containers. “What we’ve seen in the past six months is that our containers offering is growing exponentially month over month. And as customers are deploying containers we’re able to run them on excess capacity, and save them huge amounts of money,” he explained. Spotinst management console. Screenshot: Spotinst. Shachar is clear that they are not offering a brokerage service here. Instead, his customers sign up for Spotinst as a cloud service, and his company makes money by taking a percentage of the money customers save by using this spot capacity. The company began by working with AWS spot instances, but has since expanded its market to include Google and Microsoft extra capacity as well. In the future, depending on their requirements, customers could potentially move across clouds seamlessly if they wish, moving to wherever the best available price is at any given moment, using Spotinst to manage the transitions. While that’s not something they offer now, it is on the roadmap, he says. It’s worth noting that just yesterday, VMware bought CloudHealth Technologies, a company that helps customers manage a multi-cloud environment from a single console. Shachar acknowledges that a company like his could be also be an attractive target for a large company, but he and his co-founders are only looking toward building the business and continuing to improve the product. The company currently has 100 employees, but with the additional investment, Shachar expects to double that in the next year between their U.S. office in San Francisco and their engineering office in Tel Aviv.","Spotinst, the startup that helps companies purchase and manage excess cloud infrastructure capacity, announced a hefty $35 million Series B today led by Highland Capital. Existing investors Leaders Fund, Intel Capital and Vertex Ventures also participated. To…",2018-08-28T13:08:10Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","Spotinst, excess cloud capacity management service, snares $35M Series B",http://techcrunch.com/2018/08/28/spotinst-excess-cloud-capacity-management-service-snares-35m-series-b/,https://techcrunch.com/wp-content/uploads/2018/08/GettyImages-521014210.jpg?w=567,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"It’s KubeCon/CloudNativeCon this week, the world’s largest confab for all things cloud-native, containers, Kubernetes and DevOps. Every company that’s doing anything remotely related to those topics is announcing news at the sold-out event. That includes the popular cloud hosting service DigitalOcean, which is announcing the launch of its Kubernetes-as-a-Service offering to all developers today. This is still a limited release, though, with full general availability planned for early 2019. DigitalOcean’s service first launched into early access in May. In total, about 30,000 developers singed up for early access and the team now feels that it’s ready for a wider rollout. Like all of the company’s service, the focus here is on simplicity. By default, there’s nothing all that simple about setting up and managing Kubernetes clusters, but DigitalOcean has abstracted away most of this and promises that the service is “production-ready” for “developers of all skill levels.” The early access release of the service already introduced most of the basics like node provisioning, handling durable storage, firewall, load balancing and similar tools. This new release adds open APIs for integrations with existing developer tools, support for the latest versions of Kubernetes (with 1.13.1 support coming soon), as well as a new configuration experience that guides developers through the process of provisioning, configuring and deploying new clusters. “Kubernetes promises to be one of the leading technologies in a developers arsenal to gain the scalability, portability and availability needed to build modern apps. Unfortunately, for many its extremely complex to manage and deploy,” said DigitalOcean VP of Product Shiven Ramji in today’s announcement. “With DigitalOcean Kubernetes, we make running containerized apps consumable for any developer, regardless of their skills or resources.” While Digital Ocean started out as a standard hosting company with virtual private servers, the company has recently expanded its portfolio to the point where it now looks more like a nascent cloud computing company with a set of offerings that include virtual machines, a storage service and load balancing tools. Kubernetes container support is a logical next step now that it has those pieces in place. And while the Kubernetes market often focuses on large enterprises, there’s plenty of room to grow for a company like DigitalOcean that focuses on individual developers and smaller companies — and they, too, would like to have an easy way to use and manage containers.","It’s KubeCon/CloudNativeCon this week, the world’s largest confab for all things cloud-native, containers, Kubernetes and DevOps. Every company that’s doing anything remotely related to those topics is announcing news at the sold-out event. That includes the …",2018-12-11T16:00:28Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Digital Ocean launches its container service,http://techcrunch.com/2018/12/11/digital-ocean-launches-its-container-service/,https://techcrunch.com/wp-content/uploads/2017/10/gettyimages-157113429.jpg?w=597,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Solo.io, a Cambridge, MA-based startup that helps enterprises adopt cloud-native technologies, is coming out of stealth mode today and announcing both its Series A funding round and the launch of its Gloo Enterprise API gateway. Redpoint Ventures led the $11 million Series A round, with participation from seed investor True Ventures. Like most companies at the Series A state, Solo.io plans to use the money to invest in the product development of its enterprise and open source tools, as well as to grow its sales and marketing teams. Solo.io offers a number of open source tools like the Gloo function gateway, the Sqoop GraphQL server and the SuperGloo (see a theme here?) service mesh orchestration platform. In addition, the team has also, among others, open-sourced its Kubernetes debugger, a tool for building and running unikernels. Its first commercial offering, though, is an enterprise version of the Gloo function gateway. Built on top of the Envoy proxy, Gloo can handle the routing necessary to connect incoming API requests to microservices, serverless applications (on the likes of AWS Lambda) and traditional monolithic applications behind the proxy. Gloo handles the load balancing and other functions necessary to aggregate the incoming API requests and route them to their destinations. “Costumers who use Gloo to connect between microservices and serverless found that invocation of [AWS] Lambda is 350ms faster than the AWS API Gateway,” Idit Levine, the founder and CEO of Solo.io, told me. “Gloo also offers them direct money saving, since AWS bills per invocation. In general, Gloo offers money saving because it allows our clients to use the less expensive technologies — like their legacy apps, and sometimes containers — whenever they can, and limit the use of more expensive stuff to whenever it’s necessary.” The enterprise version adds features like audit controls, single sign-on and more advanced security tools to the platform. In addition to broadening its customer base, the company plans to invest heavily into its customer success and support teams, as well as its evangelism and education efforts, Levine tells me. Helping enterprises easily adopt innovative technologies like microservices, serverless and service mesh is our goal at Solo.io, Levine in today’s announcement. Melding different technologies into one coherent environment, by supplying a suite of tools to route, debug, manage, monitor and secure applications, lets organizations focus on their software without worrying about the complexity of the underlying environment.","Solo.io, a Cambridge, MA-based startup that helps enterprises adopt cloud-native technologies, is coming out of stealth mode today and announcing both its Series A funding round and the launch of its Gloo Enterprise API gateway. Redpoint Ventures led the $11 …",2018-12-10T16:00:35Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Solo.io raises $11M to help enterprises adopt cloud-native technologies,http://techcrunch.com/2018/12/10/solo-io-raises-11m-to-help-enterprises-adopt-cloud-native-technologies/,https://techcrunch.com/wp-content/uploads/2018/12/GettyImages-pha206000015.jpg?w=566,techcrunch,TechCrunch,cloud,1
Russell Brandom,"An affidavit unsealed today shows Apple’s unexpected role in bringing a Russian spam king to justice. Once listed as one of the ten worst spammers in the world, Peter Levashov allegedly ran the Kelihos botnet under the alias “Severa,” renting out access to spammers and other cybercriminals. But despite Levashov’s significant efforts at anonymity, court records show that federal agents had been surveilling his iCloud account since May 20th, 2016, funneling back crucial information that may have led to his arrest. The affidavit (embedded below) lays out Severa’s role in administering the Kelihos spam botnet, and how server records, Jabber messages and online payments led investigators to Levashov. Two Kelihos-linked servers seized in Luxembourg showed frequent logins to Levashov’s mail.ru account, apparently the result of Levashov using the servers as a proxy. Investigators also found an iCloud account registered in Levashov’s name, registered from an IP address that had often connected to the Luxembourg server. The affidavit requests information on the account — including “login IP addresses associated with session times and dates” — based on Levashov’s apparent connection to Kelihos spam empire. The request was successful. The same day the request was filed, a warrant was granted and Apple was placed under a gag order forbidding the company from sharing information about the case. With Levashov in Russia, the case was put on hold until he traveled to an extraditable country. The following April, nearly a year after the warrant was served, Levashov traveled to Barcelona on vacation and was promptly arrested by local authorities, reportedly at the request of US law enforcement. We still don’t know for sure how US officials became aware that Levashov was in Barcelona. However, the standing iCloud warrant would have given authorities a running tab of IP addresses used to log in to the account, which could easily have tipped them off to the vacation. The affidavit was made public in the wake of Levashov’s extradition from Spain to the United States, which was only recently finalized. Levashov was arraigned in Connecticut federal court on Friday, and US attorney Bryan Schroder requested that the court unseal the affidavit in connection with that proceeding. Reached by The Verge, an Apple representative said the company does not comment on law enforcement proceedings. US v. Levashov indictment by Russell Brandom on Scribd","Apple’s cloud storage system helped the FBI find the man behind the Kelihos botnet, according to a newly unsealed affidavit.",2018-02-05T23:04:50Z,"{'id': 'the-verge', 'name': 'The Verge'}",Feds tracked down Russian spam kingpin with help from his iCloud account,https://www.theverge.com/2018/2/5/16975896/icloud-warrant-kelihos-botnet-levashov-affidavit,https://cdn.vox-cdn.com/thumbor/SBG3ciGoz9l7iAyeJfyd80XJ1MU=/0x146:2040x1214/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/8726987/acastro_170621_1777_0004_fin.jpg,the-verge,The Verge,cloud,1
Tom Simonite,"Googles attempt to wrest more cloud computing dollars from market leaders Amazon and Microsoft got a new boss late last year. Next week, Thomas Kurian is expected to lay out his vision for the business at the companys cloud computing conference, and build on his predecessors strategy of emphasizing Googles strength in artificial intelligence.
That strategy is complicated by controversies over how Google and its clients use the powerful technology. After employee protests over a Pentagon contract in which Google trained algorithms to interpret drone imagery, the cloud unit now subjects itsand its customersAI projects to ethical reviews. They have caused Google to turn away some business. There have been things that we have said no to, says Tracy Frey, director of AI strategy for Google Cloud, although she declines to say what.
But this week, the company fueled criticism that those mechanisms cant be trusted when it fumbled an attempt to introduce outside oversight over its AI development.
Googles ethics reviews tap a range of experts. Frey says product managers, engineers, lawyers, and ethicists assess proposed new services against Googles AI principles. Some new products announced next week will come with features or limitations added as a result.
Last year, that process led Google not to launch a facial recognition service, something that rivals Microsoft and Amazon have done. This week, more than 70 AI researchersincluding nine who work at Googlesigned an open letter calling on Amazon to stop selling the technology to law enforcement.
Frey says that tricky decisions over howor whetherto release AI technology will become more common as the technology advances.
In February, San Francisco research institute OpenAI said it would not release new software it created that is capable of generating surprisingly fluent text because it might be used maliciously. The episode was dismissed by some researchers as a stunt, but Frey says it provides a powerful example of the kind of restraint needed as AI technology gets more powerful. We hope to be able to have that same courageous stance, she says. Google said last year that it modified research on lip-reading software to minimize the risk of misuse. The technology could help the hard of hearingor be used to infringe on privacy.
Not everyone is convinced that Google itself can be trusted to make ethical decisions about its own technology and business.
Googles AI principles have been criticized as too vague and permissive. Weapons projects are banned but military work is still allowed. The principles say Google will not pursue technologies whose purpose contravenes widely accepted principles of international law and human rights, but the company has been testing a search engine for China which if launched would have to perform political censorship.
Since Google revealed its AI principles, the company has been dogged by questions about how they would be enforced without external oversight. Last week Google announced a panel of eight outsiders it said would help implement the principles. Late Thursday it said that new Advanced Technology External Advisory Council was being shut down and that the company was going back to the drawing board.
The U-turn came after thousands of Google employees signed a petition protesting the inclusion of Kay Coles James, president of conservative think tank the Heritage Foundation. She worked on President Trumps transition team and has spoken against policies aimed at helping trans and LGBTQ people. As the controversy grew, one council member resigned and another, Oxford University philosopher Luciano Floridi, said Google had made a grave error in appointing James.
Os Keyes, a researcher at the University of Washington who joined hundreds of outsiders in signing the Googlers petition protesting James inclusion, says the episode suggests Google cares more about currying political favor with conservatives than the impact of AI technology. The idea of responsible AI as practiced by Google is not actually responsible, Keyes says. They mean not harmful, unless harm makes money.
Anything that adds friction to new products or deals could heighten Kurians challenge. He took over at Google Cloud last year after the departure of Diane Greene, a storied engineer and executive who led a broad expansion of the unit after joining in 2016. Although Googles cloud business made progress during Greenes tenure, Amazons and Microsofts did too. Oppenheimer estimates that Google has 10 percent of the cloud market, well behind Amazons 45 percent and Microsofts 17 percent.
Google is not the only big company talking more about AI ethics lately. Microsoft has its own internal ethical review process for AI deals and also says it has turned down some AI projects. Frey says such reviews dont have to slow down a business and that Googles ethical AI checkups can generate new business because of growing awareness of the risks that come with AIs power. Google Cloud needs to encourage trust in AI to succeed in the long term, she says. If that trust is broken at any point we run the risk of not being able to realize the important and valuable effects of AI being infused in enterprises around the world, Frey says.
More Great WIRED Stories","Google trails Amazon and Microsoft in selling cloud services to other companies, but its efforts to catch up may be hindered by its own ethics policies.",2019-04-05T19:12:00Z,"{'id': 'wired', 'name': 'Wired'}",Google Needs to Grow Its Cloud Business—Very Carefully,https://www.wired.com/story/google-needs-grow-cloud-business-carefully/,https://media.wired.com/photos/5ca6696c79a1dd495e6dd94c/191:100/pass/Google-Cloud-1068871874.jpg,wired,Wired,cloud,1
Frederic Lardinois,"Cloud Spanner, Google’s globally distributed relational database service, is getting a bit more distributed today with the launch of a new region and new ways to set up multi-region configurations. The service is also getting a new feature that gives developers deeper insights into their most resource-consuming queries.
With this update, Google is adding to the Cloud Spanner lineup Hong Kong (asia-east2), its newest data center location. With this, Cloud Spanner is now available in 14 out of 18 Google Cloud Platform (GCP) regions, including seven the company added this year alone. The plan is to bring Cloud Spanner to every new GCP region as they come online.
The other new region-related news is the launch of two new configurations for multi-region coverage. One, called eur3, focuses on the European Union, and is obviously meant for users there who mostly serve a local customer base. The other is called nam6 and focuses on North America, with coverage across both costs and the middle of the country, using data centers in Oregon, Los Angeles, South Carolina and Iowa. Previously, the service only offered a North American configuration with three regions and a global configuration with three data centers spread across North America, Europe and Asia.
While Cloud Spanner is obviously meant for global deployments, these new configurations are great for users who only need to serve certain markets.
As far as the new query features are concerned, Cloud Spanner is now making it easier for developers to view, inspect and debug queries. The idea here is to give developers better visibility into their most frequent and expensive queries (and maybe make them less expensive in the process).
In addition to the Cloud Spanner news, Google Cloud today announced that its Cloud Dataproc Hadoop and Spark service now supports the R language, in addition to Python 3.7 support on App Engine.","Cloud Spanner, Google’s globally distributed relational database service, is getting a bit more distributed today with the launch of a new region and new ways to set up multi-region configurations. The service is also getting a new feature that gives develope…",2018-12-19T18:18:41Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google’s Cloud Spanner database adds new features and regions,http://techcrunch.com/2018/12/19/googles-cloud-spanner-database-adds-new-features-and-regions/,https://techcrunch.com/wp-content/uploads/2017/02/google-cloud-spanner.jpg?w=711,techcrunch,TechCrunch,cloud,1
Ron Miller,"Okta and fellow cloud company ServiceNow got together to build an app that helps ServiceNow customers using their security operations tools find security issues related to identity and take action immediately. The company launched the Okta Identity Cloud for Security Operations app today. It’s available in the ServiceNow app store and has been designed for customers who are using both toolsets. When a customer downloads and installs the app, it adds a layer of identity information inside the ServiceNow security operations interface, giving the operations team access to information about who specifically is involved with a security problem without having to exit their tool to find the information. Okta is a cloud identity management company, while ServiceNow is a cloud service management company. ServiceNow approached Okta about this integration because research has shown that that vast majority of breaches are related to compromised user credentials. The sooner the security operations team can track down the source of those credentials, the sooner they can begin resolving situation. The way it works is a company detects a breach through the ServiceNow security tool. Instead of searching logs and and taking days or weeks to find the source of the breach, security operations can see the problem user directly in the ServiceNow interface. With that information, they can decide immediately how to mitigate the issue. That could involve forcing the person to log out of all their applications and logging back in with new credentials and two-factor identification, suspending the user for 24 hours or a number of other actions at the discretion of the security personnel. Okta identity tools in the ServiceNow interface. Screenshot: ServiceNow The combination of the two products results in a better solution for customers who are using both tools anyway, says Okta COO and co-founder Frederic Kerrest. “It reduces incident triage, improves risk scoring and accelerates containment,” he explained. The integration takes advantage of the Okta Advanced Integration Network and involves a set of APIs for for inserting Okta functionality inside of other applications. Among the other companies Okta is working with on this kind of integration is Palo Alto Networks. This is not the first time the two companies have worked together, says Kerrest. There have been a couple of other cases where ServiceNow has used Okta as the default identity management solution in their products. Featured Image: nicescene/Getty Images",Okta and fellow cloud company ServiceNow got together to build an app that helps ServiceNow customers using their security operations tools find security issues related to identity and take action immediately. The company launched the Okta Identity Cloud for …,2018-01-18T13:22:50Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Okta teams up with ServiceNow to bring identity layer to breach containment,http://techcrunch.com/2018/01/18/okta-teams-up-with-servicenow-to-bring-identity-layer-to-breach-containment/,https://tctechcrunch2011.files.wordpress.com/2018/01/gettyimages-821300022.jpg,techcrunch,TechCrunch,cloud,1
Ron Miller,"Earlier this week news broke of a pair of massive chip vulnerabilities dubbed Spectre and Meltdown (for an explainer see this post ). We learned that the larger cloud vendors like Amazon, Google and Microsoft have been in touch with chip vendors and have been working behind the scenes to mitigate the vulnerabilities. But what about smaller cloud hosting vendors like Linode, OVH and Packet who were not in the inner circle? How were they coping with it? Details have begun to emerge. These companies left on the outside looking in have been forced to scramble to find answers for their hundreds of thousands of customers and to find ways to protect them from this massive threat. Executives from these “Tier 2” vendors began informally contacting one another when the news of the threats broke on Wednesday. Much like the children’s book Swimmy by Leo Lionni, the companies realized by banding together they could behave like a much bigger fish. “Then suddenly he said, “I have it!” “We are going to swim all together like the biggest fish in the sea!” ~ Leo Lionni, Swimmy According to information provided by the companies, the contact began when Edouard Bonlieu, VP for marketing strategy at French hosting provider Scaleway, reached out to Packet CEO, Zack Smith about an information sharing arrangement. Bonlieu had already contacted fellow French provider OVH. Eventually six cloud providers — Scaleway, DigitalOcean, Packet, Vultur, Linode and OVH — formed a consortium of sorts to help one another and share information. In order to make the process more efficient, they started a Slack channel with CEOs, CTOs and engineers from the various companies sharing information and fixes as they became available. This approach has allowed them to get information much more quickly, taking advantage of modern enterprise communications tools like Slack. “Not being part of the select group that received advance notice of Meltdown and Spectre, we’ve been playing catch up. Banding together with the folks at Scaleway, OVH and others has allowed us to short circuit a painfully slow game of telephone and provide our customers with as much detail as possible as they try to understand where and how their systems may be vulnerable,” Nathan Goulding, Packet’s SVP of engineering explained. Yann Léger, SVP at Scaleway added, “We discovered these vulnerabilities in the press before the full disclosure and started to put pressure on manufacturers as fast as we could to fully understand the situation. Working with other cloud players is one of the best decisions we’ve made so far to mitigate the issue in the most accurate way,” he said. The companies also sent out tweets and wrote blog posts to keep their customers updated with the latest information they had. This was a case where cloud companies that normally compete hard with one another had to work together for the good of all in the true spirit of cloud computing cooperation. The large vendors have a direct pipeline to the various parties providing patches, fixes and detailed information. The Tier 2 vendors didn’t have that luxury and this level of cooperation appears to be helping them cope with an extremely difficult situation. Featured Image: Johanna Parkin/Getty Images","Earlier this week news broke of a pair of massive chip vulnerabilities dubbed Spectre and Meltdown (for an explainer see this post). We learned that the larger cloud vendors like Amazon, Google and Microsoft have been in touch with chip vendors and have been …",2018-01-06T18:44:52Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",How Tier 2 cloud vendors banded together to cope with Spectre and Meltdown,http://techcrunch.com/2018/01/06/how-tier-2-cloud-vendors-banded-together-to-cope-with-spectre-and-meltdown/,https://tctechcrunch2011.files.wordpress.com/2018/01/gettyimages-182108931.jpg,techcrunch,TechCrunch,cloud,1
Ron Miller,"In a world shifting to the cloud, Microsoft has carved out a place trying to help companies caught between two worlds — on-prem legacy solutions and the public and private cloud. To help further that hybrid mission, the company announced it was acquiring Avere Systems today for an undisclosed amount. Microsoft describes Avere as “a leading provider of high-performance NFS and SMB file-based storage for Linux and Windows clients running in cloud, hybrid and on-premises environments.” That’s a mouthful, but essentially the company has focused on maximizing storage performance, especially in more expensive flash storage, regardless of where you store your files. Avere’s president and CEO Ronald Bianchini Jr., wrote in a company blog post that the company has worked hard since its inception to provide highly efficient file storage solutions. “Our customers efficiently share both storage and compute resources across multiple data centers, and effectively implement and use private and public cloud infrastructures,” Bianchini wrote. That is precisely a focus of Microsoft’s cloud strategy, so it appears to be an excellent fit. In a blog post announcing the acquisition, Microsoft’s corporate vice president for Microsoft Azure, Jason Zander, described Avere’s approach. “Avere uses an innovative combination of file system and caching technologies to support the performance requirements for customers who run large-scale compute workloads.” This could be particularly useful for high-performance media customers like Sony Pictures Imageworks, animation studio Illumination Mac Guff and Moving Picture Company (MPC). But it doesn’t stop there. Zander wrote that the Avere technology could also be useful in life sciences, education, oil and gas, financial services and manufacturing or anywhere companies could benefit from more efficient use of high performance storage and compute resources. Microsoft expects the deal to be finalized in the coming months subject to the customary approval process in these kinds of deals. The company has indicated that the Avere team will be coming on board as part of the deal and they will retain their Pittsburgh offices. As Avere becomes part of the Microsoft Azure family, it’s unclear what impact it will have on existing customers, but there is always some adjustment when a company gets acquired in this fashion. Avere was founded in 2008 and has raised $86 million with its most recent investment, a $14 million Series E coming last year in March. Investors in the E round included Google and Western Digital, but it was Microsoft who comes away with the prize. Previous investors included Lightspeed Ventures, Tenaya Capital, Menlo Ventures and Norwest Venture Partners. Featured Image: JASON REDMOND/Getty Images","In a world shifting to the cloud, Microsoft has carved out a place trying to help companies caught between two worlds — on-prem legacy solutions and the public and private cloud. To help further that hybrid mission, the company announced it was acquiring Aver…",2018-01-03T15:01:00Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Microsoft acquires Avere Systems to further hybrid computing mission,http://techcrunch.com/2018/01/03/microsoft-acquires-avere-systems-to-further-hybrid-computing-mission/,https://tctechcrunch2011.files.wordpress.com/2018/01/gettyimages-881858762.jpg,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"IBM today announced a partnership with low-code development platform Mendix that will bring Mendix and native integration with many of IBM’s Watson IoT and AI services to the IBM Cloud. This deal is an evolution of a previous partnership that involved what was then called IBM Bluemix (now IBM Cloud). With this partnership, IBM is bringing Mendix to the IBM Cloud and integrating it deeply with its own tools. As Don Boulia, the general manager for the IBM Cloud Platform, told me, the typical developer on the IBM platform doesn’t quite fit the profile of somebody who would opt for a low-code/no-code platform like Mendix, but this move allows IBM to go after a segment of the developer population that includes exactly this group of users who currently don’t have the option/ability to use IBM’s cloud services (as well as more advanced developers who simply want to build a quick prototype). By bringing Mendix to its cloud, IBM clearly hopes to attract more users to the wide variety of services it now offers. Specifically, this includes deep integrations with Watson AI services like Conversation for building chatbots, translation services, text to speech, the Watson Tone Analyzer for understanding emotions in text and the Watson Visual Recognition service. Using the Mendix connector kit, developers also will be able to access IBM’s IoT and blockchain services. Once you start getting into IBM’s IoT and blockchain APIs, though, chances are that you’ll have to go beyond the visual development tools to get the full value out of these tools. The promise of Mendix is that it speeds up application development by 10x (for some reason, all low-code platforms seem to make the same 10x promise…). IBM also notes that the service allows developers to easily deploy their applications in the IBM Cloud as a container or through platforms like Cloud Foundry.",IBM today announced a partnership with low-code development platform Mendix that will bring Mendix and native integration with many of IBM’s Watson IoT and AI services to the IBM Cloud. This deal is an evolution of a previous partnership that involved what wa…,2018-01-25T14:00:54Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",IBM brings Mendix’s low-code platform to its cloud,http://techcrunch.com/2018/01/25/ibm-brings-mendixs-low-code-platform-to-its-cloud/,https://tctechcrunch2011.files.wordpress.com/2018/01/offering_7240c5cc-62f1-45b8-8d8f-b9146d76d2ab.png,techcrunch,TechCrunch,cloud,1
Ron Miller,"Several cloud vendors began responding to the chip kernel vulnerability that has the industry reeling today. Each Infrastructure as a Service vendor clearly has a stake here because each one is selling CPU cycles on their platforms. TechCrunch sent a request for comment to six major cloud vendors including AWS, Microsoft, Google, IBM, Rackspace and DigitalOcean. At the time of publication, we had heard directly from three of the companies: Microsoft, Rackspace and DigitalOcean. In the case of Google, we learned their response indirectly through a published blog post. We have not yet heard from AWS or IBM. Microsoft “We’re aware of this industry-wide issue and have been working closely with chip manufacturers to develop and test mitigations to protect our customers. We are in the process of deploying mitigations to cloud services and are releasing security updates today to protect Windows customers against vulnerabilities affecting supported hardware chips from AMD, ARM and Intel. We have not received any information to indicate that these vulnerabilities had been used to attack our customers.” Google “As soon as we learned of this new class of attack, our security and product development teams mobilized to defend Google’s systems and our users’ data. We have updated our systems and affected products to protect against this new type of attack. We also collaborated with hardware and software manufacturers across the industry to help protect their users and the broader web. These efforts have included collaborative analysis and the development of novel mitigations.” (See here, here, here and here for additional blog posts from Google outlining their responses.) DigitalOcean “DigitalOcean has been actively investigating the Intel chip issue which was disclosed earlier today. We’ve been working to gather as much information as we can to ensure our customers remain protected. Intel unfortunately has not made it easy to get a full picture of the issue due to their information embargo. At this time we are working under the assumption that this flaw will impact all of our customers and we’re presuming that rebooting Droplets (a DigitalOcean cloud server) will be necessary. We will be providing advanced notification to any and all customers impacted as we learn more. This is a developing issue and we are unable to forecast timeframes for implementing a fix at this time.” (See DigitalOcean’s blog pos t for additional details.) Rackspace “On 2 January 2018, Rackspace was made aware of a suspected Intel CPU architecture vulnerability. The full extent and performance impact of this vulnerability and potential remediation are currently unknown as the vulnerability has not yet been publicly disclosed. Our engineers are engaging with the appropriate vendors and reviewing the Rackspace environment and will take appropriate action. Should actions that would impact customer environments be taken Rackspace will communicate to affected customers.” Should additional responses become available, we will continue to update this article. Featured Image: Terry Why/Getty Images",Several cloud vendors began responding to the chip kernel vulnerability that has the industry reeling today. Each Infrastructure as a Service vendor clearly has a stake here because each one is selling CPU cycles on their platforms. TechCrunch sent a request …,2018-01-04T00:57:47Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Cloud infrastructure vendors begin responding to chip kernel vulnerability,http://techcrunch.com/2018/01/03/cloud-infrastructure-vendors-begin-responding-to-chip-kernel-vulnerability/,https://tctechcrunch2011.files.wordpress.com/2018/01/gettyimages-128243344.jpg,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Microsoft today announced that it is joining the MariaDB Foundation, the nonprofit behind the popular relational database founded by the original developers of MySQL. As a platinum sponsor, the company is joining Booking.com, Alibaba Cloud and Tencent Cloud. In addition, Microsoft is also launching a preview of the Azure Database for MariaDB service today, which is joining Azure Database for MySQL and PostgreSQL in the Azure stable of managed database service. As far as users are concerned, MariaDB is essentially a drop-in replacement for MySQL that isn’t linked to Oracle, which — thanks to its acquisition of Sun, which acquired MySQL — is currently the corporate home of the open-source database. “On behalf of the Board of Directors, the MariaDB Foundation welcomes Microsoft as a Platinum member,” MariaDB (and MySQL) founder Monty Widenius writes in today’s announcement. “I founded MariaDB to give back to the open source community and ensure a strong and open future for MySQL. I’ve seen firsthand how Microsoft has been changing its business in an open way, and how Microsoft Azure is open and flexible. Microsoft is a leading contributor on GitHub and we look forward to engineers from Microsoft and its developer ecosystem supporting MariaDB.” Widenius always had a very pragmatic view of open source. A few years ago, he left SkySQL, which is now the MariaDB Corp. (which commercializes the database) to start the MariaDB Foundation, but then came back to MariaDB Corp. as its CTO. As for Microsoft, it’s worth noting that the company is definitely embracing open source at this point. It’s now a sponsor of the Linux Foundation and some of that group’s projects, as well as the Open Source Initiative, the Cloud Foundry Foundation and others.","Microsoft today announced that it is joining the MariaDB Foundation, the nonprofit behind the popular relational database founded by the original developers of MySQL. As a platinum sponsor, the company is joining Booking.com, Alibaba Cloud and Tencent Cloud. …",2017-11-15T15:00:36Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Microsoft joins the MariaDB Foundation and launches Azure Database for MariaDB,https://techcrunch.com/2017/11/15/microsoft-joins-the-mariadb-foundation-and-launches-azure-database-for-mariadb/,https://s0.wp.com/wp-content/themes/vip/techcrunch-2013/assets/images/techcrunch.opengraph.default.png,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Over the last few years, we’ve seen the launch of a number of open source foundations like the Cloud Native Compute Foundation, the Cloud Foundry Foundation and others. Most of these run under the Linux Foundation, but one of the largest open source foundation outside of that group’s orbit is the OpenStack Foundation, which — at least until now — has solely focused on the development of the OpenStack cloud computing platform. Times are changing, though. After spending the last few days at the bi-annual OpenStack Summit, it’s becoming increasingly clear that the OpenStack Foundation is starting to look beyond the OpenStack platform and that the future of the organization may just look a bit more like the Linux Foundation, though with a more streamlined vision that will have the group focus on open infrastructure projects that align with its current interests but that don’t necessarily need to be part of the OpenStack platform or follow the current guidelines for these projects. This move could send ripple through the open source ecosystem because if it’s successful, it’ll create another organization that will sit next to the Linux Foundation, the Apache Foundation and others, yet with the community support and experience that the OpenStack team has built. It may also set up the OpenStack Foundation to compete for hosting projects that would otherwise have looked at moving to the Linux Foundation, which will could set up an interesting rivalry between the two. Boris Renski, the co-founder and CMO of Mirantis, who made an early bet on OpenStack a few years ago, told me that if there’s a team that can pull this off, OpenStack CTO Mark Collier, executive director Jonathan Bryce and VP or marketing and community services Lauren Sell are the best people to do it. In his view, opening up the Foundation to more projects is exactly the right move, especially now that the project is stable and has the resources and community to tackle new challenges. What would this new Foundation focus on? As Bryce told us, the Foundation plans to look at a couple of focus areas: data center cloud infrastructure, container infrastructure, edge computing (and Collier, too, seems to be especially interested in this), continuous integration/continuous delivery and — potentially — machine learning and AI. There are still plenty of unanswered questions, of course. What kind of license would these new projects follow under, for example? Bryce and Collier hinted at the Apache license, which the OpenStack project has long used, too, but there seems to be some flexibility. And what’s going to be the first project the foundation will bring in? Or will it be incubated within the Foundation? So far, the reaction of the community has mostly echoed that of Renski, though I’ve also heard some skepticism as people worry that this new approach will take some focus away from the current OpenStack project. Bryce, Collier and Sell don’t think so, but all three noted that they would listen to the community. Chances are, though, this new approach will re-invigorate the existing community and infuse some new purpose into the Foundation and the events it organizes. One thing the Foundation did categorically count out, though, is changing its name. Bryce noted that “OpenStack” is a broad enough term to accommodate new projects — and the Linux Foundation is still the Linux Foundation, too, after all, even though Linux is just one of the many projects it now hosts.","Over the last few years, we’ve seen the launch of a number of open source foundations like the Cloud Native Compute Foundation, the Cloud Foundry Foundation and others. Most of these run under the Linux Foundation, but one of the largest open source foundatio…",2017-11-10T14:30:46Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",The OpenStack Foundation starts to look at projects beyond OpenStack,https://techcrunch.com/2017/11/10/the-openstack-foundations-starts-to-look-beyond-openstack/,https://tctechcrunch2011.files.wordpress.com/2017/11/mvimg_20171107_162623.jpg,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google acquired API management service Apigee back in 2016 but it’s been pretty quiet around the service in recent years. Today, however, Apigee announced a number of smaller updates that introduce a few new integrations with the Google Cloud platform, as well as a major new partnership with cloud data management and integration firm Informatica that essentially makes Informatica the preferred integration partner for Google Cloud. Like most partnerships in this space, the deal with Informatica involves some co-selling and marketing agreements, but that really wouldn’t be all that interesting. What makes this deal stand out is that Google is actually baking some of Informatica’s tools right into the Google Cloud dashboard. This will allow Apigee users to use Informatica’s wide range of integrations with third-party enterprise applications while Informatica users will be able to publish their APIs through Apigee and have that service manage them for them. Some of Google’s competitors, including Microsoft, have built their own integration services. As Google Cloud director of product management Ed Anuff told me, that wasn’t really on Google’s roadmap. “It takes a lot of know-how to build a rich catalog of connectors,” he said. “You could go and build an integration platform but if you don’t have that, you can’t address your customers needs.” Instead, Google went to look for a partner who already has this large catalog and plenty of credibility in the enterprise space. Similarly, Informatica’s senior VP and GM for big data, cloud and data integration Ronen Schwartz noted that many of his company’s customers are now looking to move into the cloud and this move will make it easier for Informatica’s customers to bring their services into Apigee and open them up for external applications. “With this partnership, we are bringing the best of breed of both worlds to our customers,” he said. “And we are doing it now and we are making it available in an integrated, optimized way.”","Google acquired API management service Apigee back in 2016 but it’s been pretty quiet around the service in recent years. Today, however, Apigee announced a number of smaller updates that introduce a few new integrations with the Google Cloud platform, as wel…",2018-07-12T20:06:37Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google’s Apigee teams up with Informatica to extend its API ecosystem,http://techcrunch.com/2018/07/12/googles-apigee-teams-up-with-informatica-to-extend-its-api-ecosystem/,https://techcrunch.com/wp-content/uploads/2018/06/IMG_20180610_181025-1.jpg?w=655,techcrunch,TechCrunch,cloud,1
Taylor Hatmaker,"On Thursday, the Pentagon opened bidding for a huge cloud computing contract that could be worth as much as $10 billion. Given its size, the Joint Enterprise Defense Infrastructure contract, known as JEDI, is alluring for major cloud computing companies that might not normally do much business with the Department of Defense. Announced in March, JEDI is structured as a winner-take-all contract with a potential ten year term, though the Pentagon clarified that the original award will span just the first two years, so all ten years aren’t set in stone up front. While it’s not yet sparked the same level of outcry as Google’s AI contract with the Pentagon known Project Maven, JEDI isn’t uncontroversial. The now infamous Project Maven project was a smaller, more specific contract with direct implications for the military’s use of drones, while JEDI is broader and bigger, seeking a vendor to provide cloud services for all branches of the military. Google has since abandoned plans to renew the contract, but Maven was likely a kind of test run for the company in the lead-up to JEDI. While Amazon is largely regarded as the most likely winner, Google, Microsoft, IBM and Oracle are also among the major tech companies expected to throw their hats into the ring. Earlier in the process, it looked possible that companies could band together to form unlikely alliances against the perceived frontrunner, though it appears in the final request for proposal that the Pentagon plans to award the contract to a single company capable of handling it. Interested parties will have until September 17 to submit proposals, so we can certainly expect to hear more from companies in the running and workers who oppose JEDI involvement in the months to come.","On Thursday, the Pentagon opened bidding for a huge cloud computing contract that could be worth as much as $10 billion. Given its size, the Joint Enterprise Defense Infrastructure contract, known as JEDI, is alluring for major cloud computing companies that …",2018-07-27T00:19:20Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","Tech companies can bid on the Pentagon’s $10 billion cloud contract, starting today",http://techcrunch.com/2018/07/26/jedi-10-billion-department-of-defense-bidding/,https://techcrunch.com/wp-content/uploads/2018/04/gettyimages-853699560.jpg?w=729,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"One of Google’s first hardware products was its search appliance, a custom-built server that allowed businesses to bring Google’s search tools to the data behind their firewalls. That appliance is no more, but Google today announced the spiritual successor to it with an update to Cloud Search. Until today, Cloud Search only indexed G Suite data. Now, it can pull in data from a wide variety of third-party services that can run on-premise or in the cloud, too, making the tool far more useful for large businesses that want to make all of their data searchable by their employees. “We are essentially taking all of Google expertise in search and are applying it to your enterprise content,” Google said. One of the launch customers for this new service is Whirlpool, which built its own search portal and indexed over 12 million documents from more than a dozen services using this new service. “This is about giving employees access to all the information from across the enterprise, even if it’s traditionally siloed data whether that’s in a database or a legacy productivity tool and make all of that available in a single index,” Google explained. To enable this functionality, Google is making a number of software adapters available that will bridge the gap between these third-party services and Cloud Search. Over time, Google wants to add support for more services and bring this cloud-based technology on par with what its search appliance was once capable of.","One of Google’s first hardware products was its search appliance, a custom-built server that allowed businesses to bring Google’s search tools to the data behind their firewalls. That appliance is no more, but Google today announced the spiritual successor to…",2018-07-25T16:00:41Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google brings its search technology to the enterprise,http://techcrunch.com/2018/07/25/google-brings-its-search-technology-to-the-enterprise/,https://techcrunch.com/wp-content/uploads/2018/06/GettyImages-932459182-1.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"It’s a big day for Amazon’s EC2 cloud computing service today. Not only can you now run EC2 inside a Snowball Edge device, but the company also announced a bunch of new EC2 instance types in the cloud. Thanks to these new instance types, developers now have access to a new instance type (Z1d) with custom Xeon processors that can run at up to 4.0 GHz, as well as new memory-optimized instances (R5) that run at up to 3.1 GHz and that feature up to 50 percent more CPU power and 60 percent more memory than their predecessors. There are also some bare metal variants of these instances, as well as an R5d version that features local NVMe storage. As Amazon’s Jeff Barr notes in today’s announcement, these new instances types were made possible by AWS’s Nitro system, which allows the company to combine the various building blocks that make up an EC2 instance in new ways. The new Z1d instances are obviously meant for applications that need a lot of compute power (and for applications that can’t make use of GPUs for that). Amazon specifically mentions electronic design automation, high-performance computing for the financial services industry and relational databases. The R5 instances, which can feature up to 96 cores and 768 GiB of memory, are mostly meant for in-memory caching applications and in-memory and big data analytics.","It’s a big day for Amazon’s EC2 cloud computing service today. Not only can you now run EC2 inside a Snowball Edge device, but the company also announced a bunch of new EC2 instance types in the cloud. Thanks to these new instance types, developers now have a…",2018-07-17T16:14:29Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","Amazon’s EC2 gets faster processors, new high-memory instances",http://techcrunch.com/2018/07/17/amazons-ec2-gets-faster-processors-new-high-memory-instances/,https://techcrunch.com/wp-content/uploads/2017/11/img_20171128_215144.jpg?w=533,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"The Google -incubated Go language is one of the fastest growing programming languages today, with about one million active developers using it worldwide. But the company believes it can still accelerate its growth, especially when it comes to its role in writing cloud applications. And to do this, the company today announced Go Cloud, a new open-source library and set of tools that makes it easier to build cloud apps with Go. While Go is highly popular among developers, Google argues that the language was missing a standard library for interfacing with cloud services. Today, developers often have to essentially write their own libraries to use the features of each cloud, but organizations today want to be able to easily move their workloads between clouds. What Go Cloud then gives these developers is a set of open generic cloud APIs for accessing blog storage, MySQL databases and runtime configuration, as well as an HTTP server with built-in logging, tracing and health checking. Right now, the focus is on AWS and Google Cloud Platform. Over time, Google plans to add more features to Go Cloud and support for more cloud providers (and those cloud providers can, of course, build their own support, too). This, Google argues, allows developer teams to build applications that can easily run on any supported cloud without having to re-architect large parts of their applications. As Google VP of developer relations Adam Seligman told me, the company hopes this move will kick off an explosion of libraries around Go — and, of course, that it will accelerate Go’s growth as a language for the cloud.","The Google -incubated Go language is one of the fastest growing programming languages today, with about one million active developers using it worldwide. But the company believes it can still accelerate its growth, especially when it comes to its role in writ…",2018-07-24T21:37:15Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google wants Go to become the go-to language for writing cloud apps,http://techcrunch.com/2018/07/24/google-wants-go-to-become-the-go-to-language-for-writing-cloud-apps/,https://techcrunch.com/wp-content/uploads/2018/07/GettyImages-908368194.jpg?w=750,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"All of the hyper-scale cloud providers love to tout their new customer acquisitions: Google talking about Spotify ; Microsoft signing up various Adobe services for Azure; or AWS working with the likes of GE. That’s a sign of how competitive this market is, despite AWS’s continuing market share leadership. At its annual re:Invent conference in Las Vegas, Amazon’s cloud service today announced that Turner, Time-Warner’s entertainment, sports and news company, is making AWS its preferred cloud provider. Turner’s brands and partners include channels like TBS, TNT, Cartoon Network, CNN and Adult Swim. Turner says that it is bringing “decades of content” to the AWS cloud, including CNN’s 15-petabyte video archive. The company is also moving thousands of virtual machines to AWS and expects to use a wide range of Amazon’s AI technologies to better analyze and extract video metadata to offer its viewers enhanced personalized experiences and, of course, to help its advertisers, content creators and analysts better understand viewing trends (the emphasis here is probably on the advertisers). “We’re changing our entire broadcast technology stack to a fully digital, cloud environment built on AWS, which will enable us to adapt to new video delivery models, as well as provide our viewers with more personalized content and advertisement,” said Turner CTO Jeremy Legg in a canned statement. “Our relationship with AWS and the services they provide are essential to our success. Given that we reach over 80 percent of adults and 70 percent of millennials every month, we needed a cloud provider that has the ability to support massive-scale media businesses like ours which often have spikes in demand across our diverse portfolio.” AWS stressed that Turner isn’t the first media company to make the move to its cloud. Others include the BBC, C-SPAN, Hulu, Netflix, PBS, GoPro, Lionsgate and Spotify (which still keeps a presence on the AWS platform). Featured Image: John Greim/Getty Images","All of the hyper-scale cloud providers love to tout their new customer acquisitions: Google talking about Spotify; Microsoft signing up various Adobe services for Azure; or AWS working with the likes of GE. That’s a sign of how competitive this market is, des…",2017-11-28T14:00:30Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Time-Warner’s Turner opts for AWS as its preferred cloud provider,https://techcrunch.com/2017/11/28/time-warners-turner-opts-for-aws-as-its-preferred-cloud-provider/,https://tctechcrunch2011.files.wordpress.com/2017/11/gettyimages-498133654.jpg,techcrunch,TechCrunch,cloud,1
Ron Miller,"Oracle is learning that it’s hard for enterprise companies born in the data center to make the transition to the cloud, an entirely new way of doing business. Yesterday it reported its earnings and it was a mixed bag, made harder by changing the way the company counts cloud revenue. In its earnings press release from yesterday, it put it this way: “Q4 Cloud Services and License Support revenues were up 8% to $6.8 billion. Q4 Cloud License and On-Premise License revenues were down 5% to $2.5 billion.” Let’s compare that with the language from their Q3 revenue in March : “Cloud Software as a Service (SaaS) revenues were up 33% to $1.2 billion. Cloud Platform as a Service (PaaS) plus Infrastructure as a Service (IaaS) revenues were up 28% to $415 million. Total Cloud Revenues were up 32% to $1.6 billion.” See how they broke out the cloud revenue loudly and proudly in March, yet chose to combine their cloud revenue with license revenue in June. In the post-reporting earnings call, Safra Catz, Oracle Co-CEO, responding to a question from analyst John DiFucci, took exception to the idea that the company was somehow obfuscating cloud revenue by reporting it in this way. “So first of all, there is no hiding. I told you the Cloud number, $1.7 billion. You can do the math. You see we are right where we said we’d be.” She says the new reporting method is due to the new combined licensing products that lets customer use their license on-premise or in the cloud. Fair enough, but if your business is booming you probably want to let investors know about that. They seem to be uneasy about this approach with the stock down over 7 percent today as of publishing this article. Oracle Stock Chart: Google Oracle could of course settle all of this by spelling out their cloud revenue, but instead chose a different path. John Dinsdale, an analyst with Synergy Research, a firm that watches the cloud market was dubious about Oracle’s reasoning. “Generally speaking, when a company chooses to reduce the amount of financial detail it shares on its key strategic initiatives, that is not a good sign. I think one of the justifications put forward is that is becoming difficult to differentiate between cloud and non-cloud revenues. If that is indeed what Oracle is claiming, I have a hard time buying into that argument. Its competitors are all moving in the opposite direction,” he said. Indeed most are. While it’s often hard to tell exactly the nature of cloud revenue, the bigger players have been more open about this. For instance in its most recent earnings report, Microsoft reported its Azure cloud revenue grew 93 percent. Amazon reported its cloud revenue from AWS was up 49 percent to $5.4 billion in revenue, getting very specific about the revenue number. Further you can see from Synergy’s most recent market share cloud growth numbers from the 4th quarter last year, Oracle was lumped in with “the Next 10,” not large enough to register on its own. That Oracle chose not to break out cloud revenue this quarter can’t be seen as a good sign. To be fair, we haven’t really seen Google break out their cloud revenue either with one exception in February. But when the guys at the top of the market shout about their growth, and the guys further down don’t, you can draw your own conclusions.","Oracle is learning that it’s hard for enterprise companies born in the data center to make the transition to the cloud, an entirely new way of doing business. Yesterday it reported its earnings and it was a mixed bag, made harder by changing the way the compa…",2018-06-20T19:02:40Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Oracle could be feeling cloud transition growing pains,http://techcrunch.com/2018/06/20/oracle-could-be-feeling-cloud-transition-growing-pains/,https://techcrunch.com/wp-content/uploads/2017/10/gettyimages-856460700.jpg?w=600,techcrunch,TechCrunch,cloud,1
Ingrid Lunden,"Workday, the cloud-based platform that offers HR and other back-office apps for businesses, is making an acquisition to expand its portfolio of services: it’s buying Adaptive Insights, a provider of cloud-based business planning and financial modelling tools, for $1.55 billion. The acquisition is notable because Adaptive Insights had filed for an IPO as recently as May 17. Workday says that the $1.55 billion price tag includes “the assumption of approximately $150 million in unvested equity issued to Adaptive Insights employees” related to that IPO. This deal is expected to close in Q3 of this year. IPO filings are known to sometimes trigger M&amp;A. Most recently, iZettle was acquired by PayPal just after it filed to go public. Skype once famously was acquired by eBay while it was waiting to IPO. Workday itself went public in 2012 and currently has a market cap of nearly $27 billion. The deal will give Workday another string to its bow, in its attempt to become the go-to place for all for back-office services for its business customers: the company plans to integrate Adaptive Insights’ tools into its existing platform. “Adaptive Insights is an industry leader with its Business Planning Cloud platform, and together with Workday, we will help customers accelerate their finance transformation in the cloud,” said Aneel Bhusri, Co-Founder and CEO, Workday, in a statement. “I am excited to welcome the Adaptive Insights team to Workday and look forward to coming together to continue delivering industry-leading products that equip finance organizations to make even faster, better business decisions to adapt to change and to drive growth.” In the case Adaptive Insights, which says it has ‘thousands’ of customers, its growth mirrors that both of cloud services and specifically about how business intelligence has developed into a distinct software category of its own over the years, with not just the CFO but an army of in-house analysts relying on analytics of a business’ data to help make small and big decisions. “The market opportunity here is huge as the CFO has become a power player in the C-Suite,” CEO Tom Bogan told TechCrunch when it raised $75 million in 2015, when it first passed the billion-dollar mark for its valuation. Bogan previously also held a role as chairman of Citrix. “As a former CFO myself, I have seen this first hand and it is accelerating.” Other examples of this force includes Twitter’s Anthony Noto catapulting from CFO to COO (and is now a CEO running SoFi). Around 25 percent of CEOs at Fortune 500 companies are former CFOs. Adaptive Insights had raised $175 million prior to this. Bogan will stay on and lead the business and report directly to Bhusri. “Joining forces with Workday accelerates our vision to drive holistic business planning and digital transformation for our customers,” said Bogan, in a separate statement. “Most importantly, both Adaptive Insights and Workday have an employee-first and customer-centric approach to developing enterprise software that will only increase the power of the combined companies.”","Workday, the cloud-based platform that offers HR and other back-office apps for businesses, is making an acquisition to expand its portfolio of services: it’s buying Adaptive Insights, a provider of cloud-based business planning and financial modelling tools,…",2018-06-11T12:23:32Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Workday acquires financial modelling startup Adaptive Insights for $1.55B,http://techcrunch.com/2018/06/11/workday-acquires-financial-modelling-startup-adaptive-insights-for-1-55b/,https://techcrunch.com/wp-content/uploads/2018/04/gettyimages-165959955.jpg?w=539,techcrunch,TechCrunch,cloud,1
Virginia K. Smith,"When we last rounded up our staff’s Weekly Upgrades, our editors were improving our cloud storage, putting more money into our retirement funds, and re-discovering the joys of Pokémon GO. This week, we’re taking advantage of unseasonably warm weather, moving into new apartments, prepping our emergency supplies, and trying new expense-sharing apps. What upgrades did you make this week? Let us know in the comments. Everything is blooming and spring is beautiful and now it’s warm enough I can sit at the little table on my porch to work. My upgrade is sitting on my porch. beth skwarecki, health editor I moved! And unpacked, decorated, updated my mailing address and voter registration, got internet installed, bought a bookshelf, shopped for cleaning supplies and Command hooks and did all of those other fun tasks you need to do when you move somewhere new. Now I’m ready for a nap. alicia adamczyk, staff writer I have been trying—so far, unsuccessfully—to find a great EPUB reader for my iPad. I tried Google Play Books, which was promising given how easy it was to upload the ebooks to the service, but the formatting of each ebook got completely messed up. I’m going to try the Kindle app next, but I hate that you have to basically email your files in... david murphy, senior tech editor I went on a group trip and used the app Splitwise to keep track of all expenses. You can manage who paid for what (and who participated in each expense). At the end, it gives everyone one amount to pay that evens everything out, making those group expenses simple! joel kahn, senior video producer In our office we have little lockers that weirdly come without handles, so I bought a PopSocket—a stick-on handle for your phone—and stuck it to my locker. Instant doorknob. I felt like I’d created a door in the middle of the air, like in a Bugs Bunny cartoon. nick douglas, staff writer I bought jumper cables. My car’s battery died recently because it’s been sitting in my driveway for weeks and I thought I had some cables in the back. I didn’t... Double check every once in a while to make sure you have the tools and emergency supplies you think you have. patrick allan, staff writer","When we last rounded up our staff’s Weekly Upgrades, our editors were improving our cloud storage, putting more money into our retirement funds, and re-discovering the joys of Pokémon GO. Read more...",2018-05-04T17:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",Do Your Work Outside Whenever Possible,https://lifehacker.com/do-your-work-outside-whenever-possible-1825773614,"https://i.kinja-img.com/gawker-media/image/upload/s--bSXjhagJ--/c_fill,fl_progressive,g_center,h_450,q_80,w_800/unqjepx9kvggnxgk9zno.jpg",,Lifehacker.com,cloud,1
Natasha Lomas,"Europe-based cloud storage startup Tresorit, which mainly focuses on selling to small to medium size businesses, has added a file restore feature to its e2e encrypted cloud storage platform. It’s touting this as a helpful feature if you’re trying to recover from a ransomware attack. Or, more prosaically, if you’ve accidentally deleted something. Here’s a GIF showing the file recovery feature in action: The file restore feature covers files stored in Tresorit’s cloud and files synced locally to a user’s devices. Obviously, if files are only stored locally and not backed up or synced to Tresorit’s cloud there’s no fallback restoration in the event of a ransomware infection. (While files stored in Tresorit’s cloud that not synced locally would not be affected by any local ransomware infection.) Tresorit already had a file versioning feature, which allows users to recover any previously saved versions of their files. But it says the addition of file restore helps mitigate the types of ransomware attacks that encrypt files without deleting them first. There’s no time limitation on the file restore option. Files can always be recovered so long as the user hasn’t confirmed permanent deletion. Which does mean, over time, the feature may end up eating into your storage limit — at least if you don’t tidy up and fully delete files you no longer need. “Non-permanently deleted items count towards the storage space of a user. So, it requires some ‘housekeeping’ from the user,” confirms a Tresorit spokeswoman. “But it is easy to get rid of all these deleted items that a user doesn’t need by selecting ‘Remove deleted items’.” Also helpful: Tresorit has announced it’s doubling the amount of storage space it offers for individual plans — with its premium (aimed at individuals) and solo (freelancers and professionals) plan users now getting 200GB and 200TB respectively. Today it’s also introduced a new basic plan which it describes as a “more capable” free version — intended to help external collaboration between its business users and their clients or partners (who may not be Tresorit users). Last year it launched free subscriptions for NGOs and activists for whom strong privacy is not just a nice to have. And the spokeswoman tells us more than 100,000 people are now using its tools — which includes both consumer (so some non-paying) and business users. “Almost two-thirds of our customers are European, led by the traditionally security and privacy conscious countries like Germany and Switzerland. The next biggest European markets are the UK and the Benelux-states. The second largest region is North-America (mostly the US),” she says, adding that Europe’s incoming update to its data protection framework is also driving local uptake. “With only a few months to go until the GDPR, we are seeing an even higher demand for secure, end-to-end encrypted online services with European data centers. A lot of smaller companies are just starting the preparation for the GDPR, and looking for secure services they can easily switch to.” Tresorit’s zero-knowledge e2e encryption architecture means that, unlike cloud storage giants like Dropbox, it cannot decrypt and access users’ files. So it cannot be subpoenaed to hand over content data itself. Although it can provide some user and service activity data in exchange to lawful requests — such as names, email addresses, billing details and so on. The company recently started publishing a Transparency Report to list any government data requests it receives and provide details on how it handles such requests. “During the period covered in this (from September 24, 2013, to November 30, 2017), we received one informal request from a Swiss police authority to retain certain user data, however, as there was no official decision by Swiss authorities on this case, in the end, we didn’t hand over any data,” the spokeswoman tells us. “As a Swiss company, Tresorit is primarily subject to Swiss jurisdiction regarding data protection and criminal procedures. Without an official decision by a Swiss cantonal or federal authority, no information can be provided to foreign requests.”",Europe-based cloud storage startup Tresorit which mainly focuses on selling to small to medium size businesses has added a file restore feature to its e2e encrypted cloud storage platform which it’s touting as a helpful feature if you’re trying to recover fro…,2018-02-20T15:28:50Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Tresorit adds file restore to its e2e encrypted cloud storage service,http://techcrunch.com/2018/02/20/tresorit-adds-file-restore-to-its-e2e-encrypted-cloud-storage-service/,https://tctechcrunch2011.files.wordpress.com/2018/02/tresorit_filerestore2.png,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Despite its name, the Linux Foundation has long been about more than just Linux. These days, it’s a foundation that provides support to other open source foundations and projects like Cloud Foundry, the Automotive Grade Linux initiative and the Cloud Native Computing Foundation. Today, the Linux Foundation is adding yet another foundation to its stable: the LF Deep Learning Foundation. The idea behind the LF Deep Learning Foundation is to “support and sustain open source innovation in artificial intelligence, machine learning, and deep learning while striving to make these critical new technologies available to developers and data scientists everywhere.” The founding members of the new foundation include Amdocs, AT&amp;T, B.Yond, Baidu, Huawei, Nokia, Tech Mahindra, Tencent, Univa and ZTE. Others will likely join in the future. “We are excited to offer a deep learning foundation that can drive long-term strategy and support for a host of projects in the AI, machine learning, and deep learning ecosystems,” said Jim Zemlin, executive director of The Linux Foundation. The foundation’s first official project is the Acumos AI Project, a collaboration between AT&amp;T and Tech Mahindra that was already hosted by the Linux Foundation. Acumos AI is a platform for developing, discovering and sharing AI models and workflows. Like similar Linux Foundation-based organizations, the LF Deep Learning Foundation will offer different membership levels for companies that want to support the project, as well as a membership level for non-profits. All LF Deep Learning members have to be Linux Foundation members, too.","Despite its name, the Linux Foundation has long been about more than just Linux. These days, it’s a foundation that provides support to other open source foundations and projects like Cloud Foundry, the Automotive Grade Linux initiative and the Cloud Native C…",2018-03-26T16:00:46Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",The Linux Foundation launches a deep learning foundation,http://techcrunch.com/2018/03/26/the-linux-foundation-launches-a-deep-learning-foundation/,https://techcrunch.com/wp-content/uploads/2018/03/gettyimages-871793108.jpg?w=602,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google today announced that it has expanded its recently launched Cloud Platform region in the Netherlands with an additional zone. The investment, which is worth a reported 500 million euros, expands the existing Netherlands region from two to three regions. With this, all four of the Central European Google Cloud Platform zones now feature three zones (which are akin to what AWS would call “availability zones”) that allow developers to build highly available services across multiple data centers. Google typically aims to have a least three zones in every region, so today’s announcement to expand its region in the Dutch province of Groningen doesn’t come as a major surprise. With this move, Google is also making Cloud Spanner, Cloud Bigtable, Managed Instance Groups, and Cloud SQL available in the region. Over the course of the last two years, Google has worked hard to expand its global data center footprint. While it still can’t compete with the likes of AWS and Azure, which currently offers more regions than any of its competitors, the company now has enough of a presence to be competitive in most markets. In the near future, Google also plans to open regions in Los Angeles, Finland, Osaka and Hong Kong. The major blank spots on its current map remain Africa, China (for rather obvious reasons) and Eastern Europe, including Russia.","Google today announced that it has expanded its recently launched Cloud Platform region in the Netherlands with an additional zone. The investment, which is worth a reported 500 million euros, expands the existing Netherlands region from two to three regions.…",2018-03-16T16:10:00Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google expands its Cloud Platform region in the Netherlands,http://techcrunch.com/2018/03/16/google-expands-its-cloud-platform-region-in-the-netherlands/,https://techcrunch.com/wp-content/uploads/2018/03/gettyimages-628050484.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Containers are quickly becoming the standard way for deploying new applications in the cloud and that’s even true for the most traditional of enterprises. It’s no surprise then that every major cloud provider is betting on containers — and more specifically on the open source Kubernetes container orchestration service. IBM has long offered its own take on this with its Cloud Container Service and today it’s offering an industry first: bare metal Kubernetes as a managed service. Why does that matter? Running containers on bare metal has some performance benefits and development teams will be able to choose the kid of machine they want to run their containers on. But at the same time, knowing that you are running your containers on a machine that you don’t share with other customers also adds another layer of security and isolation that you don’t get from running on a regular container service. As IBM’s CTO for Watson and Cloud Bryson Koehler told me, this now also opens up the ability to attach GPUs to these machines, which in turn enables the kind of machine learning and high performance computing workloads that many enterprises are now starting to experiment with. “If you look at the types of workloads that enterprises are moving to the cloud, bare metal is a huge benefactor in terms of isolation and flexibility,” Koehler told me. Like the rest of the IBM Cloud Container Service, this is a fully managed services with the kind of automatic updates and security patches that come with that.",Containers are quickly becoming the standard way for deploying new applications in the cloud and that’s even true for the most traditional of enterprises. It’s no surprise then that every major cloud provider is betting on containers — and more specifically o…,2018-03-14T11:00:04Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",IBM launches bare metal Kubernetes,http://techcrunch.com/2018/03/14/ibm-launches-bare-metal-kubernetes/,https://techcrunch.com/wp-content/uploads/2018/03/gettyimages-930841482.jpg?w=535,techcrunch,TechCrunch,cloud,1
Ron Miller,"One of the joys of cloud computing is handing over your data to the cloud vendor and letting them handle the heavy lifting. Up until now that has meant they updated the software or scaled the hardware for you. Today, AWS took that to another level when it announced Amazon DynamoDB Continuous Backups and Point-In-Time Recovery (PITR). With this new service, the company lets you simply enable the new backup tool, and the backup happens automatically. Amazon takes care of the rest, providing a continuous backup of all the data in in your DynamoDB database. But it doesn’t stop there, it lets the backup system act as a recording of sorts. You can rewind your data set to any point in time in the backup to any time with “per second granularity” up to 35 days in the past. What’s more, you can access the tool from the AWS Management Console, an API call or via the AWS Command Line Interface (CLI). Screenshot: Amazon “We built this feature to protect against accidental writes or deletes. If a developer runs a script against production instead of staging or if someone fat-fingers a DeleteItem call, PITR has you covered. We also built it for the scenarios you can’t normally predict,” Amazon’s Randall Hunt wrote in the blog post announcing the new feature. If you’re concerned about the 35 day limit, you needn’t be as the system is an adjunct to your regular on-demand backups, which you can keep for as long as you need. Amazon’s Chief Technology Officer, Werner Vogels, who introduced the new service at the Amazon Summit in San Francisco today, said it doesn’t matter how much data you have. Even with a terabyte of data, you can make use of this service. “This is a truly powerful mechanism here,” Vogels said. The new service is available in various regions today. You can learn about regional availability and pricing options here.","One of the joys of cloud computing is handing over your data to the cloud vendor and letting them handle the heavy lifting. Up until now that has meant they updated the software or scaled the hardware for you. Today, AWS took that to another level when it ann…",2018-04-04T18:11:47Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",AWS adds automated point-in-time recovery to DynamoDB,http://techcrunch.com/2018/04/04/aws-adds-automated-point-in-time-recovery-to-dynamodb/,https://techcrunch.com/wp-content/uploads/2018/04/gettyimages-165959955.jpg?w=539,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Backblaze made its name as a cloud-based storage backup provider (and using lots of hard drives in the process) for individuals and businesses. With B2, however, the company also launched a file-centric cloud storage service for developers in 2015 that aims to challenge similar offerings from AWS and Azure on price. Since Backblaze doesn’t offer any other cloud computing services, that was always a bit of a hard sell since developer had to work with multiple vendors to write their apps and pay for the data transfer between B2 and their compute providers. But Backblaze is sweetening the deal today with a partnership with Packet and ServerCentral that will allow for free data transfer. This matchup makes a lot of sense for the three independent cloud providers involved. Packet is mostly known for its bare-metal server offerings while ServerCentral focuses on helping businesses manage their IT infrastructure. Packer offers a block storage service, too, but the use case for that is somewhat different from Backblaze’s storage service. “With over a decade of building the lowest-cost cloud storage, we’re proud to be able to profitably offer customers storage at one-fourth of the price of S3,” noted Gleb Budman, CEO of Backblaze. “Our partnerships with Packet and ServerCentral now enable us to jointly support a variety of new uses and customers. Both providers offer world class compute offerings. And, by charging $0.00 for transfers between Backblaze and our Compute Partners, customers choose what’s best for their particular workload.” Likewise, Packet and ServerCentral argue that their services allow developers to use their services and save on their AWS bill. “Many cloud users realize too late that traditional vendor pricing schemes are meant to lock in customer data. In our industry, it’s known as data gravity and Dropbox just saved $74 million by avoiding it with their move off of AWS,” said Jacob Smith, co-founder of Packet. “The question we had was ‘how can we democratize that Dropbox effect?’ The answer was clear – directly connecting with Backblaze’s B2 Cloud Storage offering.”","Backblaze made its name as a cloud-based storage backup provider (and using lots of hard drives in the process) for individuals and businesses. With B2, however, the company also launched a file-centric cloud storage service for developers in 2015 that aims t…",2018-04-03T15:00:39Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Backblaze partners with Packet and Server Central to better challenge the big clouds,http://techcrunch.com/2018/04/03/backblaze-partners-with-packet-and-server-central-to-better-challenge-the-big-clouds/,https://techcrunch.com/wp-content/uploads/2015/09/backblaze-b2-08-storage-pod.png?w=604,techcrunch,TechCrunch,cloud,1
Romain Dillet,"French startup Platform.sh has raised a $34 million funding round. The company wants to help you manage your cloud infrastructure by handling the most tedious part of the job. When you use Platform.sh for your application, the startup is going to handle testing and deployment to your cloud infrastructure. Every time you want to iterate and update your application to a new version, deployment is as easy as a git commit. Partech is leading the round, with Idinvest Partners, Benhamou Global Ventures, SNCF Digital Ventures and existing investor Hi Inov also participating. Platform.sh targets big clients. The company is currently working with 650 enterprise clients, such as Magento, Gap Inc. and The Financial Times. In 2018, revenue has more than doubled compared to the same period last year. Platform.sh can create new instances and deploy clones of your web applications in less than 60 seconds. That’s how you can deploy with confidence and save time. The idea is that Platform.sh helps you deploy 10 times or 20 times per day. Your users won’t see a difference as your website will remain available during the entire day. Behind the scene, Platform.sh uses multiple cloud vendors for its infrastructure, including Amazon Web Services, Microsoft Azure and Orange Business Services. Platform.sh isn’t the only continuous deployment solution out there. And many tech companies are going to build their own continuous deployment process on top of open source technologies. But many companies don’t have a big tech team and can outsource this part of their infrastructure. If you’re building a media or e-commerce website, you might want to focus on other parts of your business for instance. In that case, Platform.sh provides a one-stop shop for your cloud hosting needs.","French startup Platform.sh has raised a $34 million funding round. The company wants to help you manage your cloud infrastructure by handling the most tedious part of the job. When you use Platform.sh for your application, the startup is going to handle testi…",2018-05-23T10:01:27Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Platform.sh raises $34 million to simplify cloud deployment,http://techcrunch.com/2018/05/23/platform-sh-raises-34-million-to-simplify-cloud-deployment/,https://techcrunch.com/wp-content/uploads/2015/07/8681750288_354823d8d3_o.jpg?w=598,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google Cloud is launching a new feature today that will give its users a new way to monitor and optimize how their data flows between their servers in the Google Cloud and other Google Services, on-premises deployments and virtually any other internet endpoint. As the name implies, VPC Flow Logs are meant for businesses that already use Google’s Virtual Private Cloud features to isolate their resources from other users. Google Cloud gives developers more insights into their networks VPC Flow Logs monitors and logs all the network flows (both UDP and TCP) that are sent from and received by the virtual machines inside a VPC, including traffic between Google Cloud regions. All of that data can be exported to Stackdriver Logging or BigQuery, if you want to keep it in the Google Cloud, or you can use Cloud Pub/Sub to export it to other real-time analytics or security platform. The data updates every five seconds and Google premises that using this service has no impact on the performance of your deployed applications. As the company notes in today’s announcement, this will allow network operators to get far more insights into the details of how the Google network performs and to troubleshoot issues if they arise. In addition, it will also allow them to optimize their network usage and costs by giving them more information about their global traffic. All of this data is also quite useful for performing forensics when it looks like somebody may have gotten into your network, too. If that’s your main use case, though, you probably want to export your data to a specialized security information and event management (SIEM) platform from vendors like Splunk or ArcSight.","Google Cloud is launching a new feature today that will give its users a new way to monitor and optimize how their data flows between their servers in the Google Cloud and other Google Services, on-premises deployments and virtually any other internet endpoin…",2018-04-05T16:00:24Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud gives developers more insights into their networks,http://techcrunch.com/2018/04/05/google-cloud-gives-developers-more-insights-into-their-networks/,https://techcrunch.com/wp-content/uploads/2018/04/gettyimages-910201616.jpg?w=618,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"For the longest time, enterprises worried about how secure their data would be in the cloud, and for some, that was a major roadblock for moving out of their own data centers. At this point, most major cloud providers have shown that security isn’t an issue and that they are quite capable of securing their customers’ data. Security is an ever-changing field, though, so there’s always more to be done. As if to hammer this point home, Google today is announcing a slew of new security features for both the Google Cloud Platform and G Suite. Let’s start with the Cloud Platform updates. The highlight of the launch is the new VPC Service Controls tool for protecting API-based services. I know that name doesn’t scream excitement, but it basically provides businesses an additional security layer for their API-based services on GCP. The idea here is to prevent attackers from exfiltrating data from cloud services. Here is how Google’s VP for security and privacy Gerhard Eschelbeck describes it: “ Imagine constructing an invisible border around everything in an app that prevents its data from escaping, and having the power to set up, reconfigure, and tear down these virtual perimeters at will.” To give its users deeper insights into their security posture, Google also is launching its Cloud Security Command Center today. This new tool will give enterprises deeper insights into their security posture across Google Cloud services like App Engine, Compute Engine and Cloud Storage. The service provides insights into where sensitive data is stored, but also which apps may be vulnerable to cross-site scripting attacks. In addition, the service scans firewall rules and regularly looks for changes in a company’s security settings and alerts operators of changes to make sure those weren’t unauthorized. One of the more interesting aspects of this service is that Google is also partnering with a number of security vendors like Cloudflare, CrowdStrike, RedLock, Palo Alto Networks and Qualys to detect DDoS attacks, policy violations, network intrusions and other threats. As far as DDoS attacks go, Google also today announced a new service called Cloud Armor (see, GCP can do naming right sometimes!). Cloud Armor is both a DDoS and application defense service that provides all the usual IP white- and blacklisting tools and integrates with Google’s Global Load Balancing service. Other Cloud Platform security updates include new logging tools, updates to the Data Loss Prevention API and new tools for managing access to GCP resources. The Google Cloud Platform is now also FedRamp certified at the Moderate Impact level, though unless you work for the U.S. federal government or a state or local agency, you probably don’t care much about that. As for G Suite, Google is launching a couple of new charts in that product’s security center dashboard (for Oauth activity and Business Email Compromise scan threats) and users can now customize the dashboard, too. Exciting stuff. What’s probably more interesting is that Google now defaults to turning on features like flagging emails from untrusted senders with encrypted attachments or embedded scripts. The service will now also warn users of emails that try to spoof employee names from domains that look similar to your official one and, among other things, it’ll now default to expanding shortened URLs to scan for malicious links. There are a couple of more updates here, but the main point Google is clearly trying to make is that it takes security very seriously. I don’t think anybody doubted that before, though, given the company’s investment in various security efforts over the years, but if there’s nothing like announcing a few dozen major and minor updates to remind people.","For the longest time, enterprises worried about how secure their data would be in the cloud, and for some, that was a major roadblock for moving out of their own data centers. At this point, most major cloud providers have shown that security isn’t an issue a…",2018-03-21T13:30:37Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google brings new security features to its cloud,http://techcrunch.com/2018/03/21/google-brings-new-security-features-to-its-cloud/,https://techcrunch.com/wp-content/uploads/2017/08/google-sign.jpg?w=536,techcrunch,TechCrunch,cloud,1
Ron Miller,"One of the characteristics of cloud computing is that when you launch a virtual machine, it gets distributed wherever it makes the most sense for the cloud provider. That usually means sharing servers with other customers in what is known as a multi-tenant environment. But what about times when you want a physical server dedicated just to you? To help meet those kinds of demands, Google announced the Beta of Google Compute Engine Sole-tenant nodes, which have been designed for use cases such a regulatory or compliance where you require full control of the underlying physical machine, and sharing is not desirable. “Normally, VM instances run on physical hosts that may be shared by many customers. With sole-tenant nodes, you have the host all to yourself,” Google wrote in a blog post announcing the new offering. Diagram: Google Google has tried to be as flexible as possible, letting the customer choose exactly what configuration they want in terms CPU and memory. Customers can also let Google choose the dedicated server that’s best at any particular moment, or you can manually select the server if you want that level of control. In both cases, you will be assigned a dedicated machine. If you want to play with this, there is a free tier and then various pricing tiers for a variety of computing requirements. Regardless of your choice, you will be charged on a per-second basis with a one-minute minimum charge, according to Google. Since this feature is still in Beta, it’s worth noting that it is not covered under any SLA. Microsoft and Amazon have similar offerings.","One of the characteristics of cloud computing is that when you launch a virtual machine, it gets distributed wherever it makes the most sense for the cloud provider. That usually means sharing servers with other customers in what is known as a multi-tenant en…",2018-06-07T16:59:43Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud announces the Beta of single tenant instances,http://techcrunch.com/2018/06/07/google-cloud-announces-the-beta-of-single-tenant-instances/,https://techcrunch.com/wp-content/uploads/2018/01/gettyimages-451216438.jpg?w=600,techcrunch,TechCrunch,cloud,1
Ron Miller,"Drone.io, makers of the open source Drone continuous integration/continuous delivery tool (CI/CD), announced Drone Cloud today, a new CI/CD cloud service that it’s making available for free to open source projects. The company is teaming with Packet, which is offering to run the service for free on its servers. Drone.io co-founder Brad Rydzewski says his company is “a container-native continuous delivery platform, and its goal is to help automate the developer workflow from testing to release.” Continuous delivery is an approach built on cloud-native, the idea that you can manage cloud and on prem with single set of management tools. From a developer standpoint, it relies on containers as a way to continuously deliver application updates as changes happen. As part of that approach, the newly announced Drone Cloud provides a publicly hosted CI/CD cloud offering. “It’s free for the open source community. So it’s an open source only offering. There’s no paid plan, and it’s only available to public Github repositories,” Rydzewski explained. Rydzewski says the service was born out of a need for his own project. He found it hard to find publicly-hosted solutions that offered a way to test a variety of operating systems and chip architectures. “It’s really something we needed for ourselves. The Drone community wanted to run Drone on Windows on Arm 64 processors. We actually had no way to build and test the software on these platforms because there was just no publicly-hosted solution that we could use,” he explained. When they decided to build this solution for themselves, they figured it was going to be useful to other open source projects that also want to ship and support multiple operating systems and architectures. They got Packet on board to offer the infrastructure. Packet offers a variety of server options with different operating systems and chips, and Rydzewski says this was an important consideration for the open source developers who will take advantage of this service. Packet is making the latest Intel Xeon Scalable, AMD EPYC and Arm-based servers available to users of the Drone Cloud service for free as part of a multi-year donation to support the project. “As open source software is deployed to increasingly diverse environments, from traditional data centers to cars and even drones, the need for multi-architecture builds has exploded,” Jacob Smith, co-founder and CMO at Packet said in a statement. This is Packet’s way of supporting that effort. Drone does not intend to establish a paid layer for Drone Cloud, according to Rydzewski, but he hopes it shows what Drone can do, which in turn could attract some developers to the paid version of the product. In addition to supporting the open source version, the company offers a paid version that can be installed on premises as a part of a private cloud-native development environment.","Drone.io, makers of the open source Drone continuous integration/continuous delivery tool (CI/CD), announced Drone Cloud today, a new CI/CD cloud service that it’s making available for free to open source projects. The company is teaming with Packet, which is…",2018-11-21T18:00:25Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","Drone.io, Packet team on free continuous delivery service for open source developers",http://techcrunch.com/2018/11/21/drone-io-packet-team-on-free-continuous-delivery-service-for-open-source-developers/,https://techcrunch.com/wp-content/uploads/2018/11/GettyImages-885300382.jpg?w=600,techcrunch,TechCrunch,cloud,1
Jonathan Shieber,"More posts by this contributor Theres a dark cloud on the horizon. The behavior of cloud infrastructure providers, such as Amazon, threatens the viability of open source. I first wrote about this problem in a prior piece on TechCrunch. In 2018, thankfully, several leaders have mobilized (amid controversy) to propose multiple solutions to the problem. Heres whats happened in the last month. The Problem Go to Amazon Web Services (AWS) and hover over the Products menu at the top. You will see numerous open-source projects that Amazon did not create, but runs as-a-service. These provide Amazon with billions of dollars of revenue per year. To be clear, this is not illegal. But it is not conducive to sustainable open-source communities, and especially commercial open-source innovation. Two Solutions In early 2018, I gathered together the creators, CEOs or general counsels of two dozen at-scale open-source companies, along with respected open source lawyer Heather Meeker, to talk about what to do. We wished to define a license that prevents cloud infrastructure providers from running certain software as a commercial service, while at the same time making that software effectively open source for everyone else, i.e., everyone not running that software as a commercial service. With our first proposal, Commons Clause, we took the most straightforward approach: we constructed one clause, which can be added to any liberal open source license, preventing the licensee from Selling the softwarewhere Selling includes running it as a commercial service. (Selling other software made with Commons Clause software is allowed, of course.) Applying Commons Clause transitions a project from open source to source-available. We also love the proposal being spearheaded by another participant, MongoDB, called the Server Side Public License (SSPL). Rather than prohibit the software from being run as a service, SSPL requires that you open-source all programs that you use to make the software available as a service, including, without limitation, management software, user interfaces, application program interfaces, automation software, monitoring software, backup software, storage software and hosting software, all such that a user could run an instance of the service. This is known as a copyleft. These two licenses are two different solutions to exactly the same problem. Heather Meeker wrote both solutions, supported by feedback organized by FOSSA. The initial uproar and accusations that these efforts were trying to trick the community fortunately gave way to understanding and acknowledgement from the open source community that there is a real problem to be solved here, that it is time for the open source community to get real, and that it is time for the net giants to pay fairly for the open source on which they depend. In October, one of the board members of the Apache Software Foundation (ASF) reached out to me and suggested working together to create a modern open source license that solves the industrys needs. Kudos to MongoDB Further kudos are owed to MondoDB for definitively stating that they will be using SSPL, submitting SSPL in parallel to an organization called Open Source Initiative (OSI) for endorsement as an open source license, but not waiting for OSIs endorsement to start releasing software under the SSPL. OSI, which has somehow anointed itself as the body that will decide whether a license is open source, has a habit of myopically debating whats open source and whats not. With the submission of SSPL to OSI, MongoDB has put the ball in OSIs court to either step up and help solve an industry problem, or put their heads back in the sand. In fact, MongoDB has done OSI a huge favor. MongoDB has gone and solved the problem and handed a perfectly serviceable open source license to OSI on a silver platter. Open Source Sausage The public archives of OSIs debate over SSPL are at times informative and at times amusing, bordering on comical. After MongoDBs original submission, there were rah-rah rally cries amongst the members to find reasons to deem SSPL not an open source license, followed by some +1s. Member John Cowan reminded the group that just because OSI does not endorse a license as open source, does not mean that it is not open source : As far as I know (which is pretty far), the OSI doesnt do that. They have never publicly said License X is not open source. People on various mailing lists have done so, but not the OSI as such. And they certainly dont say Any license not on our OSI Certified ™ list is not open source, because that would be false. Its easy to write a license that is obviously open source that the OSI would never certify for any of a variety of reasons. Eliot Horowitz (CTO and co-founder of MongoDB) responded cogently to questions, comments and objections, concluding with: In short, we believe that in todays world, linking has been superseded by the provision of programs as services and the connection of programs over networks as the main form of program combination. It is unclear whether existing copyleft licenses clearly apply to this form of program combination, and we intend the SSPL to be an option for developers to address this uncertainty. Much discussion ensued about the purpose, role and relevance of OSI. Various sundry legal issues were raised or addressed by Van Lindberg, McCoy Smith, and Bruce Perens. Heather Meeker (the lawyer who drafted both Commons Clause and SSPL ) stepped in and completely addressed the legal issues that had been raised thus far. Various other clarifications were made by Eliot Horowitz, and he also conveyed willingness to change the wording of the license if it would help. Discussion amongst the members continued about the role, relevance and purpose of OSI, with one member astutely noting that there were a lot of free software wonks in the group, attempting to bastardize open source to advocate their own agenda: If, instead, OSI has decided that they are now a Free Software organization, and that Free Software is what we do, and that our focus is on Free software then, then lets change the name to the Free Software Initiative and open the gates for some other entity, who is all about Open Source, to take on that job, and do it proudly. :-) There was debate over whether SSPL discriminates against types of users, which would disqualify it from being open source. Eliot Horowitz provided a convincing explanation that it did not, which seemed to quiet the crowd. Heather Meeker dropped some more legal knowledge on the group, which seemed to sufficently address outstanding issues. Bruce Perens, the author of item 6 of the so-called open source definition, acknowledged that SSPL does not violate item 6 or item 9 of the definition, and subsequently suggested revising item 9 such that SSPL would violate it : Were not falling on our swords because of this. And we can fix OSD #9 with a two word addition or performed as soon as the board can meet. But its annoying. Kyle Mitchell, himself an accomplished open source lawyer, opposed such a tactic. Larry Rosen pointed out that some members assertion (that it is fundamental to open source that everyone can use a program for any purpose) is untrue. Still more entertaining discussion ensued about the purpose of OSI and the meaning of open source. Carlos Piana succinctly stated why SSPL was indeed open source. Kyle Mitchell added that if licenses were to be judged in the manner that the group was judging SSPL, then GPL v2 was not open source either. Groundswell Meanwhile Dor Lior, the founder of database company ScyllaDB compared SSPL and AGPL side-to-side and argued that MongoDB would have been better off with Commons Clause or just swallowed a hard pill and stayed with APGL. Player.FM released their service based on Commons Clause licensed RediSearch, after in-memory database company Redis Labs placed RediSearch and four other specific add-on modules (but not Redis itself) under Commons Clause, and graph database company Neo4J placed its entire codebase under Commons Clause and raised an $80M Series E. Then Michael DeHaan, creator of Red Hat Ansible, chose Commons Clause for his new project. When asked why he did not choose any of the existing licenses that OSI has endorsed to be open source, he said: This groundswell in 2018 should be ample indication that there is an industry problem that needs to be fixed. Eliot Horowitz summarized and addressed all the issues, dropped the mic, and left for a while. When it seemed like SSPL was indeed following all the rules of open source licenses, and was garnering support of the members, Brad Kuhn put forward a clumsy argument for why OSI should change the rules as necessary to prevent SSPL from being deemed open source, concluding: Its likely the entire license evaluation process that we use is inherently flawed. Mitchell clinched the argument that SSPL is open source with definitive points. Horowitz thanked the members for their comments and offered to address any concerns in a revision, and returned a few days later with a revised SSPL. OSI has 60 days since MongoDBs new submission to make a choice: Wake up and realize that SSPL, perhaps with some edits, is indeed an open source license, OR Effectively signal to the world that OSI does not wish to help solve the industrys problems, and that theyd rather be policy wonks and have theoretical debates. Wonk here is meant in the best possible way. Importantly, MongoDB is proceeding to use the SSPL regardless. If MongoDB were going to wait until OSIs decision, or if OSI were more relevant, we might wait with bated breath to hear whether OSI would endorse SSPL as an open source license. As it stands, OSIs decision is more important to OSI itself, than to the industry. It signals whether OSI wants to remain relevant and help solve industry problems or whether it has become too myopic to be useful. Fearful of the latter, we looked to other groups for leadership and engaged with the Apache Software Foundation (ASF) when they reached out in the hopes of creating a modern open source license that solves the industrys needs. OSI should realize that while it would be nice if they deemed SSPL to be open source, it is not critical. Again in the words of John Cowan, just because OSI has not endorsed a license as open source, does not mean its not open source. While we greatly respect almost all members of industry associations and the work they do in their fields, it is becoming difficult to respect the purpose and process of any group that anoints itself as the body that will decide whether a license is open sourceit is arbitrary and obsolete. Errata In my zest to urge the industry to solve this problem, in an earlier piece, I had said that if one takes open source software that someone else has built and offers it verbatim as a commercial service for ones own profit (as cloud infrastructure providers do) thats not in the spirit of open-source. Thats an overstatement and thus, frankly, incorrect. Open source policy wonks pointed this out. I obviously dont mind rattling their cages but I should have stayed away from making statements about whats in the spirt so as to not detract from my main argument. Conclusion The behavior of cloud infrastructure providers poses an existential threat to open source. Cloud infrastructure providers are not evil. Current open source licenses allow them to take open source software verbatim and offer it as a commercial service without giving back to the open source projects or their commercial shepherds. The problem is that developers do not have open source licensing alternatives that prevent cloud infrastructure providers from doing so. Open source standards groups should help, rather than get in the way. We must ensure that authors of open source software can not only survive, but thrive. And if that means taking a stronger stance against cloud infrastructure providers, then authors should have licenses available to allow for that. The open source community should make this an urgent priority. Disclosures I have not invested directly or indirectly in MongoDB. I have invested directly or indirectly in the companies behind the open source projects Spring, Mule, DynaTrace, Ruby Rails, Groovy Grails, Maven, Gradle, Chef, Redis, SysDig, Prometheus, Hazelcast, Akka, Scala, Cassandra, Spinnaker, FOSSA, and in Amazon.","Salil Deshpande Contributor Salil Deshpande serves as the managing director of Bain Capital Ventures. He focuses on infrastructure software and open source. More posts by this contributor Let’s define “container-native” After the Satoshi Roundtable, is there …",2018-11-29T17:30:54Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",The crusade against open source abuse,http://techcrunch.com/2018/11/29/the-crusade-against-open-source-abuse/,https://techcrunch.com/wp-content/uploads/2018/11/7539784386_8ae151f29c_o.jpg?w=600,techcrunch,TechCrunch,cloud,1
Lucas Matney,"At Google I/O earlier this month, the company announced Cloud Anchors, a tool that shares 3D data captured by a user’s smartphone with the cloud and can match that data up with another user to create a shared AR experience where each person’s phone is seeing the same things in the same places. Today, Google is rolling out Cloud Anchor functionality to its AR drawing app called Just a Line which it released a couple months ago. Just a Line is hardly a breakout hit for Google, but the simplistic app that lets users paint the 3D world with a white line offers a nice testbed for early AR functionality that’s just as experimental. What will likely differentiate Google’s offering from whatever Apple ends up shipping is that the Cloud Anchors is cross-platform. The Just a Line app is available on both Android and iOS, and with today’s update users on both platforms will be able to collaborate and view items in a shared space. What’s generally important about multiplayer AR experiences is making the process simple enough for users to sync their spatial map with another user so that they see the same digital objects in the same physical locations. What Google has built seems a bit cumbersome with each user needing to stand next to each other to pair their environments. It also seems that the functionality is limited to two people at the moment. Just a Line isn’t the most high stakes place for Google to be dropping this feature, so there is clearly room for the company to keep updating what they’ve got as they see what early usage looks like.","At Google I/O earlier this month, the company announced Cloud Anchors, a tool that shares 3D data captured by a user’s smartphone with the cloud and can match that data up with another user to create a shared AR experience where each person’s phone is seeing …",2018-05-30T17:02:02Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google builds its cross-platform multiplayer AR tech into a doodling app,http://techcrunch.com/2018/05/30/google-builds-its-cross-platform-multiplayer-ar-tech-into-a-doodling-app/,https://techcrunch.com/wp-content/uploads/2018/05/da5bz2f1yuuclz8uqsx9bn1zi_4y187ilyiaifkftkv6dzovdrwifb9mkt3blebljqxbddwbzl5ix5w2gfxt8airvpm6-yblwinwqurmtlmd3ohvg9mijxvjg3rielalvpvanund.gif?w=711,techcrunch,TechCrunch,cloud,1
Matt Burns,"Starting in Norway and Sweden, vehicles from Volvo cars and Volvo trucks are now able to share traffic information through Volvo’s Connected Cloud. Let’s say car A runs into a road hazard. Car A will then upload that information to the Volvo Connected Cloud where it will broadcast the information to Volvo Cars and Trucks warning Truck B, which is carrying a load of chickens, to avoid the road hazard. It’s important to note Volvo cars and trucks are independent vehicle manufacturers though they clearly share some DNA. This is an extension of the Volvo Connected Cloud that the auto maker launched in 2016. Right now the service is only available in Sweden and Norway, where Volvo cars and trucks make up a significant amount of the vehicles. Vehicle to vehicle communication is among a growing market to improve data presentation to the driver, and in the future, autonomous vehicles. This collaboration these Volvo brands is a big step towards the future. That said, there’s an argument to be made that Volvo could open source the platform and lure other manufacturers aboard so we can all get our chickens delivered in time no matter what sort truck is driving.","Starting in Norway and Sweden, vehicles from Volvo cars and Volvo trucks are now able to share traffic information through Volvo’s Connected Cloud. Let’s say car A runs into a road hazard. Car A will then upload that information to the Volvo Connected Cloud w…",2018-05-07T13:24:54Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Volvo cars and trucks can now share real-time traffic information,http://techcrunch.com/2018/05/07/volvo-cars-and-trucks-can-now-share-real-time-traffic-information/,https://techcrunch.com/wp-content/uploads/2017/11/216702_new_volvo_xc40_exterior.jpg?w=600,techcrunch,TechCrunch,cloud,1
Sarah Perez,"Plex today announced it’s shutting down its troubled Plex Cloud service, via a forum post that hasn’t found its way over to the company’s official blog – likely a choice the company made in order to downplay the news, or avoid media scrutiny. Plex Cloud, launched in fall 2016, was meant to serve as a way for Plex customers to save their files to online storage services like OneDrive, Dropbox and Google Drive, instead of having to host their saved files locally on their own machines or network-attached storage devices. But now that will no longer be an option, as the service will stop functioning on November 30, 2018, Plex says. Plex Cloud had struggled from the beginning with technical issues. Almost immediately, its debut launch partner, Amazon, stopped working with Plex Cloud. Users were complaining that Amazon Drive files couldn’t be accessed and wondered if Amazon was imposing upload limits. There were also concerns that Plex Cloud users whose libraries included pirated movies and TV shows could be putting themselves at risk by publishing those files to the cloud. Unlike Plex’s Cloud Sync, which syncs select local media to the cloud to access when the local server was offline, Plex Cloud is a full-fledged Plex Media Server in the cloud. That meant the media was hosted independently of local storage, and was transcoded for compatibility with Plex player apps, as needed. This led to some technical challenges Plex hasn’t been able to overcome, though it sometimes declined to explain what exact challenges Plex Cloud was facing. The company admitted last March the problems it was having were very difficult. “It’s definitely not a trivial thing to take the best media server on the planet and make it work seamlessly as a scalable cloud service, load-balanced and clustered across multiple geographic regions. It turns out a lot can go wrong,” a blog post then admitted. In February 2018, Plex announced it would disable new server creation for Plex Cloud users – something it said it had to do while “working to address challenges with performance, quality, and overall user experience inherent with cloud provider integrations.” At the time, it said it would “evaluate the long-term plan for the service.” The subtext, of course, was that Plex Cloud may be shut down if Plex couldn’t figure out how to overcome the technical issues. Today’s that day, unfortunately. Plex says it tried to address the issues that came up while keeping costs under control, but hasn’t found a solution. The announcement states: We’ve made the difficult decision to shut down the Plex Cloud service on November 30th, 2018. As you may know, we haven’t allowed any new Plex Cloud servers since February of this year, and since then we’ve been actively working on ways to address various issues while keeping costs under control. We hold ourselves to a high standard, and unfortunately, after a lot of investigation and thought, we haven’t found a solution capable of delivering a truly first class Plex experience to Plex Cloud users at a reasonable cost. While we are super bummed about the impact this will have on our happy Cloud users, ending support for it will allow us to focus on improving core functionality, adding new features and content, and delivering on our mission to provide a world-class product that we can all rely on and enjoy. On November 30, 2018, Plex Cloud users will no longer be able to access their cloud server. That means customers who want to continue to stream those files through Plex will need to download them locally on a media server or NAS device on their local network. Plex, of course, will not delete the files you’ve uploaded to cloud services, like Dropbox or Google Drive. They will remain there as long as you have a subscription to those services. While the loss of Plex Cloud will be upsetting to Plex users who were happily enjoying the service without issues, the company’s decision to shutter instead of solve the problems is indicative of the new direction Plex has been headed in recent months. Originally a software application designed for hosting users’ personal media collections, Plex has since launched its own tools for watching live TV through an antenna and recording shows to a DVR in an effort to attract the growing number of cord cutters. It has also launched support of podcasts and rolled out personalized apps in order to bring in more mobile users. It’s unclear how well Plex’s shifts have been working to attract new users and paying subscribers, as the company doesn’t break out the latter figure. As of May, Plex said it had 15 million registered users.","Plex today announced it’s shutting down its troubled Plex Cloud service, via a forum post that hasn’t found its way over to the company’s official blog – likely a choice the company made in order to downplay the news, or avoid media scrutiny. Plex Cloud, laun…",2018-09-11T20:50:56Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Plex Cloud will shut down November 30 due to technical challenges,http://techcrunch.com/2018/09/11/plex-cloud-will-shut-down-november-30-due-to-technical-challenges/,https://techcrunch.com/wp-content/uploads/2017/03/plex-cloud1.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"It’s no surprise that Google used its Cloud Next 2018 event in Tokyo today — one of a number of international Cloud Next events that follow its flagship San Francisco conference — to announce a couple of new initiatives that specifically focus on the Japanese market. These announcements include a couple of basic updates like translating its Machine Learning with TensorFlow on Google Cloud Platform Coursera specialization, its Associate Cloud Engineer certification and fifty of its hands-on Qwiklabs into Japanese. In addition, Google is also launching an Advanced Solutions Lab in Tokyo as well. Previously Google opened similar labs in Dublin, Ireland, as well as Sunnyvale and New York. These labs offer a wide range of machine learning-centric training options, collaborative workspaces for teams that are part of the company’s four-week machine learning training program, and access to Google experts. (Photo by Hitoshi Yamada/NurPhoto via Getty Images) The company also today announced that it is working with Fast Retailing, the company behind brands like Uniqlo, to help it adopt new technologies. As its name implies, Fast Retailing would like to retail faster, so it’s looking at Google and its G Suite and machine learning tools to help it accelerate its growth. The code name for this project is ‘ Ariake.’ “Making information accessible to all our employees is one of the foundations of the Ariake project, because it empowers them to use human traits like logic, judgment, and empathy to make decisions,” says Tadashi Yanai, CEO of Fast Retailing. “We write business plans every season, and we use collaborative tools like G Suite make sure they’re available to all. Our work with Google Cloud has gone well beyond demand forecasting; it’s fundamentally changed the way we work together.”",It’s no surprise that Google used its Cloud Next 2018 event in Tokyo today — one of a number of international Cloud Next events that follow its flagship San Francisco conference — to announce a couple of new initiatives that specifically focus on the Japanese…,2018-09-19T00:00:02Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google launches new AI initiatives in Japan,http://techcrunch.com/2018/09/18/google-launches-new-ai-initiatives-in-japan/,https://techcrunch.com/wp-content/uploads/2018/09/GettyImages-954857508.jpg?w=619,techcrunch,TechCrunch,cloud,1
Ron Miller,"Salesforce and Amazon’s cloud arm, AWS, have had a pretty close relationship for some time, signing a $400 million deal for infrastructure cloud services in 2016, but today at Dreamforce, Salesforce’s massive customer conference taking place this week in San Francisco, they took it to another level. The two companies announced they were offering a new set of data integration services between the two cloud platforms for common customers. Matt Garman, vice president of Amazon Elastic Compute Cloud, says customers looking to transform digitally are still primarily concerned about security when moving data between cloud vendors, More specifically, they were asking for a way to move data more securely between the Salesforce and Amazon platforms. “Customers talked to us about sensitive data in Salesforce and using deep analytics and data processing on AWS and moving them back and forth in secure way,” he said. Today’s announcements let them do that. In practice, Salesforce customers can set up a direct connection using AWS Private Link to connect directly to private Salesforce APIs and move data from Salesforce to an Amazon service such as Redshift, the company’s data warehouse product, without ever exposing the data to the open internet. Further, Salesforce customers can set up Lambda functions so that when certain conditions are met in Salesforce, it triggers an action such as moving data (or vice versa). This is commonly known as serverless computing and developers are increasingly using event triggers to drive business processes. Finally, the two companies are integrating more directly with Amazon Connect, the Amazon contact center software it launched in 2017. This is where it gets more interesting because of course Salesforce offers its own contact center services with Salesforce Service Cloud. The two companies found a way to help common customers work together here to build what they are calling AI-driven self-service applications using Amazon Connect on the Salesforce mobile Lightning development platform. This could involve among other things, building mobile applications that take advantage of Amazon Lex, AWS’s bot building application and Salesforce Einstein, Salesforce’s artificial intelligence platform. Common customers can download the Amazon Connect CTI Adapter on the Salesforce AppExchange. Make no mistake, this is a significant announcement in that it involves two of the most successful cloud companies on the planet working directly together to offer products and services that benefit their common customers. This was not lost on Bret Taylor, president and chief product officer at Salesforce. “Were enabling something that wouldnt have been possible. It’s really exciting because it’s something unique in the marketplace,” he said. What’s more, it comes on the heels of yesterday’s partnership news with Apple, giving Salesforce two powerful partners to work with moving forward. While the level of today’s news is unprecedented between the two companies, they have been working together for some time. As Garman points out, Heroku, which Salesforce bought in 2010 and Quip, which it bought last year were both built on AWS from the get-go. Salesforce, which mostly runs its own data centers in the U.S. runs most of its public cloud on AWS, especially outside the U.S. Conversely, Amazon uses Salesforce tools internally.","Salesforce and Amazon’s cloud arm, AWS, have had a pretty close relationship for some time, signing a $400 million deal for infrastructure cloud services in 2016, but today at Dreamforce, Salesforce’s massive customer conference taking place this week in San …",2018-09-25T12:00:06Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","Salesforce, AWS expand partnership with secure data sharing between platforms",http://techcrunch.com/2018/09/25/salesforce-aws-expand-partnership-with-direct-data-integration-between-platforms/,https://techcrunch.com/wp-content/uploads/2018/09/GettyImages-181167399-1.jpg?w=482,techcrunch,TechCrunch,cloud,1
Shannon Liao,"Today, Google announced a partnership with the Australian-based startup Digital Asset, which makes tools to build blockchain-based apps. The partnership adds to one Google already has with BlockApps, another startup that helps people make decentralized apps, which announced the collaboration last Wednesday on Twitter. Google wrote in a blog post that its Cloud customers can “now explore ways they might use distributed ledger technology (DLT) frameworks” by using Digital Asset and BlockApps. It’s not clear exactly how that will work for customers and whether they will be getting access to proprietary software, but more details will be unveiled this week during the Google Next ‘18 event. The company added that it would introduce open-source integrations for apps built with the blockchain-based platforms Hyperledger Fabric and Ethereum later this year in the Google Cloud Product marketplace. Digital Asset’s CEO Blythe Masters stated in a blog post that the partnership would “provide developers with a full stack solution so they can unleash the potential for web-paced innovation in blockchain.” In exchange for early access to the software (which is set for a 2019 release), Digital Asset tells The Verge, “Google Cloud is helping Digital Asset reach a wider audience of developers across different industry segments that we couldn’t reach ourselves due to our size.” It confirmed that Google was not paying for early access. The move signals that Google is looking into digital ledger technology to give its cloud services an edge over Microsoft Azure and Amazon Web Services, which currently both hold more market share than Google Cloud. It also confirms a Bloomberg report from March, which said Google had been investing in and acquiring startups with digital ledger experience, although Google hasn’t particularly expressed specific interest in acquiring Digital Asset or BlockApps. Google could build blockchain apps with Digital Asset’s proprietary programming language Digital Asset’s partnership gives Google developers access to a software development kit so that they will then be able to play around with its proprietary programming language for smart contracts. From there, Google can build blockchain apps, and later down the line, potentially license those apps to other companies or give customers access to the apps. Google has had an interest in blockchain for several years. The company was the second most active corporate investor in blockchain tech from the 2012 to 2017 period, according to CBInsights, just trailing after the Japan-based SBI Holdings. We’ve reached out to Google for more information.","Today, Google announced a partnership with the Australian-based startup Digital Asset, which makes tools to build blockchain-based apps. It also has a partnership with BlockApps, another startup that helps people make decentralized apps, which announced the c…",2018-07-23T17:56:05Z,"{'id': 'the-verge', 'name': 'The Verge'}",Google wants to bring blockchain technology to its cloud services,https://www.theverge.com/2018/7/23/17602762/google-cloud-blockchain-digital-asset-blockapps,https://cdn.vox-cdn.com/thumbor/yfNGR9ra2DMPLYbLg4mkM-rIXqs=/0x146:2040x1214/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/9019761/acastro_170808_1777_google_logo_02.jpg,the-verge,The Verge,cloud,1
Klint Finley,"The definition of cloud computing may be nebulous, but its promise is clear. Instead of filling a warehouse with servers and paying people to manage them, a company can pay a cloud computing provider to provide computing resources on demand and pay only for what it actually uses. This prospect lured organizations ranging from startups to massive corporations to stodgy government agencies onto cloud offerings from Amazon, Google, Microsoft, and others. Those now well-established services might save companies from buying and managing physical servers, but they don't quite deliver on the dream of paying only for what you use. Services like Amazon s EC2 still require you to selectand pay fora specific amount of computing resources, including memory and network capacity. You can dial those resources up or down as demand for your web application changes. But that might still leave you with less computing power than you need for a sudden or unanticipated spike in traffic, leaving your app less responsive to users. So, many cloud customers agree to pay for more resources than they routinely need. Worse, these virtual servers still require customers to do a lot of real maintenance, says Donald F. Ferguson, the cofounder and CTO of video-streaming company Seeka TV and a professor of computer science at Columbia University. For example, customers still need to install security updates, and test them to make sure they don't break existing features. Ferguson, who spent decades as a software architect for companies including Dell and Microsoft and helped create IBMs application server WebSphere, says this consumes more time than you might think. ""For most projects, we'd spend 20 percent of our time managing cloud environments,"" he says. That's why Seeka TV relies on a burgeoning approach to cloud computing that eliminates virtual servers. The idea goes by the paradoxical name of ""serverless computing."" So-called ""serverless"" services do actually rely on servers. The difference is that users don't manage the servers. Instead of renting and managing virtual servers, simply upload the code you want to run, or the data you want to store, and pay for the resources you actually use. For the developer, the server is practically invisible. Ferguson says about 99 percent of Seeka TV's code runs on serverless platforms. Thats reduced the time spent managing the cloud environment to practically zero, he says. Its not just startups using serverless. Motorola Solutions uses serverless computing for some of its work with law enforcement and public safety customers, says executive Andrew Sinclair. ""Serverless functions allow us to spend more time on developing new features and less time on provisioning and managing servers,"" he says. Cloud management company Cloudability saw a nearly seven-fold increase in serverless usage among its customers in the last quarter of 2017, compared with the prior quarter, according to a report the company released earlier this year. ""Everybody knows about serverless, and everyone is thinking about where to use it,"" says Craig Lowery, research director at industry analysis firm Gartner. 'Function as a Service' Serverless computing has been around for years, but only recently has it become viable to create entire applications based on it. Over the past decade, programmers have shifted more of the code for running applications from servers to either your phone or your browser. Many developers would prefer not to maintain servers for the few things that can't run on the users device, like data storage and password authentication. The earliest serverless services, which were designed to handle specific tasks, helped developers outsource these sorts of things. Amazon's storage service S3, for example, allows you to upload data without worrying about how many servers it's stored on, and only pay for what you use. Twilio offers a service that lets developers use its computing infrastructure to send text messages to users, without installing or configuring specific servers. A newer service called Auth0 handles tasks like password verification. The first wave of serverless computing proved popular with app developers, but developers still needed traditional servers for a lot of their custom code. That changed in 2014 with the launch of ""function-as-a-service"" offerings like Hook.io and Amazon Lambda, which enable developers to upload small chunks of their own code, and pay only for the computing resources that code consumes. That made it possible for companies like Seeka TV to rely almost exclusively on serverless computing. Relying on serverless computing presents challenges. For one, developers must write software differently than if they planned to run an application on a desktop or traditional cloud service. And some services, including Amazon Lambda limit the size of a code package, meaning developers of large applications must divide them into pieces. Thats complicated for existing applications, so serverless is more suited for creating new applications that are built with this sort of modularity in mind. Performance can also be an issue for serverless computing. A traditional, server-based application typically runs non-stop, ready to answer any query from a user. But services like Lambda stop running code that isn't in use and only load it back into memory when it's needed. That can slow response time for users. Additional delays develop as different serverless components communicate with one another over a network. Sinclair says Motorola Solutions shies away from serverless computing for applications that need quick responses after lengthy periods of inactivity. That means there will always be a market for more conventional cloud computing services, says Mango Capital founder Robin Vasan, who has invested both in serverless companies like Netlify and companies that offer tools for managing more traditional cloud services. ""I think serverless is the future, but it's not the future for everything,"" he says. ""Not every function should live in a serverless framework."" Ferguson admits that serverless computing isn't right for every problem, but says the sorts of performance issues that Vasan describes are rarely a problem for Seeka TV. Its code is used often enough that Amazon Lambda rarely, if ever, has to shut it down. Meanwhile, an ecosystem of tools is emerging to help with the challenges of serverless computing. Motorola Solutions uses a service called Twistlock to monitor security of its serverless code, for example. Lowery and Ferguson liken serverless computing today to "" object-oriented programming,"" which made it easier for software developers to reuse code, in the 1980s. It took time for developers to learn the new approach, and for the tools and other resources to mature. By the 1990s, though, it became the default, but not exclusive, approach to programming. They think serverless will follow a similar path. ""Any application that can be made serverless, will be made serverless,"" Lowery says. ""If it won't work as serverless, then you'd look at other options."" Ferguson's students are already there. He still teaches students to build software using methods other than serverless computing. ""But it feels like I'm punishing the students,"" he says. ""Because serverless is so much easier."" More Great WIRED Stories","There still servers, but this cloud computing approach lets you turn services on and off more quickly, and only pay for what you use.",2018-11-17T12:00:00Z,"{'id': 'wired', 'name': 'Wired'}",The Promise of (Practically) ‘Serverless Computing’,https://www.wired.com/story/promise-practically-serverless-computing/,https://media.wired.com/photos/5bef0ff1ab39b96abf26eeda/191:100/pass/ServerlessComputing.jpg,wired,Wired,cloud,1
Frederic Lardinois,"Nvidia today announced that its Quadro Virtual Machine Workstation (vWS) is now available in the Microsoft Azure Marketplace. The promise of the Quadro vWS it to allow businesses to run high-end graphics applications in the cloud, using any of the Nvidia’s high-end and mid-level cloud  GPUs like the P100, V100, P4 or P40. For the Azure cloud, this specifically means that the Quadro vWS can use Nvidia’s Tesla GPUs with 24GB of frame buffer per GPU.
Were focused on delivering the best and broadest range of GPU-accelerated capabilities in the public cloud, said Talal Alqinaw, senior director of Microsoft Azure, in today’s announcement. NVIDIA Quadro vWS expands customer choice of GPU offerings on Azure to bring powerful professional workstations in the cloud to meet the needs of the most demanding applications from any device, anywhere.
The promise of a virtual workstation, of course, is that you can easily spin them up and down as needed and only pay for when they are running. And all the underlying infrastructures is managed by somebody else, as are software and driver updates, in addition to all the financial calculations that come into effect when you are renting workstations in the cloud.
Nvidia argues that these workstations should be especially of interested to users in industries like architecture, entertainment, oil and gas and manufacturing.
It’s worth noting that Microsoft’s own Windows Virtual Desktop on Azure also supports the Quadro vWS on Tesla GPUs. Indeed, virtual desktops and enabling GPUs for them seems to have been an area of focus for the Azure team in recent months.","Nvidia today announced that its Quadro Virtual Machine Workstation (vWS) is now available in the Microsoft Azure Marketplace. The promise of the Quadro vWS it to allow businesses to run high-end graphics applications in the cloud, using any of the Nvidia’s hi…",2019-01-29T17:00:27Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Nvidia’s Quadro Virtual Workstations are now available on Azure,http://techcrunch.com/2019/01/29/nvidias-quadro-virtual-workstations-are-now-available-on-azure/,https://techcrunch.com/wp-content/uploads/2019/01/ces-2019-0028.jpg?w=600,techcrunch,TechCrunch,cloud,1
Ron Miller,"Egnyte launched in 2007 just two years after Box, but unlike its enterprise counterpart, which went all-cloud and raised hundreds of millions of dollars, Egnyte saw a different path with a slow and steady growth strategy and a hybrid niche, recognizing that companies were going to keep some content in the cloud and some on prem. Up until today it had raised a rather modest $62.5 million, and hadn’t taken a dime since 2013, but that all changed when the company announced a whopping $75 million investment. The entire round came from a single investor, Goldman Sachs’ Private Capital Investing arm, a part of Goldmans Special Situations group. Holger Staude, vice president of Goldman Sachs Private Capital Investing will join Egnyte’s board under the terms of the deal. He says Goldman liked what it saw, a steady company poised for bigger growth with the right influx of capital. In fact, the company has had more than eight straight quarters of growth and have been cash flow positive since Q4 in 2016. “We were impressed by the strong management team and the companys fiscal discipline, having grown their top line rapidly without requiring significant outside capital for the past several years. They have created a strong business model that we believe can be replicated with success at a much larger scale,” Staude explained. Company CEO Vineet Jain helped start the company as a way to store and share files in a business context, but over the years, he has built that into a platform that includes security and governance components. Jain also saw a market poised for growth with companies moving increasing amounts of data to the cloud. He felt the time was right to take on more significant outside investment. He said his first step was to build a list of investors, but Goldman shined through, he said. “Goldman had reached out to us before we even started the fundraising process. There was inbound interest. They were more aggressive compared to others. Given there was prior conversations, the path to closing was shorter,” he said. He wouldn’t discuss a specific valuation, but did say they have grown 6x since the 2013 round and he got what he described as “a decent valuation.” As for an IPO, he predicted this would be the final round before the company eventually goes public. “This is our last fund raise. At this level of funding, we have more than enough funding to support a growth trajectory to IPO,” he said. Philosophically, Jain has always believed that it wasn’t necessary to hit the gas until he felt the market was really there. “I started off from a point of view to say, keep building a phenomenal product. Keep focusing on a post sales experience, which is phenomenal to the end user. Everything else will happen. So this is where we are,” he said. Jain indicated the round isn’t about taking on money for money’s sake. He believes that this is going to fuel a huge growth stage for the company. He doesn’t plan to focus these new resources strictly on the sales and marketing department, as you might expect. He wants to scale every department in the company including engineering, posts-sales and customer success. Today the company has 450 employees and more than 14,000 customers across a range of sizes and sectors including Nasdaq, Thoma Bravo, AppDynamics and Red Bull. The deal closed at the end of last month.","Egnyte launched in 2007 just two years after Box, but unlike its enterprise counterpart, which went all-cloud and raised hundreds of millions of dollars, Egnyte saw a different path with a slow and steady growth strategy and a hybrid niche, recognizing that c…",2018-10-10T13:00:19Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Egnyte hauls in $75M investment led by Goldman Sachs,http://techcrunch.com/2018/10/10/egnyte-hauls-in-75m-investment-led-by-goldman-sachs/,https://techcrunch.com/wp-content/uploads/2018/10/GettyImages-847311142-1.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"It’s been less than six months since Adobe acquired commerce platform Magento for $1.68 billion and today, at Magento’s annual conference, the company announced the first set of integrations that bring the analytics and personalization features of Adobe’s Experience Cloud to Magento’s Commerce Cloud. In many ways, the acquisition of Magento helps Adobe close the loop in its marketing story by giving its customers a full spectrum of services that go from analytics, marketing and customer acquisition all the way to closing the transaction. It’s no surprise then that the Experience Cloud and Commerce Cloud are growing closer to, in Adobe’s words, “make every experience shoppable.” “From the time that this company started to today, our focus has been pretty much exactly the same,” Adobe’s SVP of Strategic Marketing Aseem Chandra told me. “This is, how do we deliver better experiences across any channel in which our customers are interacting with a brand? If you think about the way that customers interact today, every experience is valuable and important. […] It’s no longer just about the product, it’s more about the experience that we deliver around that product that really counts.” So with these new integrations, Magento Commerce Cloud users will get access to an integration with Adobe Target, for example, the company’s machine learning-based tool for personalizing shopping experiences. Similarly, they’ll get easy access to predictive analytics from Adobe Analytics to analyze their customers’ data and predict future churn and purchasing behavior, among other things. These kinds of AI/ML capabilities were something Magento had long been thinking about, Magento’s former CEO and new Adobe SVP fo Commerce Mark Lavelle told me, but it took the acquisition by Adobe to really be able to push ahead with this. “Where the world’s going for Magento clients — and really for all of Adobe’s clients — is you can’t do this yourself,” he said. “you need to be associated with a platform that has not just technology and feature functionality, but actually has this living and breathing data environment that that learns and delivers intelligence back into the product so that your job is easier. That’s what Amazon and Google and all of the big companies that we’re all increasingly competing against or cooperating with have. They have that type of scale.” He also noted that at least part of this match-up of Adobe and Magento is to give their clients that kind of scale, even if they are small- or medium-sized merchants. The other new Adobe-powered feature that’s now available is an integration with the Adobe Experience Manager. That’s Adobe’s content management tool that itself integrates many of these AI technologies for building personalized mobile and web content and shopping experiences. “The goal here is really in unifying that profile, where we have a lot of behavioral information about our consumers,” said Aseem. “And what Magento allows us to do is bring in the transactional information and put those together so we get a much richer view of who the consumers are and how we personalize that experience with the next interaction that they have with a Magento-based commerce site.” It’s worth noting that Magento is also launching a number of other new features to its Commerce Cloud that include a new drag-and-drop editing tool for site content, support for building Progressive Web Applications, a streamlined payment tool with improved risk management capabilities, as well as a new integration with the Amazon Sales Channel so Magento stores can sync their inventory with Amazon’s platform. Magneto is also announcing integrations with Google’s Merchant Center and Advertising Channels for Google Smart Shopping Campaigns.","It’s been less than six months since Adobe acquired commerce platform Magento for $1.68 billion and today, at Magento’s annual conference, the company announced the first set of integrations that bring the analytics and personalization features of Adobe’s Exp…",2018-10-09T10:00:16Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","After its acquisition, Magento starts integrating Adobe’s personalization and analytics tools",http://techcrunch.com/2018/10/09/after-its-acquisition-magento-starts-integrating-adobes-personalization-and-analytics-tools/,https://techcrunch.com/wp-content/uploads/2018/03/gettyimages-535058991.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google’s Cloud Services Platform for managing hybrid clouds that span on-premise data centers and the Google cloud, is coming out of beta today. The company is also changing the product’s name to Anthos, a name that either refers to a lost Greek tragedy, the name of an obscure god in the Marvel universe, or rosemary. That by itself would be interesting but minor news. What makes this interesting is that Google also today announced that Anthos will run on third-party clouds as well, including AWS and Azure.
“We will support Anthos and AWS and Azure as well, so people get one way to manage their application and that one way works across their on-premise environments and all other clouds,” Google’s senior VP for its technical infrastructure, Urs Hölzle, explained in a press conference ahead of today’s announcement.
So with Anthos, Google will offer a single managed service that will let you manage and deploy workloads across clouds, all without having to worry about the different environments and APIs. That’s a big deal and one that clearly delineates Google’s approach from its competitors’. This is Google, after all, managing your applications for you on AWS and Azure.
“You can use one consistent approach — one open-source based approach — across all environments,” Hölzle said. “I can’t really stress how big a change that is in the industry, because this is really the stack for the next 20 years, meaning that it’s not really about the three different clouds that are all randomly different in small ways. This is the way that makes these three cloud — and actually on-premise environments, too — look the same.”
Anthos/Google Cloud Services Platform is based on the Google Kubernetes Engine, as well as other open source projects like the Istio service mesh. It’s also hardware agnostic, meaning that users can take their current hardware and run the service on top of that without having to immediately invest in new servers.
Why is Google doing this? “We hear from our customers that multi-cloud and hybrid is really an acute pain point,” Hölzle said. He noted that containers are the enabling technology for this but that few enterprises have developed a unifying strategy to manage these deployments and that it takes expertise in all major clouds to get the most out of them.
Enterprises already have major investments in their infrastructure and created relationships with their vendors, though, so it’s no surprise that Google is launching Anthos with over 30 major hardware and software partners that range from Cisco to Dell EMC, HPE and VMWare, as well as application vendors like Confluent, Datastax, Elastic, Portworx, Tigera, Splunk, GitLab, MongoDB and others.
Anthos is a subscription-based service, with the list prices starting at $10,000/month per 100 vCPU block. Enterprise prices then to be up for negotiation, though, so many customers will likely pay less.
It’s one thing to use a service like this for new applications, but many enterprises already have plenty of line-of-business tools that they would like to bring to the cloud as well. For them, Google is launching the first beta of Anthos Migrate today. This service will auto-migrate VMs from on-premises or other clouds into containers in the Google Kubernetes Engine. The promise here is that this is essentially an automatic process and once the container is on Google’s platform, you’ll be able to use all of the other features that come with the Anthos platform, too.
Google’s Hölzle noted that the emphasis here was on making this migration as easy as possible. “There’s no manual effort there,” he said.","Google’s Cloud Services Platform for managing hybrid clouds that span on-premise data centers and the Google cloud, is coming out of beta today. The company is also changing the product’s name to Anthos, a name that either refers to a lost Greek tragedy, the …",2019-04-09T16:00:45Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google’s hybrid cloud platform is coming to AWS and Azure,http://techcrunch.com/2019/04/09/googles-anthos-hybrid-cloud-platform-is-coming-to-aws-and-azure/,https://techcrunch.com/wp-content/uploads/2019/04/GettyImages-910201592.jpg?w=587,techcrunch,TechCrunch,cloud,1
Sarah Perez,"Twitter today announced a new collaboration with Google that will see it moving a portion of infrastructure to Google’s Cloud Platform. The move is another high-profile win for Google in the cloud computing market, following its recent deal with Fitbit. Specifically, Twitter says it’s moving its cold data storage and its flexible compute Hadoop clusters – something it believes it needs to do in order to keep scaling its business. Currently, Twitter’s Hadoop clusters are housed in its own physical data centers, the company told TechCrunch. This is a relatively small percentage of its total infrastructure, however – the bulk of its infrastructure will still remain in Twitter’s own data centers, we understand. Twitter declined to share the impact of the move, in terms of cost savings or financial impacts. The company publicly announced the news in a blog post this afternoon, where it noted it’s been assessing its platform and infrastructure needs over the past few years. It said it needs to be well-positioned to keep up with its growth. Even though Twitter’s user growth has been flat, it sees hundreds of millions of tweets sent every day. “The Hadoop compute system is the core of our data platform, and Twitter runs multiple large Hadoop clusters that are among the biggest in the world,” explained Twitter CTO Parag Agrawal, in the announcement. “In fact, our Hadoop file systems host more than 300PB of data across tens of thousands of servers,” he said. Agrawal said that when the migration is complete, it will enable “faster capacity provisioning; increased flexibility; access to a broader ecosystem of tools and services; improvements to security; and enhanced disaster recovery capabilities.” “Architecturally, we will also be able to separate compute and storage for this class of Hadoop workloads, which has a number of long-term scaling and operational benefits,” he added. Twitter isn’t saying how long the migration will take. The move to Google Cloud comes at a time when there’s heavy competition among cloud service providers – and Amazon is leading in terms of revenue. Grabbing a portion of Twitter’s business, then, is another big name win for Google, following its collaboration with Fitbit, announced just days ago. “There is strong alignment with Twitter’s engineering strategy to meet the demands of its platform and the services Google Cloud offers at a global scale,” said Brian Stevens, Google Cloud CTO, in a statement. “Google Cloud Platform’s data solutions and trusted infrastructure will provide Twitter with the technical flexibility and consistency that its platform requires, and we look forward to an ongoing technical collaboration with their team.”","Twitter today announced a new collaboration with Google that will see it moving a portion of infrastructure to Google’s Cloud Platform. The move is another high-profile win for Google in the cloud computing market, following its recent deal with Fitbit. Speci…",2018-05-03T17:00:45Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Twitter is moving a portion of its infrastructure to Google Cloud,http://techcrunch.com/2018/05/03/twitter-is-moving-a-portion-of-its-infrastructure-to-google-cloud/,https://techcrunch.com/wp-content/uploads/2016/08/twitter-impressionist.jpg?w=711,techcrunch,TechCrunch,cloud,1
Ron Miller,"Kubernetes, the open source container orchestration tool, does a great job of managing a single cluster, but Upbound, a new Seattle-based startup wants to extend this ability to manage multiple Kubernetes clusters across multi-cloud environment. It’s a growing requirement as companies deploy ever-larger numbers of clusters and choose a multi-vendor approach to cloud infrastructure services. Today, the company announced a $9 million Series A investment led by GV (formerly Google Ventures) along with numerous unnamed angel investors from the cloud-native community. As part of the deal, GV’s Dave Munichiello will be joining the company board of directors. It’s important to note that the company is currently working on the product and could be a year away from a release, but the vision is certainly compelling. As Upbound CEO and founder Bassam Tabbara says, his company’s solution could allow customers to run, scale and optimize their workloads across clusters, regions and clouds as a single entity. That level of control could enable them to set rules and policies across those clusters and clouds. For example, a customer might control costs by creating a rule to find the cloud with lowest cost for processing a given job, or provide failover control across regions and clouds — all automatically. It would provide the general ability to have highly granular control across multiple environments that isn’t really possible now, Tabarra explained. That vision of enterprise portability is certainly something that caught the eye of GV’s Munichiello. “Upbound presents a credible approach to multi-cloud computing built on the success of Kubernetes, and as a response to the growing enterprise demand for hybrid and multi-cloud environments,” he said in a statement. Companies are working with multiple Kubernetes clusters today. As an example, CERN, the European physics organization is running 210 clusters. JD.com, the Chinese shopping site has over 20,000 servers running Kubernetes. The largest cluster is made up of 5000 servers. As these projects scale, they require a tool to help manage their workloads across these larger environments. The company’s founder isn’t new to cloud-native computing or open source. Tabarra was part of the team responsible for producing the open source project, Rook, an offshoot of Kubernetes and a Cloud Native Computing Foundation Sandbox project. Rook helps orchestrate distributed storage systems running in cloud native environments in a similar way that Kubernetes does for containerized environments. That project provided some of the ground work for what Upbound is trying to do on a broader scale beyond pure storage. The computing world is suddenly all about abstraction. We started with virtual machines, which allowed you take an individual server and make it into multiple virtual machines. That led to containers, which could take the same machine in let you launch hundreds of containers. Kubernetes is an open source container orchestration tool that has rapidly gained acceptance by allowing operations to treat a cluster of Kubernetes nodes as a single entity, making it much easier to launch and manage containers. Upbound launched last Fall and currently has 8 employees, but Tabbara says they are actively seeking new engineers. The nature of their business is about distributed workloads and he says the workforce will be similar. They won’t have to work in Seattle. He says the plan is to use and contribute to open source whenever possible and to open source parts of the product when it’s available.","Kubernetes, the open source container orchestration tool, does a great job of managing a single cluster, but Upbound, a new Seattle-based startup wants to extend this ability to manage multiple Kubernetes clusters across multi-cloud environment. It’s a growin…",2018-05-02T15:00:09Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Upbound grabs $9 M Series A to automate multi-cloud management,http://techcrunch.com/2018/05/02/upbound-grabs-9-m-series-a-to-automate-multi-cloud-management/,https://techcrunch.com/wp-content/uploads/2018/05/gettyimages-685035919.jpg?w=667,techcrunch,TechCrunch,cloud,1
Romain Dillet,"French cloud hosting company Scaleway is rolling out new instances with an Nvidia Tesla P100 GPU. The company is opting for simple pricing with a single configuration that costs 1 per hour ($1.13).
Many companies now use GPU instances to train models for their machine learning-powered app or service. Some companies also leverage these cloud instances to generate 3D models and other GPU-intensive tasks. If you dont want to buy a bunch of expensive GPUs, you can leverage GPUs on demand with your favorite cloud hosting company. Once youre done, you can shut down your cloud instance.
Scaleways RENDER-S instance is powered by an Nvidia Tesla P100 with 16GB of HBM2 memory. It comes with 45GB of RAM, 400GB of storage (local NVMe SSD so it should be super fast for video processing) and 10 cores of an Intel Xeon Gold 6148 with the AVX-512 instruction set. If you plan on keeping your instance running for a while, the RENDER-S instance costs 1 per hour or 500 ($567) per month, whichever is lower.
On Google Cloud, you can get an on-demand instance with an Nvidia P100 for $1.60 per hour in Europe and Asia, or $1.46 per hour in the U.S. Microsoft also sells cloud instances with a P100 GPU for $2.07 per hour. It seems like Scaleway is competing with those offers in particular.
Amazon also has GPU instances on Amazon Web Services. You can find instances with an Nvidia Tesla V100 GPU, a more powerful graphics processing unit. Those instances are also more expensive at over $3 per hour prices vary slightly depending on the data center. You can also find AWS instances with older GPUs, but they dont perform that well.
OVH also offers instances with Tesla V100 GPUs for 2.30 per hour ($2.61). I couldnt find GPU instances on DigitalOcean or Linode.
Most people probably dont need GPU instances. But it can be an important factor for companies looking for their next cloud provider. If you want to centralize everything under one bill, you need to pick a company with a large offering.",French cloud hosting company Scaleway is rolling out new instances with an Nvidia Tesla P100 GPU. The company is opting for simple pricing with a single configuration that costs €1 per hour ($1.13). Many companies now use GPU instances to train models for the…,2019-03-07T17:19:51Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Scaleway releases cloud GPU instances for €1 per hour,http://techcrunch.com/2019/03/07/scaleway-releases-cloud-gpu-instances-for-e1-per-hour/,https://techcrunch.com/wp-content/uploads/2019/03/data-center-tesla-p100-social-media-1200.jpg?w=762,techcrunch,TechCrunch,cloud,1
Ingrid Lunden,"Multi-cloud architecture is a huge trend in enterprise, and today F5 made a big move to bring its own business closer to it. The company, which provides cloud and security application services, announced that it has acquired NGINX, the commercial company behind the popular open source web server, for $670 million.
We’d actually been hearing murmurs of this acquisition for a while with a pricetag of around $700 million. On top of that, our sources say NGINX was shopping itself around and other companies that had been looking at it included Citrix. That deal fell apart on price.
NGINX had last raised money nine months ago, a $43 million round led by Goldman Sachs to fuel expansion, and had positioned itself as a strong alternative to F5 in recent years. (It had not disclosed its valuation in that round.) F5 itself, by coincidence, was said to have retained Goldman Sachs in 2016 to field acquisition interest in itself, although that never led to anything.
F5s acquisition of NGINX strengthens our growth trajectory by accelerating our software and multi-cloud transformation, said François Locoh-Donou, President &amp; CEO of F5, in a statement. By bringing F5s world-class application security and rich application services portfolio for improving performance, availability, and management together with NGINXs leading software application delivery and API management solutions, unparalleled credibility and brand recognition in the DevOps community, and massive open source user base, we bridge the divide between NetOps and DevOps with consistent application services across an enterprises multi-cloud environment.
Indeed, our sources noted that growth had stalled somewhat at the company, which was one reason for its interest in NGINX. F5 has a market cap of $9.6 billion at the close of markets today. In its last quarterly earnings, the company said its revenues had grown just four percent compared to the year before. NGINX meanwhile has been a juggernaut in providing open source tools for maintaining and running websites since first emerging in 2004 as an alternative to Apache. The company currently runs 375 million websites with some 1,500 paying customers taking additional services like support, load balancing, and API gateway and analytics.
F5 said that it will be merging its own operations with those of NGINX, with current NGINX CEO Gus Robertson, as well as its founders Igor Syosev and Maxim Konovalov all joining the company.
NGINX and F5 share the same mission and vision. We both believe applications are at the heart of driving digital transformation. And we both believe that an end-to-end application infrastructureone that spans from code to customeris needed to deliver apps across a multi-cloud environment, said Robertson, in a statement. Im excited to continue this journey by adding the power of NGINXs open source innovation to F5s ADC leadership and enterprise reach. F5 gains depth with solutions designed for DevOps, while NGINX gains breadth with access to tens of thousands of customers and partners.","Multi-cloud architecture is a huge trend in enterprise, and today F5 made a big move to bring its own business closer to it. The company, which provides cloud and security application services, announced that it has acquired NGINX, the commercial company behi…",2019-03-11T22:28:46Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","F5 acquires NGINX for $670M to move into open-source, multi-cloud services",http://techcrunch.com/2019/03/11/f5-acquires-nginx-for-670m-to-move-into-open-source-multi-cloud-services/,https://techcrunch.com/wp-content/uploads/2018/06/GettyImages-147273767.jpg?w=699,techcrunch,TechCrunch,cloud,1
Romain Dillet,"Its not every day that you read a headline with both Google and Microsoft. Google is announcing today that its managed database service Cloud SQL will soon support Microsoft SQL Server. The company showed a sneak preview at its Google Cloud Next conference.
The message is clear  if your company uses Microsoft SQL Server database, you dont have to use Microsoft Azure. Your database will work just fine on Googles Cloud SQL.
Google already supported Microsoft SQL Server in traditional virtual machines  so you had to manage it yourself. If you have a license and you want Google to manage your database for you, Cloud SQL will be able to do it. No backup, no manual replication, no patch, etc.
Many enterprise customers still rely heavily on a traditional on-prem server infrastructure. Google is trying to remove all the obstacles you could find when moving to the cloud.
In other Cloud SQL news, customers who use PostgreSQL can now use version 11 of PostgreSQL. Amazon RDS also supports version 11.
Finally, Googles managed NoSQL database service Cloud Bigtable now supports multi-region replication. That feature was already available in beta. You can now safely read and write your NoSQL data from multiple regions at the same time.",It’s not every day that you read a headline with both Google and Microsoft. Google is announcing today that its managed database service Cloud SQL will soon support Microsoft SQL Server. The company showed a sneak preview at its Google Cloud Next conference. …,2019-04-10T16:22:04Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google’s managed database service to support Microsoft SQL Server,http://techcrunch.com/2019/04/10/googles-managed-database-service-to-support-microsoft-sql-server/,https://techcrunch.com/wp-content/uploads/2019/04/IMG_20190410_092252.jpg?w=533,techcrunch,TechCrunch,cloud,1
Danny Crichton,"Tomorrow: Google Cloud Next conference call
Join TechCrunchs enterprise mavens Frederic Lardinois and Ron Miller as they discuss whats happening on the ground and at the (I am sure, very exciting) parties this week at Google Cloud Next in SF. They will talk live tomorrow at 1pm EDT / 10am PDT, and be sure to check your inbox one hour before for the dial-in information.
When to ditch that nightmare customer
No, we are not firing any Extra Crunch members yet. But multi-time founder Joe Procopio has a strong view that not only should you identify your worst customers, you should fire them, stat. Heres a preview of his personal journey learning this fundamental tenet of startups:","Tomorrow: Google Cloud Next conference call Join TechCrunch’s enterprise mavens Frederic Lardinois and Ron Miller as they discuss what’s happening on the ground and at the (I am sure, very exciting) parties this week at Google Cloud Next in SF. They will talk…",2019-04-10T20:58:39Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","AI dumbos, Niantic, mobility, design, and Google Next",http://techcrunch.com/2019/04/10/ai-dumbos-niantic-mobility-design-and-google-next/,https://techcrunch.com/wp-content/uploads/2019/02/extra-crunch-featured-image.jpg?w=753,techcrunch,TechCrunch,cloud,1
Kirsten Korosec,"Microsoft has made a strategic investment in ride-hailing and on-demand services company Grab as part of a deal that includes collaborating on big data and AI projects. Under the agreement, Singapore-based Grab will adopt Microsoft Azure as its preferred cloud platformAzure cloud computing service. Microsoft and Grab didn’t disclose financial terms. The idea behind the tie-up is for Grab to use Microsoft’s product to scale its own digital platform, which has grown beyond ride-hailing. Grab also has its own payment service and makes food deliveries. Our partnership with Grab opens up new opportunities to innovate in both a rapidly evolving industry and growth region, said Peggy Johnson, executive vice president at Microsoft, said in a statement. The companies said they’ll also work on several “innovative deep technology projects,” including new authentication measures such as facial recognition with built-in AI (for drivers and customers who opt in) to replace the old-school method of checking IDs. The companies will also investigate uses for natural language processing, machine learning and AI in Grab’s platform such as map creation, fraud detection services and the ability for passengers to take a photo of their current location and have it translated into an actual address for the driver. Grab and Microsoft already have a loose connection of sorts via Toyota. Toyota, which recently made a $1 billion investment into Grab (and another $1 billion from other investors), has been working with Microsoft for a couple of years now. In 2017, Microsoft agreed to license its patents for Internet-connected cars to Toyota, a milestone deal at the time that signaled the technology company’s willingness and interest to expand its transportation-related services. The two companies struck a deal in April 2016 to create a data analytics company called Toyota Connect focused on bringing new Internet-connected services to cars. Toyota Connect uses Microsoft Azure to capture and evaluate data and help develop new products for drivers, businesses with car fleets, and dealers.","Microsoft has made a strategic investment in ride-hailing and on-demand services company Grab as part of a deal that includes collaborating on big data and AI projects. Under the agreement, Singapore-based Grab will adopt Microsoft Azure as its preferred clou…",2018-10-09T01:28:06Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Microsoft invests in Grab to bring AI and big data to on-demand services,http://techcrunch.com/2018/10/08/microsoft-invests-in-grab-to-bring-ai-and-big-data-to-on-demand-services/,https://techcrunch.com/wp-content/uploads/2017/07/grabcar.jpg?w=533,techcrunch,TechCrunch,cloud,1
Ron Miller,"Just a day after Google decided to drop out of the Pentagon’s massive $10 billion, 10-year JEDI cloud contract bidding, Microsoft announced increased support services for government clients. In a long blog post, the company laid out its government focused cloud services. While today’s announcement is not directly related to JEDI per se, the timing is interesting just three days ahead of the October 12th deadline for submitting RFPs. Today’s announcement is about showing just how comprehensive the company’s government-specific cloud services are. In a blog post, Microsoft corporate vice president for Azure, Julia White made it clear the company was focusing hard on the government business. “In the past six months we have added over 40 services and features to Azure Government, as well as publishing a new roadmap for the Azure Government regions providing ongoing transparency into our upcoming releases,” she wrote. “Moving forward, we are simplifying our approach to regulatory compliance for federal agencies, so that our government customers can gain access to innovation more rapidly. In addition, we are adding new options for buying and onboarding cloud services to make it easier to move to the cloud. Finally, we are bringing an array of new hybrid and edge capabilities to government to ensure that government customers have full access to the technology of the intelligent edge and intelligent cloud era,” White added. While much of the post was around the value proposition of Azure in general such as security, identity, artificial intelligence and edge data processing services, there were a slew of items aimed specifically at the government clients. For starters, the company is increasing its FedRAMP compliance, a series of regulations designed to ensure vendors deliver cloud services securely to federal government customers. Specifically Microsoft is moving from FedRAMP moderate to high ratings on 50 services. “By taking the broadest regulatory compliance approach in the industry, were making commercial innovation more accessible and easier for government to adopt,” White wrote. In addition, Microsoft announced it’s expanding Azure Secret Regions, a solution designed specifically for dealing with highly classified information in the cloud. This one appears to take direct aim at JEDI. “We are making major progress in delivering this cloud designed to meet the regulatory and compliance requirements of the Department of Defense and the Intelligence Community. Today, we are announcing these newest regions will be available by the end of the first quarter of 2019. In addition, to meet the growing demand and requirements of the U.S. Government, we are confirming our intent to deliver Azure Government services to meet the highest classification requirements, with capabilities for handling Top Secret U.S. classified data,” White wrote. The company’s announcements, which included many other pieces that have been previously announced, is clearly designed to show off its government chops at a time where a major government contract is up for grabs. The company announced Azure Stack for Government in August, another piece mentioned in this blog post.","Just a day after Google decided to drop out of the Pentagon’s massive $10 billion, 10-year JEDI cloud contract bidding, Microsoft announced increased support services for government clients. In a long blog post, the company laid out its government focused clo…",2018-10-09T14:48:25Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Microsoft shows off government cloud services with JEDI due date imminent,http://techcrunch.com/2018/10/09/microsoft-shows-off-government-cloud-services-with-jedi-due-date-imminent/,https://techcrunch.com/wp-content/uploads/2016/09/julia_white_microsoft.jpg?w=602,techcrunch,TechCrunch,cloud,1
Ron Miller,"Let’s start with a basic premise that the vast majority of the world’s work loads remain in private data centers. Cloud infrastructure vendors are working hard to shift those workloads, but technology always moves a lot slower than we think. That is the lens through which many cloud companies operate. The idea that you operate both on prem and in the cloud with multiple vendors is the whole idea behind the notion of the hybrid cloud. It’s where companies like Microsoft, IBM, Dell and Oracle are placing their bets. These died-in-the-wool enterprise companies see their large customers making a slower slog than you would imagine to the cloud, and they want to provide them with the tools and technologies to manage across both worlds, while helping them shift when they are ready. Cloud native computing developed in part to provide a single management fabric across on prem and cloud, freeing IT from having two sets of tools and trying somehow to bridge the gap between the two worlds. What every cloud vendor wants Red Hat — you know, that company that was sold to IBM for $34 billion this week — has operated in this world. While most people think of the company as the one responsible for bringing Linux to the enterprise, over the last several years, it has been helping customers manage this transition and build applications that could live partly on-prem and partly in the cloud. As an example, it has built OpenShift, its version of Kubernetes. As CEO Jim Whitehurst told me last year, “Our hottest product is OpenShift. People talk about containers and they forget its a feature of Linux,” he said. That is an operating system that Red Hat knows a thing or two about. With Red Hat in the fold, IBM can contend that being open source, they can build modern applications on top of open source tools and run them on IBM’s cloud or any of their competitors, a real hybrid approach. Microsoft has a huge advantage here, of course, because it has a massive presence in the enterprise already. Many companies out there could be described as Microsoft shops, and for those companies moving from on-prem Microsoft to cloud Microsoft represents a less daunting challenge than starting from scratch. Oracle has brings similar value with its core database products. Companies using Oracle databases — just about everyone — might find it easier to move that valuable data to Oracle’s cloud, although the numbers don’t suggest that’s necessarily happening (and Oracle has stopped breaking out its cloud revenue). Dell, which spent $67 billion for EMC, making the Red Hat purchase pale by comparison, has been trying to pull together a hybrid solution by combining VMware, Pivotal and Dell/EMC hardware. Cloud vendors reporting You could argue that hybrid is a temporary state, that at some point, the vast majority of workloads will eventually be running in the cloud and the hybrid business as we know it today will continually shrink over time. We are certainly seeing cloud infrastructure revenue skyrocketing with no signs of slowing down as more workloads move to the cloud. In their latest earnings reports, those who break such things out, the successful ones, reported growth in their cloud business. It’s important to note that these companies define cloud revenue in different ways, but you can see the trend is definitely up. AWS reported revenue of $6.7 billion in revenue for the quarter, up from $4.58 billion the previous year. Microsoft Intelligent Cloud, which incorporates things like Azure and server products and enterprise services, was at $8.6 billion, up from $6.9 billion IBM Technology Services and Cloud Platforms, which includes infrastructure services, technical support services and integration software reported revenue of $8.6 billion, up from $8.5 billion the previous year. Others like Oracle and Google didn’t break out their cloud revenue. Show me the money All of this is to say, there is a lot of money on the table here and companies are moving more workloads at an increasingly rapid pace. You might also have noticed that IBM’s growth is flat compared to the others. Yesterday in a call with analysts and press, IBM CEO Ginni Rometti projected that revenue for the hybrid cloud (however you define that) could reach $1 trillion by 2020. Whether that number is exaggerated or not, there is clearly a significant amount of business here and IBM might see it as a way out of its revenue problems, especially if they can leverage consulting/ services along with it. There is probably so much business that there is room for more than one winner, but if you asked before Sunday if IBM had a shot in this mix against its formidable competitors, especially those born in the cloud like AWS and Google, most probably wouldn’t have given them much chance. When Red Hat eventually joins forces with IBM, it at least gives their sales teams a compelling argument, one that could get them into the conversation and that is probably why they were willing to spend so much money to get it. It puts them back in the game, and after years of struggling, that is something. And in the process, it has stirred up the hybrid cloud market in a way we didn’t see coming last week before this deal.","Let’s start with a basic premise that the vast majority of the world’s work loads remain in private data centers. Cloud infrastructure vendors are working hard to shift those workloads, but technology always moves a lot slower than we think. That is the lens …",2018-10-30T21:02:43Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",The hybrid cloud market just got a heck of a lot more compelling,http://techcrunch.com/2018/10/30/the-hybrid-cloud-market-just-got-a-heck-of-a-lot-more-compelling/,https://techcrunch.com/wp-content/uploads/2018/10/GettyImages-170882358.jpg?w=574,techcrunch,TechCrunch,cloud,1
Ron Miller,"Celonis has been helping companies analyze and improve their internal processes using machine learning. Today the company announced it was providing that same solution as a cloud service with a few nifty improvements you won’t find on prem. The new approach, called Celonis Intelligent Business Cloud, allows customers to analyze a workflow, find inefficiencies and offer improvements very quickly. Companies typically follow a workflow that has developed over time and very rarely think about why it developed the way it did, or how to fix it. If they do, it usually involves bringing in consultants to help. Celonis puts software and machine learning to bear on the problem. Co-founder and CEO Alexander Rinke says that his company deals with massive volumes of data and moving all of that to the cloud makes sense. “With Intelligent Business Cloud, we will unlock that [on prem data], bring it to the cloud in a very efficient infrastructure and provide much more value on top of it,” he told TechCrunch. The idea is to speed up the whole ingestion process, allowing a company to see the inefficiencies in their business processes very quickly. Rinke says it starts with ingesting data from sources such as Salesforce or SAP and then creating a visual view of the process flow. There may be hundreds of variants from the main process workflow, but you can see which ones would give you the most value to change, based on the number of times the variation occurs. Screenshot: Celonis By packaging the Celonis tools as a cloud service, they are reducing the complexity of running and managing it. They are also introducing an app store with over 300 pre-packaged options for popular products like Salesforce and ServiceNow and popular process like order to cash. This should also help get customers up and running much more quickly. New Celonis App Store. Screenshot: Celonis The cloud service also includes an Action Engine, which Rinke describes as a big step toward moving Celonis from being purely analytical to operational. “Action Engine focuses on changing and improving processes. It gives workers concrete info on what to do next. For example in process analysis, it would notice on time delivery isnt great because order to cash is to slow. It helps accelerate changes in system configuration,” he explained. Celonis Action Engine. Screenshot: Celonis The new cloud service is available today. Celonis was founded in 2011. It has raised over $77 million. The most recent round was a $50 million Series B on a valuation over $1 billion.","Celonis has been helping companies analyze and improve their internal processes using machine learning. Today the company announced it was providing that same solution as a cloud service with a few nifty improvements you won’t find on prem. The new approach, …",2018-10-15T12:52:38Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Celonis brings intelligent process automation software to cloud,http://techcrunch.com/2018/10/15/celonis-brings-intelligent-process-automation-software-to-cloud/,https://techcrunch.com/wp-content/uploads/2018/10/GettyImages-684790654.jpg?w=533,techcrunch,TechCrunch,cloud,1
Ron Miller,"Oracle is a traditional tech company that has been struggling to gain traction in the cloud, but it could see blockchain as a way to differentiate itself. At Oracle OpenWorld today it announced the Oracle Blockchain Applications Cloud, a series of four applications designed for transactions-based processing scenarios using Internet of Things as a data source. “Customers struggle with how exactly to go from concepts like smart contracts, distributed ledger and cryptography to solving specific business problems,” Atul Mahamuni, VP of IoT and Blockchain at Oracle told TechCrunch. The company actually introduced a more generalized blockchain as a service offering at OpenWorld last year, but this year they have decided to focus more on specific use cases, announcing four new applications. The blockchain comes into account because of its nature as an irrefutable and immutable record. In cases where there is a dispute over the accuracy of a particular piece of data, the blockchain can provide incontrovertible proof. As for the Internet of Things, that provides data points you can use to provide that proof. Your sensor feeds the data and it (or some reference to it) gets added to the blockchain, leaving no room for doubt. The four applications involve supply chain-transaction data including a track and trace capability to follow a product through its delivery from inception to market, proof of provenance for valuables like drugs, intelligent temperature tracking (what they are calling Intelligent Cold Chain) and warranty and usage tracking. Intelligent Cold chain ensures that a product that is supposed to be kept cold didn’t get exposed to higher than recommended temperatures, while warranty tracking ensures that a product was being used in a proscribed fashion and should be subject to warranty claims. Each of these plays to the some of Oracle’s strengths as a company that builds databases and ERP software. It can draw on the information it tends to collect any way as part of the nature of its business processes and add it to a blockchain and other applications when it makes sense. “So what we do is we we get events and insights from IoT systems, as well as from supply chain ERP data, and we get those insights and translation from all of this and then put them into the blockchain and then do the correlations and artificial intelligence machine learning algorithms on top of those transactions,” Mahamuni explained. This year perhaps even more so than the last couple, Oracle is trying to differentiate itself from the rest of the cloud pack, as it tries to right its cloud business. By building applications on top of base technologies like blockchain, IoT and artificial intelligence, while taking advantage of their domain knowledge around databases and ERP, they are hoping to show customers they can offer something their cloud competitors can’t.","Oracle is a traditional tech company that has been struggling to gain traction in the cloud, but it could see blockchain as a way to differentiate itself. At Oracle OpenWorld today it announced the Oracle Blockchain Applications Cloud, a series of four applic…",2018-10-23T15:05:13Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Oracle delves deeper into blockchain with four new applications,http://techcrunch.com/2018/10/23/oracle-delves-deeper-into-blockchain-with-four-new-applications/,https://techcrunch.com/wp-content/uploads/2018/10/GettyImages-456416190.jpg?w=602,techcrunch,TechCrunch,cloud,1
Kate Clark,"Amazon Web Services (AWS) unveiled its latest updates to security on its cloud services platform today at AWS re:Invent, the company’s annual conference for database storage enthusiasts. AWS Security Hub is a new place for businesses to centrally manage compliance and identify security across AWS environment, says AWS chief executive officer Andy Jassy. The service will help AWS users derive insights from attack patterns and techniques so they can take action more quickly. “This is going to pretty radically change how easy it is to look at what’s happening security-wise across … AWS,” Jassy said. “Whether you’re using AWS security services like Inspector for vulnerability scanning or GuardDuty for network intrusion or Macie for anomalous data patterns or whether you’re using a very large number of third-party software security services in our ecosystem.” AWS has signed up a number of its partners for the initial roll out, including CrowdStrike, McAfee, Symantec and Tenable.","AWS unveiled a new security product today at AWS re:Invent, the company's annual conference for cloud storage enthusiasts.",2018-11-28T17:27:09Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",AWS launches Security Hub to help customers manage security & compliance,http://techcrunch.com/2018/11/28/aws-launches-security-hub-to-help-customers-manage-security-compliance/,https://techcrunch.com/wp-content/uploads/2018/11/Screen-Shot-2018-11-28-at-5.53.12-PM.png?w=736,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google announced a number of updates to its cloud-based database services today. For the most part, we’re not talking about any groundbreaking new products here but all of these updates address specific pain points that enterprises suffer when they move to the cloud. As Google Director of Product Management Dominic Preuss told me ahead of today’s announcements, Google long saw itself as a thought leader in the database space. For the longest time, though, that thought leadership was all about things like the Bigtable paper and didn’t really manifest itself in the form of products. Projects like the globally distributed Cloud Spanner database are now allowing Google Cloud to put its stamp on this market. Preuss also noted that many of Google’s enterprise users often start with lifting and shifting their existing workloads to the cloud. Once they have done that, though, they are also looking to launch new applications in the cloud — and at that point, they typically want managed services that free them from having to do the grunt work of managing their own infrastructure. Today’s announcements mostly fit into this mold of offering enterprises the kind of managed database services they are asking for. The first of these is the beta launch of Cloud Memorystore for Redis, a fully managed in-memory data store for users who need in-memory caching for capacity buffering and similar use cases. Google is also launching a new feature for Cloud Bigtable, the company’s NoSQL database service for big data workloads. Bigtable now features regional replication (or at least it will, once this has rolled out to all users within the next week or so). The general idea here is to give enterprises that previously used Cassandra for their on-premises workloads an alternative in the Google Cloud portfolio and these cross-zone replications increase the availability and durability of the data they store in the service. With this update, Google is also making Cloud SQL for PostgreSQL generally available with a 99.95 percent SLA and it’s adding commit timestamps to Cloud Spanner. What’s next for Google’s database portfolio? Unsurprisingly, Preuss wouldn’t say, but he did note that the company wants to help enterprises move as many of their workloads to the cloud as they can — and for the most part, that means managed services.","Google announced a number of updates to its cloud-based database services today. For the most part, we’re not talking about any groundbreaking new products here but all of these updates address specific pain points that enterprises suffer when they move to th…",2018-04-25T17:18:39Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud expands its bet on managed database services,http://techcrunch.com/2018/04/25/google-cloud-expands-its-bet-on-managed-database-services/,https://techcrunch.com/wp-content/uploads/2015/05/google-cloud-bigtable.jpg?w=711,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google Cloud is launching the first public beta of Cloud Composer today, a new workflow automation tool for developers that’s based on the Apache Airflow project. Typically, IT teams build their own automated workflow as needed, but the result of that is often a mess of different tools and Bash scripts that don’t always work together. Airflow and Cloud Composer allow teams to standardize on a single way of building and orchestrating workflows. Google notes that its new tool, which uses Python as its default language, will allow teams to build workflows across on-premises tools and across multiple clouds and that the open source nature of the project will ensure that developers can take their workflows and use them across platforms. While the service is deeply integrated with the Google Cloud Platform, the team stressed that there is no lock-in. “In building Cloud Composer, we wanted to combine the strengths of Google Cloud Platform with Airflow,” the Cloud Composer team writes in today’s announcement. “We set out to build a service that offers the best of Airflow without the overhead of installing and managing Airflow yourself. As a result, you spend less time on low-value work, such as getting Airflow installed, and more time on what matters: your workflows.” In Airflow — and by extension in Cloud Composer, too — the different tasks and their expected outcomes (as well as what to do when things go wrong), are defined in a so-called Directed Acyclic Graph. These are standard Python files that define the workflow down to its details. You can find the full documentation here. Google notes that it is actively participating in the Airflow community, too, and that it has contributed a number of pull requests already.","Google Cloud is launching the first public beta of Cloud Composer today, a new workflow automation tool for developers that’s based on the Apache Airflow project. Typically, IT teams build their own automated workflow as needed, but the result of that is ofte…",2018-05-01T16:00:35Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","Google launches Cloud Composer, a new workflow automation tool for developers",http://techcrunch.com/2018/05/01/google-launches-cloud-composer-a-new-workflow-automation-tool-for-developers/,https://techcrunch.com/wp-content/uploads/2018/01/gettyimages-874872698.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Only a few weeks after launching a major overhaul of its Cloud Text-to-Speech API, Google today also announced an update to that service’s Speech-to-Text voice recognition service. The new and improved Cloud Speech-to-Text API promises significantly improved voice recognition performance. The new API promises a reduction in word errors around 54 percent across all of Google’s tests, but in some areas the results are actually far better than that. Part of this improvement is a major new feature in the Speech-to-Text API that now allows developers to select between different machine learning models based on this use case. The new API currently offers four of these models. There is one for short queries and voice commands, for example, as well as one for understanding audio from phone calls and another one for handling audio from videos. The fourth model is the new default, which Google recommends for all other scenarios. In addition to these new speech recognition models, Google is also updating the service with a new punctuation model. As the Google team admits, its transcriptions have long suffered from rather unorthodox punctuation. Punctuating transcribed speech is notoriously hard though (just ask anybody who has ever tried to transcribe a speech by the current U.S. president…). Google promises that its new model results in far more readable transcriptions that feature fewer run-on sentences and more commas, periods and question marks. With this update, Google now also lets developers tag their transcribed audio or video with some basic metadata. There is no immediate benefit to the developer here, but Google says that it will use the aggregate information from all of its users to decide on which new features to prioritize next. Google is making a small change to how it charges for this service. Like before, audio transcripts cost $0.006 per 15 seconds. The video model will cost twice as much, though, at $0.012 per 15 seconds, though until May 31, using this new model will also cost $0.006 per 15 seconds.","Only a few weeks after launching a major overhaul of its Cloud Text-to-Speech API, Google today also announced an update to that service’s Speech-to-Text voice recognition service. The new and improved Cloud Speech-to-Text API promises significantly improved …",2018-04-09T15:00:51Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google launches an improved speech-to-text service for developers,http://techcrunch.com/2018/04/09/google-launches-an-improved-speech-to-text-service-for-developers/,https://techcrunch.com/wp-content/uploads/2018/04/gettyimages-841758452.jpg?w=611,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Say hello to the Bandwidth Alliance, a new group led by Cloudflare that promises to reduce the price of bandwidth for many cloud customers. The overall idea here is that customers who use both Cloudflare, which is turning eight years old this week, and a cloud provider that’s part of this alliance will get a significant discount on their egress traffic or won’t have to pay for it at all. The alliance is open, and others may join still, but right now it includes virtually every major and minor cloud provider you’ve ever heard of — with one exception. Current members include Automattic, Backblaze, Digital Ocean, DreamHost, IBM Cloud, Linode, Google, Google Cloud, Microsoft Azure, Packet, Scaleway and Vapor. Some of these will now offer free egress traffic to mutual customers with Cloudflare, while others will offer at least a 75 percent discount. That’s quite the alliance, but as Cloudflare CEO and co-founder Matthew Prince told me, once the first member joined, the rest of the pieces fell into place quickly. Surely it also helped that both Google and Microsoft have invested in Cloudflare. Why would these businesses choose to do away with what’s a minor but high-margin business, though? “The argument that we made to them was a pretty simple argument: it makes sense for you to charge for transit when you are actually paying for it,” Prince said. Most of the time, though, those costs are very minor and Cloudflare, thanks to his massive number of global peering locations, can ingest the traffic directly from the cloud provider with no middlemen involved. The first company Cloudflare partnered with was Google, thanks to that company’s CDN Interconnect program, which launched in 2015. Cloudflare was one of the initial partners in the program, though as Prince noted, there was still a lot to learn for all parties involved, especially because traffic was sometimes routed in very unpredictable ways that circumvented the cost savings mechanisms. Cloudflare learned from this, though, and is now using its own Argo technology to intelligently route traffic. As Prince noted, though, one thing that turned out to be harder than anticipated was ensuring that the cloud vendors would know that one of their customers is a mutual customer. Some have that instrumentation in place, while Cloudflare needs to pass a special header to them so they can know where their traffic is coming from. Prince also argued that this will make it easier for many companies to use multiple cloud providers without having to pay extremely high bandwidth cost. While Cloudflare’s early focus was very much on web traffic, Prince said that more than half is now API-based traffic, and that’s exactly the kind of user who will likely save quite a bit of money thanks to this. The one company that’s not part of this alliance, of course, is Amazon with its AWS platform. Prince said that Cloudflare has talked to them, though, and the group is open to all cloud and CDN providers.","Say hello to the Bandwidth Alliance, a new group led by Cloudflare that promises to reduce the price of bandwidth for many cloud customers. The overall idea here is that customers who use both Cloudflare, which is turning eight years old this week, and a clou…",2018-09-26T12:00:17Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","Cloudflare partners with Microsoft, Google and others to reduce bandwidth costs",http://techcrunch.com/2018/09/26/cloudflare-partners-with-microsoft-google-and-others-to-reduce-bandwidth-costs/,https://techcrunch.com/wp-content/uploads/2015/09/cloudflare-05.jpg?w=600,techcrunch,TechCrunch,cloud,1
Ron Miller,"When Oracle filed a protest in August with the Government Accountability Office (GAO) that the Pentagon’s $10 billion JEDI RFP process was unfair, it probably had little chance of succeeding. Today, the GAO turned away the protest. The JEDI contract has been set up as a winner-take-all affair. With $10 billion on the table, there has been much teeth-gnashing and complaining that the deck has been stacked to favor one vendor, Amazon. The Pentagon has firmly denied this, but it hasn’t stopped Oracle and IBM from complaining loudly from the get-go that there were problems with the way the RFP was set up. At least with the Oracle complaint, the GAO put that idea firmly to rest today. For starters, the GAO made it clear that the winner-take-all approach was just fine, stating “…the Defense Departments decision to pursue a single-award approach to obtain these cloud services is consistent with applicable statutes (and regulations) because the agency reasonably determined that a single-award approach is in the governments best interests for various reasons, including national security concerns, as the statute allows.” The statement went on to say that the GAO didn’t find that the Pentagon favored any vendor during the RFP period. “GAOs decision also concludes that the Defense Department provided reasonable support for all of the solicitation provisions that Oracle contended exceeded the agencys needs.” Finally, the GAO found no evidence of conflict of interest on the DOD’s part as Oracle had suggested. Oracle has been unhappy since the start of this process, going so far as having co-CEO Safra Catz steer her complaints directly to the president in a meeting last April long before the RFP period had even opened. As I wrote in an article in September, Oracle was not the only vendor to believe that Amazon was the favorite: The belief amongst the various other players, is that Amazon is in the drivers seat for this bid, possibly because they delivered a $600 million cloud contract for the government in 2013, standing up a private cloud for the CIA. It was a big deal back in the day on a couple of levels. First of all, it was the first large-scale example of an intelligence agency using a public cloud provider. And of course the amount of money was pretty impressive for the time, not $10 billion impressive, but a nice contract. Regardless, the RFP submission period ended last month. The Pentagon is expected to choose the vendor in April 2019, Oracle’s protest notwithstanding.","When Oracle filed a protest in August with the Government Accountability Office (GAO) last August that the Pentagon’s $10 billion JEDI RFP process was unfair, it probably had little chance of succeeding. Today the GAO turned away the protest. The JEDI contrac…",2018-11-14T21:58:51Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Government denies Oracle’s protest of $10B Pentagon JEDI cloud RFP,http://techcrunch.com/2018/11/14/government-denies-oracles-protest-of-10b-pentagon-jedi-cloud-rfp/,https://techcrunch.com/wp-content/uploads/2018/09/GettyImages-628711862.jpg?w=764,techcrunch,TechCrunch,cloud,1
Rob LeFebvre,"The TR-808 and TR-909 virtual instruments are full reproductions of the original hardware, according to Roland. The SRX Orchestra is the first one of the SRX series Expansion Library (from the 2000s) available as a software instrument. Roland Cloud will be a suite of high-resolution software synths and sampled instruments that musicians will be able to pull from while creating their own musical works. It sounds similar to what Adobe has done with its own photo and graphics-based Adobe Cloud. All three new additions are headed as updates to the Roland Cloud service starting in February of this year.","The Roland TR-808 and TR-909 are iconic drum machines that powered a ton of the music from the '80s and '90s. While both hardware units were recently revived as the TR-08 and TR-09, they haven't been officially emulated in software yet. That changes now as Ro…",2018-01-26T21:47:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Roland announces software versions of its 808 and 909 drum machines,https://www.engadget.com/2018/01/26/roland-announces-software-versions-808-909-drum-machines/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fs.aolcdn.com%2Fhss%2Fstorage%2Fmidas%2Fb3d339521da649c19034feb8b22218c2%2F206068697%2Fdims-3.jpeg&client=cbc79c14efcebee57402&signature=83a05d3a9c4c2b51e93f7d133fd0ea4f7a691cad,engadget,Engadget,cloud,1
Frederic Lardinois,"Like virtually every big enterprise company, a few years ago, the German auto giant Daimler decided to invest in its own on-premises data centers. And while those aren’t going away anytime soon, the company today announced that it has successfully moved its on-premises big data platform to Microsoft’s Azure cloud. This new platform, which the company calls eXtollo, is Daimler’s first major service to run outside of its own data centers, though it’ll probably not be the last.
As Daimler’s head of its corporate center of excellence for advanced analytics and big data Guido Vetter told me, that the company started getting interested in big data about five years ago. “We invested in technology — the classical way, on-premise — and got a couple of people on it. And we were investigating what we could do with data because data is transforming our whole business as well,” he said.
By 2016, the size of the organization had grown to the point where a more formal structure was needed to enable the company to handle its data at a global scale. At the time, the buzzword was ‘data lakes’ and the company started building its own in order to build out its analytics capacities.
Electric Line-Up, Daimler AG
“Sooner or later, we hit the limits as it’s not our core business to run these big environments,” Vetter said. “Flexibility and scalability are what you need for AI and advanced analytics and our whole operations are not set up for that. Our backend operations are set up for keeping a plant running and keeping everything safe and secure.” But in this new world of enterprise IT, companies need to be able to be flexible and experiment — and, if necessary, throw out failed experiments quickly.
So about a year and a half ago, Vetter’s team started the eXtollo project to bring all the company’s activities around advanced analytics, big data and artificial intelligence into the Azure Cloud and just over two weeks ago, the team shut down its last on-premises servers after slowly turning on its solutions in Microsoft’s data centers in Europe, the U.S. and Asia. All in all, the actual transition between the on-premises data centers and the Azure cloud took about nine months. That may not seem fast, but for an enterprise project like this, that’s about as fast as it gets (and for a while, it fed all new data into both its on-premises data lake and Azure).
If you work for a startup, then all of this probably doesn’t seem like a big deal, but for a more traditional enterprise like Daimler, even just giving up control over the physical hardware where your data resides was a major culture change and something that took quite a bit of convincing. In the end, the solution came down to encryption.
“We needed the means to secure the data in the Microsoft data center with our own means that ensure that only we have access to the raw data and work with the data,” explained Vetter. In the end, the company decided to use thethAzure Key Vault to manage and rotate its encryption keys. Indeed, Vetter noted that knowing that the company had full control over its own data was what allowed this project to move forward.
Vetter tells me that the company obviously looked at Microsoft’s competitors as well, but he noted that his team didn’t find a compelling offer from other vendors in terms of functionality and the security features that it needed.
Today, Daimler’s big data unit uses tools like HD Insights and Azure Databricks, which covers more than 90 percents of the company’s current use cases. In the future, Vetter also wants to make it easier for less experienced users to use self-service tools to launch AI and analytics services.
While cost is often a factor that counts against the cloud since renting server capacity isn’t cheap, Vetter argues that this move will actually save the company money and that storage cost, especially, are going to be cheaper in the cloud than in its on-premises data center (and chances are that Daimler, given its size and prestige as a customer, isn’t exactly paying the same rack rate that others are paying for the Azure services).
As with so many big data AI projects, predictions are the focus of much of what Daimler is doing. That may mean looking at a car’s data and error code and helping the technician diagnose an issue or doing predictive maintenance on a commercial vehicle. Interestingly, the company isn’t currently bringing any of its own IoT data from its plants to the cloud. That’s all managed in the company’s on-premises data centers because it wants to avoid the risk of having to shut down a plant because its tools lost the connection to a data center, for example.","Like virtually every big enterprise company, a few years ago, the German auto giant Daimler decided to invest in its own on-premises data centers. And while those aren’t going away anytime soon, the company today announced that it has successfully moved its o…",2019-02-20T11:00:50Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Why Daimler moved its big data platform to the cloud,http://techcrunch.com/2019/02/20/why-daimler-moved-its-big-data-platform-to-the-cloud/,https://techcrunch.com/wp-content/uploads/2019/02/11C1220_019.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Earlier this week, at MWC Barcelona, BMW announced its newest in-car AI initiative: BMW Natural Interaction. The idea here is to use cameras, microphones and other sensors in the car to allow you to have more natural interactions with the car, either through voice or gestures. The marquee feature here is the ability to point at something outside the car and get more information about it or, if it’s a restaurant, have the BMW Intelligent Personal Assistant (IPA) make a reservation for you. These systems will work by combining in-car AI with cloud technologies — and for those, BMW continues to bet on Microsoft’s Azure cloud.
After the announcement, I sat down with Christoph Grote, BMW Group’s senior VP for electronics. I admit that a lot of what I saw in the demo felt a bit futuristic, but Grote noted that everything he showed off during his presentation is more or less production-ready. “I don’t think I would’ve dared to stand up there if any of the things I showed today were a utopia,” he told me. “All of this is in series production and some of it is already available as part of the BMW OS 7 release. But the major work we are doing, looking ahead to the iNext [electric SUV], is about gaze, head pose and gesture tracking and combing those with the other modalities. But everything we showed today is going to go into production.”
In practice, this means that BMW will use two cameras: a wide-angle camera behind the rear-view mirror that can track the gestures of both the driver and front-seat passenger and one behind the dashboard that only looks at the driver through the steering wheel and recognizes when their eyes blink, where their eyes look and their head pose.
As Grote noted, figuring out where you are looking is not exactly easy. The camera sees your hands in relation to the car. That’s pretty straightforward. But the car, too, is situated somewhere in space, and for this to work, that localization has to be very precise, and the digital map has to be very detailed, too. “GPS isn’t enough for this,” Grote said, and noted that the company plans to use the car’s forward-facing camera to gather additional information that helps localize the car in space based on comparing the image to the digital map. The AI smarts that power these mapping features run right in the car — and in many ways, these features also lay the groundwork for self-driving cars, which obviously need highly detailed maps, too.
In many ways, this work is a continuation of BMW’s work on its IPA in-car assistant. “There, we use Azure Cognitive Service and we plan to integrate these new modalities (like gaze and gesture tracking) with the same technology. And that’s important for these multi-modal systems. […] We have a great partnership with Microsoft and we expect that’ll continue.”
Grote also noted that BMW has a long history of working in the cloud, thanks to many years of experience in offering its connected car services. “We don’t think of the car as an isolated client that connects to some service in the cloud, but that we also see these connected cars as a swarm that has collective intelligence.”
Vehicle-to-everything (V2X) connectivity is one of the hot topics in the car industry right now — especially given the advent of 5G with its low-latency connectivity — and BMW does have its own point of view here. For Grote, V2X systems that use the cellular network and connect to the cloud have major advantages over those that try to connect cars directly. These cloud-connected systems, he argues, are easier to maintain and they are able to translate between different standards or — in the long run — integrate different generations of this system to ensure that cars from different manufacturers can talk to each other.
“A cellular-based system is forward-looking, maintainable, secure and the better foundation that guarantees future development efforts versus a standard that’s 20 years old, from a time when the carriers were not interested in machine-to-machine traffic at all.”
BMW continues to bet on the cloud for many of its newest tech developments. Among car manufacturers, it’s obviously not alone here. Daimler recently announced that it has moved its big data platform to the cloud, for example. And in many ways, that move makes sense. Running online services isn’t a core competency for many of these companies, and even if they are experienced at running their own data centers by now, this isn’t what allows them to differentiate their cars in a highly competitive market. That energy is better spent on building applications, not managing them. The large cloud providers also offer global coverage, and redundancies are hard and expensive to build.","Earlier this week, at MWC Barcelona, BMW announced its newest in-car AI initiative: BMW Natural Interaction. The idea here is to use cameras, microphones and other sensors in the car to allow you to have more natural interactions with the car, either through …",2019-02-26T19:28:26Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",BMW continues to bet on the (Azure) cloud,http://techcrunch.com/2019/02/26/bmw-continues-to-bet-on-the-azure-cloud/,https://techcrunch.com/wp-content/uploads/2019/02/P90337455-highRes.jpg?w=764,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"In the coming weeks, AWS is launching new G4 instances with support for Nvidia’s T4 Tensor Core GPUs, the company today announced at Nvidia’s GTC conference. The T4, which is based on Nvidia’s Turing architecture, was specifically optimized for running AI models. The T4 will be supported by the EC2 compute service and the Amazon Elastic Container Service for Kubernetes.
NVIDIA and AWS have worked together for a long time to help customers run compute-intensive AI workloads in the cloud and create incredible new AI solutions, said Matt Garman, vice president of Compute Services at AWS, in today’s announcement. With our new T4-based G4 instances, were making it even easier and more cost-effective for customers to accelerate their machine learning inference and graphics-intensive applications.
The T4 is also the first GPU on AWS that supports Nvidia’s raytracing technology. That’s not what Nvidia is focusing on with this announcement, but creative pros can use these GPUs to take the company’s real-time raytracing technology for a spin.
For the most part, though, it seems like Nvidia and AWS expect that developers will use the T4 to put AI models into production. It’s worth noting that the T4 hasn’t been optimized for training these models, but they can obviously be used for that as well. Indeed, with the new Cuda-X AI libraries (also announced today), Nvidia now offers an end-to-end platform for developers who want to use its GPUs fr deep learning, machine learning and data analytics.
It’s worth noting that Google launched T4 support in beta a few months ago. On Google’s cloud, these GPUs are currently in beta.","In the coming weeks, AWS is launching new G4 instances with support for Nvidia’s T4 Tensor Core GPUs, the company today announced at Nvidia’s GTC conference. The T4, which is based on Nvidia’s Turing architecture, was specifically optimized for running AI mod…",2019-03-18T23:00:17Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Nvidia’s T4 GPUs are coming to the AWS cloud,http://techcrunch.com/2019/03/18/nvidias-t4-gpus-are-coming-to-the-aws-cloud/,https://techcrunch.com/wp-content/uploads/2019/03/2019-01-16_0827.png?w=574,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google today announced that it has partnered with a number of top open-source data management and analytics companies to integrate their products into its Google Cloud Platform and offer them as managed services operated by its partners. The partners here are Confluent, DataStax, Elastic, InfluxData, MongoDB, Neo4j and Redis Labs.
The idea here, Google says, is to provide users with a seamless user experience and the ability to easily leverage these open-source technologies in Google’s cloud. But there is a lot more at play here, even though Google never quite says so. That’s because Google’s move here is clearly meant to contrast its approach to open-source ecosystems with Amazon’s. It’s no secret that Amazon’s AWS cloud computing platform has a reputation for taking some of the best open-source projects and then forking those and packaging them up under its own brand, often without giving back to the original project. There are some signs that this is changing, but a number of companies have recently taken action and changed their open-source licenses to explicitly prevent this from happening.
That’s where things get interesting, because those companies include Confluent, Elastic, MongoDB, Neo4j and Redis Labs — and those are all partnering with Google on this new project, though it’s worth noting that InfluxData is not taking this new licensing approach and that while DataStax uses lots of open-source technologies, its focus is very much on its enterprise edition.
“As you are aware, there has been a lot of debate in the industry about the best way of delivering these open-source technologies as services in the cloud,” Manvinder Singh, the head of infrastructure partnerships at Google Cloud, said in a press briefing. “Given Google’s DNA and the belief that we have in the open-source model, which is demonstrated by projects like Kubernetes, TensorFlow, Go and so forth, we believe the right way to solve this it to work closely together with companies that have invested their resources in developing these open-source technologies.”
So while AWS takes these projects and then makes them its own, Google has decided to partner with these companies. While Google and its partners declined to comment on the financial arrangements behind these deals, chances are we’re talking about some degree of profit-sharing here.
“Each of the major cloud players is trying to differentiate what it brings to the table for customers, and while we have a strong partnership with Microsoft and Amazon, its nice to see that Google has chosen to deepen its partnership with Atlas instead of launching an imitation service,” Sahir Azam, the senior VP of Cloud Products at MongoDB told me. “MongoDB and GCP have been working closely together for years, dating back to the development of Atlas on GCP in early 2017. Over the past two years running Atlas on GCP, our joint teams have developed a strong working relationship and support model for supporting our customers mission critical applications.”
As for the actual functionality, the core principle here is that Google will deeply integrate these services into its Cloud Console; for example, similar to what Microsoft did with Databricks on Azure. These will be managed services and Google Cloud will handle the invoicing and the billings will count toward a user’s Google Cloud spending commitments. Support will also run through Google, so users can use a single service to manage and log tickets across all of these services.
Redis Labs CEO and co-founder Ofer Bengal echoed this. “Through this partnership, Redis Labs and Google Cloud are bringing these innovations to enterprise customers, while giving them the choice of where to run their workloads in the cloud, he said. “Customers now have the flexibility to develop applications with Redis Enterprise using the fully integrated managed services on GCP. This will include the ability to manage Redis Enterprise from the GCP console, provisioning, billing, support, and other deep integrations with GCP.",Google today announced that it has partnered with a number of top open-source data management and analytics companies to integrate their products into its Google Cloud Platform and offer them as managed services operated by its partners. The partners here are…,2019-04-09T16:00:49Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud challenges AWS with new open-source integrations,http://techcrunch.com/2019/04/09/google-gives-aws-the-open-source-middle-finger/,https://techcrunch.com/wp-content/uploads/2017/03/img_20170307_190815.jpg?w=533,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google is hosting its annual Cloud Next developer conference in San Francisco this week. While the event is still in full swing, with a mystery concert capping off most of the programming tonight, the flood of news has now subsided, so here is our list of the most important announcements from the event.
Anthos
What is it? Anthos is the new name of the Google Cloud Services Platform, Google’s managed service for allowing enterprises to run applications in their private data center and in Google’s cloud. Google decided to give the service a new name, Anthos, but also expanded support to AWS and Azure, its competitors’ cloud platforms. This will allow enterprises to use a single platform, running on Google’s cloud, to deploy and manage their applications on any cloud. Enterprises will get a single bill and have a single dashboard to manage their applications. All of this, unsurprisingly, is powered by containers and Kubernetes.
Why does it matter? It’s still highly unusual for the big cloud competitors to launch a product that allows users to run applications on other platforms. The money, after all, is in charging fees for compute time and storage allocations. Google argues that this is something its customers want and that it solves a real problem. Google, however, is also chasing its competitors and looking for ways to differentiate its approach from them. I don’t think we’ll see AWS and Azure react with similar tools, but if they do, it’s a good thing for their customers.
Open-source integrations into the Google Cloud Console
What is it? Google announced that it would deeply integrate the products of a number of open-source companies into its cloud and essentially make them first-party services. These partners are Confluent, DataStax, Elastic, InfluxData, MongoDB, Neo4j and Redis Labs, with others likely to follow over time.
Why does it matter? These integrations are a boon for Google Cloud customers who are likely already using some of these services. They’ll get a single bill and access to support from these companies, all while managing the services from a single console. The subtext here, though, is a bit more complicated and reveals Google’s approach to open source and puts it into contrast with AWS. Many of the companies that are participating here are highly critical of AWS’s treatment of open source and quite public about it. Google is working with them while the perception is that AWS simply uses the code and doesn’t give back.
Google’s AI Platform
What is it? Google sees its AI prowess as one of its main differentiators in its fight against AWS, Azure and Co. The company already offered a wide range of AI tools, ranging from developer tools and services for advanced data scientists to AutoML, a service that can automatically train models and doesn’t require a PhD. The new AI Platform offers an end-to-end solution for more advanced developers that allows them to go from ingesting data to training and testing their models, to putting them into production. The platform can also use pre-built models.
Why does it matter? AI (and machine learning) is the major focus for all big cloud providers, but the developer experience leaves lots of room for improvement. Having an end-to-end solution is obviously a major step forward here and opens up the promise of machine learning to a wider range of potential users.
Your Android phone is now a security key
What is it? Instead of using a physical security key to enable two-factor authentication, you’ll now be able to use any Android 7+ phone as a security key, too. You set it up in your Google Account and your phone will then use Bluetooth (but without the hassle of creating a Bluetooth connection) to provide your second factor. For now, this only works with Chrome, but Google hopes to turn this into a standard that other browsers and mobile operating system vendors will also support. Google also recommends you still use a regular key as a backup for that inevitable day when you lose your phone.
Why it matters? Two-factor authentication is inherently safer than just using a login and password. Systems that use SMS and push-notifications are still vulnerable to phishing attacks while security keys — and this new Android-based system uses the same standards as existing keys — prevent this by ensuring that you are on a legitimate site. This new system takes the hassle out of using a physical key and may just convince more people to use two-factor authentication.
Google Cloud Code
What is it? Cloud Code is a set of plugins and extensions for popular IDE’s like IntelliJ and VS Code. The general idea here is to provide developers with all of the necessary tools to build cloud-native applications — all without having to deal with any of the plumbing work and configuration that comes with that. Using Cloud Code, developers can simply write their applications like before, but then package them as cloud-native apps and ship them to a Kubernetes cluster for testing or production.
Why does it matter? Writing cloud-native apps is complicated and usually involves writing complex configuration files. Cloud Code ideally makes all of this so easy that it’ll be far easier for developers — and the companies that employ them — to make this move to a modern infrastructure.
Google Cloud aims at retailers
What is it? The news here is that Google is launching a vertical solution that’s squarely aimed at retailers. That doesn’t sound all that earth-shattering, does it? But Google Cloud plans to offer more of these specialized solutions over time.
Why does it matter? Google Cloud CEO Thomas Kurian told us that customers are asking for these kinds of integrated solutions that package some of the companies existing tools into integrated solutions that these enterprises can deploy. This is essentially the first time it is doing so (with maybe the exception of healthcare), but it’ll likely offer more of these over time and they could become a major factor in growing the platform’s user base.
Bonus
We also got a chance to sit down with Google Cloud’s new CEO Thomas Kurian to put some of the announcements into context and talk about his vision for Google Cloud going forward.","Google is hosting its annual Cloud Next developer conference in San Francisco this week. While the event is still in full swing, with a mystery concert capping off most of the programming tonight, the flood of news has now subsided, so here is our list of the…",2019-04-10T19:21:03Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",The 6 most important announcements from Google Cloud Next 2019,http://techcrunch.com/2019/04/10/the-6-most-important-announcements-from-google-cloud-next-2019/,https://techcrunch.com/wp-content/uploads/2019/04/IMG_20190409_162653-1.jpg?w=534,techcrunch,TechCrunch,cloud,1
Dani Deahl,"Amazon has announced new functionalities for its home security camera Cloud Cam, as well as updates to both the Echo Show and Echo Spot, as reported by Android Police. You can now check Cloud Cam’s live view from a computer, where before it was only available via the app. Also, you can use the command, “Alexa, turn on [camera name]” to turn on your Cloud Cam. These are both small but incredibly useful additions. Many other security cameras already offer the ability to view live footage online, and it seems odd that Cloud Cam launched with Alexa commands to let you switch between various rooms, but not one to simply turn it on. Four months after the device’s release, now you can. The changelog from Amazon also notes a couple updates to the Echo Show and Echo Spot. They now support two-way audio like a walkie-talkie, and users can opt-in to get motion / person detection alerts on any Echo device. These alerts will appear on-screen the Echo Show and Echo Spot, and as a green ring of light on the Echo and Echo Dot. The frequency of these notifications can be adjusted within the Cloud Cam App and played by asking, “Alexa, what’re my notifications?” or “Alexa, play my notifications.”","Amazon has announced new functionalities for its home security camera Cloud Cam, as well as updates to both the Echo Show and Echo Spot.",2018-03-13T15:34:43Z,"{'id': 'the-verge', 'name': 'The Verge'}",Amazon’s Cloud Cam now has a web interface and more Alexa commands,https://www.theverge.com/circuitbreaker/2018/3/13/17114222/amazon-cloud-cam-web-interface-alexa-commands,https://cdn.vox-cdn.com/thumbor/N1bcHPLWrUgEIxEBBvRNRfnXvVA=/0x129:1800x1071/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/9533691/bfarsace_171024_amazon_0001_2.jpg,the-verge,The Verge,cloud,1
Frederic Lardinois,"It’s no secret that Google has long planned to open a cloud region in Los Angeles. The company has long said that a second region on the U.S. West Coast was in the works, after all. What we didn’t know for sure was when this new region would go live. But as Google announced today, the new Los Angeles cloud region will go live in July, making it the company’s fifth region in the U.S. During an event in Los Angeles today, Google specifically positioned this region as the ideal region for the media and entertainment industry in the area. “ The LA region will provide our media and entertainment customers with fast, scalable compute resources, so visual effects and animation studios can spend less time waiting on renders, and more time bringing their creative visions to life,” Google notes in its announcement today. But beyond this focus on entertainment, it’s worth noting that this new region now also gives every company that best on the Google Cloud platform a second West Coast option besides its Oregon region. That’s not what Google is focusing on today, though. The company’s event and blog posts all focus on the various rendering tools like Zync Render, which Google acquired back in 2014, and Anvato, a video streaming and monetization platform it acquired in 2016. It’s worth noting that Microsoft Azure already offers a region in Southern California, though AWS does not.","It’s no secret that Google has long planned to open a cloud region in Los Angeles. The company has long said that a second region on the U.S. West Coast was in the works, after all. What we didn’t know for sure was when this new region would go live. But as G…",2018-06-26T16:00:10Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud will launch its Los Angeles region in July,http://techcrunch.com/2018/06/26/google-cloud-will-launch-its-los-angeles-region-in-july/,https://techcrunch.com/wp-content/uploads/2018/06/infrastructure_header.max-1000x1000.png?w=721,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"At CES, the Chinese tech giant Baidu today announced OpenEdge, its open source edge computing platform. At its core, OpenEdge is the local package component of Baidu’s existing Intelligent Edge (BIE) commercial offering and obviously plays well with that service’s components for managing edge nodes and apps.
Since this is obviously a developer announcement, I’m not sure why Baidu decided to use CES as the venue for this release, but there can be no doubt that China’s major tech firms have become quite comfortable with open source. Companies like Baidu, Alibaba, Tencent and others are often members of the Linux Foundation and its growing stable of projects, for example, and virtually ever major open source organization now looks to China as its growth market. It’s no surprise then that we’re also now seeing a wider range of Chinese companies that open source their own projects.
“Edge computing is a critical component of Baidus ABC (AI, Big Data and Cloud Computing) strategy,” says Baidu VP and GM of Baidu Cloud Watson Yin. “By moving the compute closer to the source of the data, it greatly reduces the latency, lowers the bandwidth usage and ultimately brings real-time and immersive experiences to end users. And by providing an open source platform, we have also greatly simplified the process for developers to create their own edge computing applications.”
A company spokesperson tells us that the open source platform will include features like data collection, message distribution and AI inference, as well as tools for syncing with the cloud.
Baidu also today announced that it has partnered with Intel to launch the BIE-AI-Box and with NXP Semiconductors to launch the BIE-AI-Board. The box is designed for in-vehicle video analysis while the board is small enough for cameras, drones, robots and similar applications.","At CES, the Chinese tech giant Baidu today announced OpenEdge, its open source edge computing platform. At its core, OpenEdge is the local package component of Baidu’s existing Intelligent Edge (BIE) commercial offering and obviously plays well with that serv…",2019-01-09T18:00:01Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Baidu Cloud launches its open source edge computing platform,http://techcrunch.com/2019/01/09/baidu-cloud-launches-its-open-source-edge-computing-platform/,https://techcrunch.com/wp-content/uploads/2017/09/gettyimages-458948860.jpg?w=609,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google today announced that Cloud Firestore, its serverless NoSQL document database for mobile, web and IoT apps, is now generally available. In addition, Google is also introducing a few new features and bringing the service to ten new regions.
With this launch, Google is giving developers the option to run their databases in a single region. During the beta, developers had to use multi-region instances and while that obviously has some advantages with regard to resilience, it’s also more expensive and not every app needs to run in multiple regions.
“Some people don’t need the added reliability and durability of a multi-region application,” Google product manager Dan McGrath told me. “So for them, having a more cost-effective regional instance is very attractive, as well as data locality and being able to place a Cloud Firestore database as close as possible to their user base.”
The new regional instance pricing is up to 50 percent cheaper than the current multi-cloud instance prices. Which solution you pick does influence the SLA guarantee Google gives you, though. While the regional instances are still replicated within multiple zones inside the region, all of the data is still within a limited geographic area. Hence, Google promises 99.999 percent availability for multi-region instances and 99.99 percent availability for regional instances.
And talking about regions, Cloud Firestore is now available in ten new regions around the world. Firestore launched with a single location when it launched and added two more during the beta. With this, Firestore is now available in 13 locations (including the North America and Europe multi-region offerings). McGrath tells me Google is still in the planning phase for deciding the next phase of locations but he stressed that the current set provides pretty good coverage across the globe.
Also new in this release is deeper integration with Stackdriver, the Google Cloud monitoring service, which can now monitor read, write and delete operations in near-real time. McGrath also noted that Google plans to add the ability to query documents across collections and to increment database values without needing a transaction soon.
It’s worth noting that while Cloud Firestore falls under the Googe Firebase brand, which typically focuses on mobile developers, Firestore offers all of the usual client-side libraries for Compute Engine or Kubernetes Engine applications, too.
“If you’re looking for a more traditional NoSQL document database, then Cloud Firestore gives you a great solution that has all the benefits of not needing to manage the database at all,” McGrath said. “And then, through the Firebase SDK, you can use it as a more comprehensive back-end as a service that takes care of things like authentication for you.”
One of the advantages of Firestore is that it has extensive offline support, which makes it ideal for mobile developers but also IoT solutions. Maybe it’s no surprise then that Google is positioning it as a tool for both Google Cloud and Firebase users.","Google today announced that Cloud Firestore, its serverless NoSQL document database for mobile, web and IoT apps, is now generally available. In addition, Google is also introducing a few new features and bringing the service to ten new regions. With this lau…",2019-01-31T17:00:55Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google’s Cloud Firestore NoSQL database hits general availability,http://techcrunch.com/2019/01/31/googles-cloud-firestore-nosql-database-hits-general-availability/,https://techcrunch.com/wp-content/uploads/2019/01/IMG_20181211_160812-1.jpg?w=533,techcrunch,TechCrunch,cloud,1
Anthony Ha,"The Daily Crunch is TechCrunch’s roundup of our biggest and most important stories. If you’d like to get this delivered to your inbox every day at around 9am Pacific, you can subscribe here.
1. Google Clouds new CEO on gaining customers, startups, supporting open source and more
Thomas Kurian, who came to Google Cloud after 22 years at Oracle, said the team is rolling out new contracts and plans to simplify pricing.
Most importantly, though, Google will go on a hiring spree: A number of customers told us we just need more people from you to help us. So thats what well do.”
2. Walmart to expand in-store tech, including Pickup Towers for online orders and robots
Walmart is doubling down on technology in its brick-and-mortar stores in an effort to better compete with Amazon. The retailer says it will add to its U.S. stores 1,500 new autonomous floor cleaners, 300 more shelf scanners, 1,200 more FAST Unloaders and 900 new Pickup Towers.
3. Udacity restructures operations, lays off 20 percent of its workforce
The objective is to do more than simply keep the company afloat, according to co-founder Sebastian Thrun. Instead, Thrun says these measures will allow Udacity to move from a money-losing operation to a break-even or profitable company by next quarter and then moving forward.
Photo By Bill Clark/CQ Roll Call via Getty Images
4. The government is about to permanently bar the IRS from creating a free electronic filing system
Thats right, members of Congress are working to prohibit a branch of the federal government from providing a much-needed service that would make the lives of all of their constituents much easier.
5. Heres the first image of a black hole
Say hello to the black hole deep inside the Messier 87, a galaxy located in the Virgo cluster some 55 million light years away.
6. Movo grabs $22.5M to get more cities in LatAm scooting
The Spanish startup targets cities in its home market and in markets across Latin America, offering last-mile mobility via rentable electric scooters.
7. Uber, Lyft and the challenge of transportation startup profits
An article arguing that everything you know about the cost of transportation is wrong. (Extra Crunch membership required.)","The Daily Crunch is TechCrunch’s roundup of our biggest and most important stories. If you’d like to get this delivered to your inbox every day at around 9am Pacific, you can subscribe here. 1. Google Cloud’s new CEO on gaining customers, startups, supporting…",2019-04-10T17:08:19Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Daily Crunch: Meet the new CEO of Google Cloud,http://techcrunch.com/2019/04/10/daily-crunch-google-cloud/,https://techcrunch.com/wp-content/uploads/2019/04/GettyImages-1135936641.jpg?w=585,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Atlassian’s Jira has become a standard for managing large software projects in many companies. Many of those same companies also use GitHub as their source code repository and, unsurprisingly, there has long been an official way to integrate the two. That old way, however, was often slow, limited in its capabilities and unable to cope with the large code bases that many enterprises now manage on GitHub. Almost as if to prove that GitHub remains committed to an open ecosystem, even after the Microsoft acquisition, the company today announced a new and improved integration between the two products. “Working with Atlassian on the Jira integration was really important for us,” GitHub’s director of ecosystem engineering Kyle Daigle told me ahead of the announcement. “Because we want to make sure that our developer customers are getting the best experience of our open platform that they can have, regardless of what tools they use.” So a couple of months ago, the team decided to build its own Jira integration from the ground up, and it’s committed to maintaining and improving it over time. As Daigle noted, the improvements here include better performance and a better user experience. The new integration now also makes it easier to view all the pull requests, commits and branches from GitHub that are associated with a Jira issue, search for issues based on information from GitHub and see the status of the development work right in Jira, too. And because changes in GitHub trigger an update to Jira, too, that data should remain up to date at all times. The old Jira integration over the so-called Jira DVCS connector will be deprecated and GitHub will start prompting existing users to do the upgrade over the next few weeks. The new integration is now a GitHub app, so that also comes with all of the security features the platform has to offer.","Atlassian’s Jira has become a standard for managing large software projects in many companies. Many of those same companies also use GitHub as their source code repository and, unsurprisingly, there has long been an official way to integrate the two. That old…",2018-10-04T15:00:41Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",GitHub gets a new and improved Jira Software Cloud integration,http://techcrunch.com/2018/10/04/github-gets-a-new-and-improved-jira-software-cloud-integration/,https://techcrunch.com/wp-content/uploads/2018/03/mvimg_20171129_165434.jpg?w=533,techcrunch,TechCrunch,cloud,1
Ron Miller,"For months, the drama has been steady in the Pentagon’s decade long, $10 billion JEDI cloud contract procurement process. This week the plot thickened when the DOD reported that it has found new evidence of a possible conflict of interest, and has reopened its internal investigation into the matter.
“DOD can confirm that new information not previously provided to DOD has emerged related to potential conflicts of interest. As a result of this new information, DOD is continuing to investigate these potential conflicts,” Elissa Smith, Department of Defense spokesperson told TechCrunch.
It’s not clear what this new information is about, but the Wall Street Journal reported this week that senior federal judge Eric Bruggink of the U.S. Court of Federal Claims ordered that the lawsuit filed by Oracle in December would be put on hold to allow the DOD to investigate further.
From the start of the DOD RFP process, there have been complaints that the process itself was designed to favor Amazon, and that were possible conflicts of interest on the part of DOD personnel. The DOD’s position throughout has been that it is an open process and that an investigation found no bearing for the conflict charges. Something forced the department to rethink that position this week.
Oracle in particular has been a vocal critic of the process. Even before the RFP was officially opened, it was claiming that the process unfairly favored Amazon. In the court case, it made the conflict part clearer, claiming that an ex-Amazon employee named Deap Ubhi had influence over the process, a charge that Amazon denied when it joined the case to defend itself. Four weeks ago something changed when a single line in a court filing suggested that Ubhi’s involvement may have been more problematic than the DOD previously believed.
At the time, I wrote:
In the document, filed with the court on Wednesday, the governments legal representatives sought to outline its legal arguments in the case. The line that attracted so much attention stated, Now that Amazon has submitted a proposal, the contracting officer is considering whether Amazons re-hiring Mr. Ubhi creates an OCI that cannot be avoided, mitigated, or neutralized. OCI stands for Organizational Conflict of Interest in DoD lingo.
And Pentagon spokesperson Heather Babb told TechCrunch:
“During his employment with DDS, Mr. Deap Ubhi recused himself from work related to the JEDI contract. DOD has investigated this issue, and we have determined that Mr. Ubhi complied with all necessary laws and regulations, Babb told TechCrunch.
Whether the new evidence that DOD has found is referring to Ubhi’s rehiring by Amazon or not, is not clear at the moment, but it has clearly found new evidence it wants to explore in this case, and that has been enough to put the Oracle lawsuit on hold.
Oracle’s court case is the latest in a series of actions designed to protest the entire JEDI procurement process. The Washington Post reported last spring that co-CEO Safra Catz complained directly to the president. The company later filed a formal complaint with the Government Accountability Office (GAO), which it lost in November when the department’s investigation found no evidence of conflict. It finally made a federal case out of it when it filed suit in federal court in December, accusing the government of an unfair procurement process and a conflict on the part of Ubhi.
The cloud deal itself is what is at the root of this spectacle. It’s a 10-year contract worth up to $10 billion to handle the DOD’s cloud business — and it’s a winner-take-all proposition. There are three out clauses, which means it might never reach that number of years or dollars, but it is lucrative enough, and could possibly provide inroads for other government contracts, that every cloud company wants to win this.
The RFP process closed in October and the final decision on vendor selection is supposed to happen in April. It is unclear whether this latest development will delay that decision.","For months, the drama has been steady in the Pentagon’s decade long, $10 billion JEDI cloud contract procurement process. This week the plot thickened when the DOD reported that it has found new evidence of a possible conflict of interest, and has reopened it…",2019-02-20T16:00:21Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",New conflict evidence surfaces in JEDI cloud contract procurement process,http://techcrunch.com/2019/02/20/new-conflict-evidence-surfaces-in-jedi-cloud-contract-procurement-process/,https://techcrunch.com/wp-content/uploads/2016/09/serverroom.jpg?w=634,techcrunch,TechCrunch,cloud,1
Romain Dillet,"Google isnt launching a gaming console. The company is launching a service instead, Stadia. Youll be able to run a game on a server and stream the video feed to your device. You wont need to buy new hardware to access Stadia, but Stadia wont be available on all devices from day one.
“With Google, your games will be immediately discoverable by 2 billion people on a Chrome browser, Chromebook, Chromecast, Pixel device. And we have plans to support more browsers and platforms over time, Google CEO Sundar Pichai said shortly after opening the conference.
As you can see, the Chrome browser will be the main interface to access the service on a laptop or desktop computer. The company says that youll be able to play with your existing controller. So if you have a PlayStation 4, Xbox One or Nintendo Switch controller, that should work just fine. Google is also launching its own controller.
As expected, if youre using a Chromecast with your TV, youll be able to turn it into a Stadia machine. Only the latest Chromecast supports Bluetooth, so lets see if youll need a recent model to play with your existing controller. Googles controller uses Wi-Fi so that should theoretically work with older Chromecast models.
On mobile, it sounds like Google isnt going to roll out its service to all Android devices from day one. Stadia could be limited to Pixel phones and tablets at first. But theres no reason Google would not ship Stadia to all Android devices later.
Interestingly, Google didnt mention Apple devices at all. So if you have an iPhone or an iPad, dont hold your breath. Apple doesnt let third-party developers sell digital content in their apps without going through the App Store. This will create a challenge for Google.
Stadia isnt available just yet. Itll launch later this year. As you can see, there are many outstanding questions after the conference. Google is entering a new industry and its going to take some time to figure out the business model and the distribution model.","Google isn’t launching a gaming console. The company is launching a service instead, Stadia. You’ll be able to run a game on a server and stream the video feed to your device. You won’t need to buy new hardware to access Stadia, but Stadia won’t be available …",2019-03-19T19:03:45Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Here’s how you’ll access Google’s Stadia cloud gaming service,http://techcrunch.com/2019/03/19/heres-how-youll-access-googles-stadia-cloud-gaming-service/,https://techcrunch.com/wp-content/uploads/2019/03/Google-GDC-Stadia-15.jpg?w=711,techcrunch,TechCrunch,cloud,1
Ron Miller,"SAP announced today at its Sapphire customer conference it was making the SAP Leonardo Blockchain service generally available. The latter is a cloud service to help companies build applications based on digital ledger-style technology. Gil Perez, senior vice president for product and innovation and head of digital customer initiatives at SAP, says most of the customers he talks to are still very early in the proof of concept stage, but not so early that SAP doesn’t want to provide a service to help move them along the maturity curve. “We are announcing the general availability of the SAP Cloud Platform Blockchain Services.” This is a generalized service on top of which customers can begin building their blockchain projects. He says SAP is taking an agnostic approach to the underlying ledger technology whether it’s the open source Hyperledger project, where SAP is a platinum sponsor, MultiChain or any additional blockchain or decentralized distributed ledger technologies. Perez said part of the reason for this flexibility is that blockchain technology is really still being defined and SAP doesn’t want to commit to any underlying ledger approach until the market decides which way to go. He says this should allow them them to minimize the impact on customers as the technology evolves. They join other enterprise companies like Oracle, IBM, Microsoft and Amazon who have previously released blockchains services for their customers. For SAP, which many companies use for the back-office management of everything from finance to logistics, the blockchain could present some interesting use cases for its customers such as supply chain management. In this case, the blockchain could help reduce paperwork, bring products to market more quickly and provide an easy audit trail. Instead of requesting a scanned copy of a signed document, you could simply click on a node on the blockchain and see the approval (or denial) and follow the products through the shipping process to the marketplace. But Perez stresses that just because it’s early doesn’t mean they aren’t working on some pretty substantial projects. He cited one with a pharmaceutical company to ensure the provenance of drugs that involved over a billion transactions already. SAP is simply trying to keep up with what customers want. Prior to the GA announced today, the company conducted a survey of 250 customers and found, that although it was early days, there is enterprise interest in exploring blockchain technology. Whether this initiative can expand into a broader business is hard to say, but SAP sees blockchain as logical adjacent technology to their core offerings.","SAP announced today at its Sapphire customer conference it was making the SAP Leonardo Blockchain service generally available. The latter is a cloud service to help companies build applications based on digital ledger-style technology. Gil Perez, senior vice …",2018-06-06T13:01:52Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",SAP latest enterprise player to offer cloud blockchain service,http://techcrunch.com/2018/06/06/sap-latest-enterprise-player-to-offer-cloud-blockchain-service/,https://techcrunch.com/wp-content/uploads/2018/06/blockchain-illo1-background_blockchain-top2.png?w=711,techcrunch,TechCrunch,cloud,1
Ron Miller,"Chances are you see a story about cloud storage, and you yawn and move on, but Wasabi, a startup from the folks who brought you Carbonite backup, might make you pause. That’s because they claim to have found a cheaper, faster way to store data, and apparently investors like what they are seeing, forking over $68 million for a Series B investment. Yes, that’s a hefty amount for an early round, but with founders who have multiple successful exits, investors might have seen a lower risk than you might think. The company didn’t go with your usual Sand Hill Road suspects here, instead opting for an unconventional set of industry veterans and family offices along with Forestay Capital, Swiss entrepreneur, Ernesto Bertarelli’s technology fund. Much like Packet, a startup that scored $25 million the other day, they are hoping to take on cloud giants by finding a seam in the market they can exploit. While Packet was looking at customized compute, Wasabi is concentrating squarely on storage, an area they understand well from their Carbonite days. CEO David Friend reports they are offering a terabyte of storage for just $5 a month, and says they are growing 30-40 percent month over month, since they launched in May 2017. In fact, he says they already have 3500 customers. They took their time building their own custom storage solution, which he claims is faster and more efficient than any out there, allowing them to undercut Amazon S3 storage prices. Amazon is charging.023 cents per gigabyte for up to 50 terabytes. That works out to $23 a terabyte, substantially more than Wasabi’s asking price. It begs the question though, how they can afford to keep scaling such a solution. For starters, they use co-location facilities like Digital Realty and Equinix for their storage solution instead of building out their own data centers. Friend says as they scale, they won’t be using their investment capital to add more capacity. Instead, they will be borrowing from banks in an apartment building kind of model, where you build the building, rent out the apartments and break even after a certain amount of time. He says, Wasabi can continue to grow this way. They are going after fat data targets like media and entertainment and genomics, where they believe companies looking for the best price possible will bypass the big three — Amazon, Google and Microsoft — to build a more cost-effective storage solution. The road is littered with failed cloud storage plays, but these folks have an experienced team and plenty of money behind them. Time will tell if they can buck the odds and take on the world’s biggest cloud companies by competing on price and performance, or if they can continue to keep prices this low as they grow and must add increasing capacity without the benefit of being webscale.","Chances are you see a story about cloud storage, and you yawn and move on, but Wasabi, a startup from the folks who brought you Carbonite backup, might make you pause. That’s because they claim to have found a cheaper, faster way to store data, and apparently…",2018-09-12T14:32:25Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Wasabi just landed $68 million to upend cloud storage,http://techcrunch.com/2018/09/12/wasabi-just-landed-68-million-to-upend-cloud-storage/,https://techcrunch.com/wp-content/uploads/2018/09/GettyImages-690478922.jpg?w=591,techcrunch,TechCrunch,cloud,1
Carlos Cadorniga,"Just to let you know, if you buy something featured here, Mashable might earn an affiliate commission. The cloud is one of the most frequently misunderstood technologies. We rely on it for everything from email to analytics to puppy GIFs, but if someone ever really, truly demanded an explanation, most of us could only give a cursory explanation. SEE ALSO: Learn how to create your own online courses for sites like Udemy But now that businesses are embracing the cloud, there’s a growing demand for cloud experts — those who not only know what it is but can manage the infrastructure. Due to this high demand, cloud computing is a very promising and lucrative career path, and you can learn all about it by taking an online course like The Essential Cloud Computing Lifetime Bundle. (Psst. This bundle of online courses is on sale. Keep reading.) Even if you are a complete noob when it comes to the cloud, this four-course bundle will teach you how to implement, maintain, and secure cloud technology. You’ll learn about virtualization and how to mitigate risk, and master tools used by all kinds of IT professionals in the small business and enterprise spaces. Most importantly, you’ll learn how to pass the sought-after certification exams administered by Cloud Security Alliance CCSK and CompTIA Cloud+. So whether you’re looking to switch careers or just brush up on your cloud skills, this bundle will help you level up your career — and the certifications may just land you a promotion. The Essential Cloud Computing Lifetime Bundle normally costs $499, but you can get it for just $19, a massive savings of 96%.","The cloud is one of the most frequently misunderstood technologies. We rely on it for everything from email to analytics to puppy GIFs, but if someone ever really, truly demanded an explanation, most of us could only give a cursory explanation. SEE ALSO: Lear…",2018-04-11T10:05:00Z,"{'id': 'mashable', 'name': 'Mashable'}",Take online classes in cloud computing for just $19 and impress your boss,https://mashable.com/2018/04/11/learn-cloud-computing-online-courses-sale/,https://i.amz.mshcdn.com/20LvaTflIVkm4y30388iHTzu3LI=/1200x630/2018%2F04%2F11%2F0e%2F431180cf223b4889a81abaf8a3db429f.20b5a.jpg,mashable,Mashable,cloud,1
Frederic Lardinois,"Google Cloud today announced the launch of preemptible GPUs. Like Google’s preemptible VMs (and AWS’s comparable spot instances), these GPUs come at a significant discount — in this case, 50 percent. But in return, Google may shut them down at any point if it needs these resources. All you get is a thirty-second warning. You can also only use any given preemptible GPU for up to 24 hours. This new feature applies to both the NVIDIA K80 and NVIDIA P100 GPUs that are currently available on the Google Cloud platform. Google will charge $0.22 per GPU hour for the K80 and $0.73 for the P100. In addition, you will also need to pay for a VM. Google will only charge you preemptible VM prices for these, but you don’t have to worry about Google shutting down your VM before your GPU workload is done (or before you get your thirty-second warning for the GPU shutdown). Google notes that these preemptible GPUs should be a good fit for any fault-tolerant machine learning workloads and other computation-heavy workloads that run in batches. Using Google’s managed instance group feature, you can also automatically re-create your preemptible instance after it goes down. Google’s move today comes only a few months after the company already significantly dropped its prices for non-preemptible GPUs. In addition to this new pricing scheme, Google also today noted that GPUs are now available in its cloud’s US-central1 region. Featured Image: Google /Google","Google Cloud today announced the launch of preemptible GPUs. Like Google’s preemptible VMs (and AWS’s comparable spot instances), these GPUs come at a significant discount — in this case, 50 percent. But in return, Google may shut them down at any point if it…",2018-01-04T17:36:11Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud launches preemptible GPUs with a 50% discount,http://techcrunch.com/2018/01/04/google-cloud-launches-preemptible-gpus-with-a-50-discount/,https://tctechcrunch2011.files.wordpress.com/2015/12/cbf_009.jpg,techcrunch,TechCrunch,cloud,1
Darrell Etherington,"Ford and Silicon Valley-based Autonomic will work together to build a new open platform upon which cities can build out infrastructure communications, including connected traffic lights and parking spots, called the “Transportation Mobility Cloud.” Ford CEO Jim Hackett announced the news on Monday at the CES 2018 keynote kicking off the annual conference. The platform is designed to help connect smart transportation services, as well as adjacent connected offerings, uniting them with one common language to help coordinate all this efforts in real-time. That means tying together personal cars with vehicle-to-everything communications built in, incorporating things like bike sharing networks, public and private transportation services, including buses, trains, ride hailing and beyond. The Transportation Mobility Cloud will support location-based services, determining routes, sending out alerts about things like service disruptions, handing identity management and payment processing, as well as dealing with data gather and analytics. It’s intended not only as a kind of connective tissue for the forward-thinking services and vehicles that will make up the smart city of tomorrow, but also as a platform upon which new apps and services can be built from the heath of data available. Ford says to think of it like “a box of Legos” with pieces that can be quickly taken apart and reassembled to build new types of assets and products to better serve city residents. It’s intended to be flexible enough to work with all partners, and to change from city-to-city depending on local requirements and implementation specifications. In a blog post detailing the news, Ford suggests some possible uses to illustrate what the platform could do, including routing autonomous vehicles away from the most densely clogged arteries occupied by human cars in times of peak traffic, and rerouting cars on the fly to help reduce congestion, or even letting cities fence off ares of the city to restrict them to EV only zones in order to help mitigate air quality and emissions issues. Ford stresses that it has designed this platform “for everyone,” a road base group that includes transit service operators, as well as competitor automakers, who it invites to join in with the effort in order to help make it as widely compatible as possible. Ford says it hopes to use its open approach to drive adoption to the point where it can claim to be the smart city platform with the most connected vehicles by the end of 2019, and eventually it hopes to achieve a 100 percent compatibility rate with vehicles and services on the road. It’s a massive undertaking, but if successful, it could pave the way to cities better able to launch and incorporate Ford’s growing stable of mobility service offerings, including things like last mile shared commute service Chariot, as wells Ford GoBike and its forthcoming autonomous ride hailing fleets. Teaming with Autonomic, a company that Ford invested in last year, will help it ramp quickly since the Palo Alto company’s staff has lots of experience building platforms intended for integration on a broad scale, including Amazon Web Services. Part of the promise of ride-hailing has been that it would reduce congestion in cities – but studies show the opposite is true, which Ford says it hopes to help correct with a platform like that that can help optimize their rollout and integration into existing services and traffic flows.","Ford and Silicon Valley-based Autonomic will work together to build a new open platform upon which cities can build out infrastructure communications, including connected traffic lights and parking spots, called the “Transportation Mobility Cloud.” Ford CEO J…",2018-01-09T17:00:48Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Ford and Autonomic are building a smart city cloud platform,http://techcrunch.com/2018/01/09/ford-and-autonomic-are-building-a-smart-city-cloud-platform/,https://tctechcrunch2011.files.wordpress.com/2018/01/1xhjowg4_v-u-5dm0srsaag.jpeg,techcrunch,TechCrunch,cloud,1
Brian Heater,"Plenty of wearables companies make health claims that are dubious at best. It’s true that companies like Fitbit and Apple are getting a bit more serious about the whole thing, participating in university studies and working with insurance companies, but on a whole, I certainly wouldn’t trust my own wellbeing to any of them. An outgrowth of the Qualcomm Tricorder XPrize, Vitaliti is designed to be work by patients recently released from the hospital, so their attending physician can continue to monitor them remotely. The device sits around the collar, a bit like a yoke that rests on your neck, with two electrodes that attach to the skin and a sensor that sits in the ear. It’s not the most comfortable or visually appealing gadget, but you can wear it for long stretches. Cloud DX founder/CEO Robert Kaul was wearing around it for three days here at the show on a single charge — though the electrodes were attached, because the squares at the Las Vegas Convention Center apparently prefer people keep their shirts on. The wearable is capable of measure some standard wearable stuff, including movement and steps, along with more complex vitals like ECG, heart rate, oxygen saturation, respiration, core body temperature and blood pressure, meeting Xprize guidelines. It connects to a mobile app, offering up readings in real time and sending out an alert when one hits the danger zone. Cloud DX is currently seeking approval from the FDA for distribution as a medical device. The company has also developed an interface for the Hololens that overlays vitals onto patients in real-time.","Plenty of wearables companies make health claims that are dubious at best. It’s true that companies like Fitbit and Apple are getting a bit more serious about the whole thing, participating in university studies and working with insurance companies, but on a …",2018-01-11T21:55:52Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Xprize finalist Cloud DX’s Vitaliti is a serious health wearable,http://techcrunch.com/2018/01/11/xprize-finalist-cloud-dxs-vitaliti-is-a-serious-health-wearable/,https://tctechcrunch2011.files.wordpress.com/2018/01/dscf3798.jpg,techcrunch,TechCrunch,cloud,1
Romain Dillet,"French startup Blade, the company behind Shadow, is about to expand its cloud gaming service to the U.S. Customers who live in California can pre-order starting today, and they’ll be able to access the service on February 15th. The rest of the U.S. will be able to subscribe later this summer. Shadow is currently live in France, Belgium, Switzerland and Luxembourg. For a flat monthly fee, you can rent a gaming PC in a data center near you. You can then access this beefy computer using desktop and mobile apps as well as the company’s own little box. It’s a full-fledged Windows 10 instance — you can install Steam, Battle.net or whatever you want. Behind the scene, each user gets a high-end dedicated Nvidia GPU. The company is currently using a mix of GeForce GTX 1080 and Quadro P5000. Shadow also gives you 8 threads on an Intel Xeon 2620 processor, 12GB of RAM and 256GB of storage. Overall, it represents 8.2 teraflops of computing power — as a comparison, Microsoft promises 6 teraflops with the Xbox One X. In Europe, the service currently costs $54 per month, or $42 per month with a three-month commitment, or $36 if you’re willing to pay for a year (€44.95/€34.95/€29.95). American customers will pay more or less the same thing for the cheapest tier — $34.95 per month for a one-year commitment. The two other tiers are a bit cheaper in the U.S. — $39.95/month for a three-month commitment and $49.95/month with no commitment. It’s also worth noting that the company bills you every month even if you choose a yearly subscription. While pre-orders are open to everyone in California, there’s only a limited quantity of Shadow instances available in the company’s Californian data center. And this is key to understanding Shadow’s rollout. You need at least 15Mbps of bandwidth and you need to be near the data center. It would take too long to register an action if you lived thousands of miles away from the data center. Blade recently signed a partnership with Equinix to roll out its servers in more data centers around the world. But it takes time to build and install servers. It’s not as easy as releasing an app in the App Store. But you can expect more expansion news in the coming months.","French startup Blade, the company behind Shadow, is about to expand its cloud gaming service to the U.S. Customers who live in California can pre-order starting today, and they’ll be able to access the service on February 15th. The rest of the U.S. will be ab…",2018-01-04T14:01:27Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Shadow launches its cloud computer for gamers in California,http://techcrunch.com/2018/01/04/shadow-launches-its-cloud-computer-for-gamers-in-california/,https://tctechcrunch2011.files.wordpress.com/2018/01/shadow_lifestyle_box3.jpg,techcrunch,TechCrunch,cloud,1
Dell Cameron,"Hackers infiltrated Tesla’s cloud environment and stole computer resources to mine for cryptocurrency, according to the security firm RedLock. According to a report released on Tuesday detailing cloud security threats, RedLock’s Cloud Security Intelligence team—yes, it’s CSI team—notified Tesla of the intrusion and the vulnerability was addressed. The electric vehicle company was reportedly running one of hundreds of open-source systems the CSI team found accessible online without password protection. The exposure allowed hackers to access Tesla’s Amazon cloud environment, RedLock said. In an email to Gizmodo, a Tesla spokesperson said there is “no indication” the breach impacted customer privacy or compromised the security of its vehicles. “We maintain a bug bounty program to encourage this type of research, and we addressed this vulnerability within hours of learning about it,” a Tesla spokesperson told Gizmodo in an email. “The impact seems to be limited to internally-used engineering test cars only, and our initial investigation found no indication that customer privacy or vehicle safety or security was compromised in any way.” According to RedLock, mining cryptocurrency is likely a more valuable use of Tesla’s servers than the data they store. “The recent rise of cryptocurrencies is making it far more lucrative for cybercriminals to steal organizations’ compute power rather than their data,” RedLock CTO Gaurav Kumar told Gizmodo. “In particular, organizations’ public cloud environments are ideal targets due to the lack of effective cloud threat defense programs. In the past few months alone, we have uncovered a number of cryptojacking incidents including the one affecting Tesla.” Kumar said the attackers leveraged the Stratum mining protocol and evaded detection by hiding the true IP address of the mining pool server behind CloudFlare and keeping CPU usage low, among other tactics. “Given the immaturity of cloud security programs today, we anticipate this type of cybercrime to increase in scale and velocity,” Kumar said. “Organizations need to proactively monitor their public cloud environments for risky resource configurations, signs of account compromise, and suspicious network traffic just as they do for their on-premise environments.” Kumar added that while breaches at cloud service providers were almost never the fault of the host—Amazon, Microsoft, Google—security is a “shared responsibility.” “Organizations of every stripe are fundamentally obliged to monitor their infrastructures for risky configurations, anomalous user activities, suspicious network traffic, and host vulnerabilities,” he said. “Without that, anything the providers do will never be enough.” RedLock estimates that 8 percent of organizations will face attacks by cryptojackers—but due to ineffective network monitoring, most will go undetected. The firm’s finding show that 73 percent of organizations “allow the root user account to be used to perform activities—behavior that goes against security best practices,” while 16 percent “have user accounts that have potentially been compromised.” RedLock further estimates that 58 percent of organizations “publicly exposed at least one cloud storage service.” Meanwhile, it found, 66 percent of databases are not encrypted.","Hackers infiltrated Tesla’s cloud environment and stole computer resources to mine for cryptocurrency, according to the security firm RedLock. Read more...",2018-02-20T16:05:00Z,"{'id': None, 'name': 'Gizmodo.com'}","Tesla's Cloud Hacked, Used to Mine Cryptocurrency",https://gizmodo.com/teslas-cloud-hacked-used-to-mine-cryptocurrency-1823155247,"https://i.kinja-img.com/gawker-media/image/upload/s--9If97wfZ--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/voogdex3tfa7zczrvppx.jpg",,Gizmodo.com,cloud,1
Brian Heater,"We’ve heard it all before. Gaming in the cloud, the ability to play the most demanding titles, regardless of the limitations of our own hardware. That was the dream of OnLive, way back in 2010. It was a dream that didn’t end particularly well for the company, which unceremoniously dissolved in 2015 and eventually had its assets snapped up by Sony. It’s easy to Monday-morning-quarterback the end of a company. Maybe it wasn’t fully formed, or maybe it was ahead of its time. Whatever the case, a number of new startups believe they can succeed where predecessors failed. Parsec co-founder and CEO Benjy Boxer, for one, believes the service’s ultimate failure was a matter of bad timing. “OnLive was ahead of its time,” he said in a conversation with TechCrunch. “AWS didn’t exist. They had to build their own servers. They had to use an old codex for encoding video. And everyone has more bandwidth today. All of those things add up. And we have patent-pending technology that makes our software really low latency. That’s what we focus on.” At its heart, the service fills the same void as OnLive and newer services like LiquidSky, which started making the rounds earlier this year. The dream is the ability to make gameplay platform- and location-agnostic. That means playing on set-top boxes, gaming with friends who don’t own their own devoted rigs and even, potentially, gaming on mobile. (Though even with the heavy lifting happening on the backend, smartphones just aren’t built to handle really expansive PC gaming titles.) Parsec’s stated goal is making those sorts of titles as accessible as more ubiquitous streaming services like Spotify or Netflix, or whatever, and the company has received a $2.25 million seed round from the likes of Lerer Hippeau Ventures, Nextview Ventures and Notation Capital. The service has been available in beta since late last year, and launched versions of its software for Windows, Mac, Android, Linux and Raspberry Pi late last week. The latter will ultimately prove an interesting test for its ability to run on low-end hardware — with the right software and a good connection, you can accomplish a lot with less than optimal specs. Though, unlike other streaming services of its ilk, Parsec’s goal isn’t the elimination of gaming PCs entirely. “We believe that eliminating consoles and gaming PCs is really good for some casual gamers, who play for less than 10 hours a week,” says Boxer. But for people who play more and own a gaming PC already, Parsec is valuable to them, too, because they can use it for free and stream from their PC or invite friends to play with them.” It’s a big ask, getting people back on board with a concept that’s tried and failed to launch time and again, and one bad experience could be enough to sour users. But if Parsec is to be believed, it’s an idea whose time has finally come.","We’ve heard it all before. Gaming in the cloud, the ability to play the most demanding titles, regardless of the limitations of our own hardware. That was the dream of OnLive, way back in 2010. It was a dream that didn’t end particularly well for the company.…",2017-12-19T20:13:52Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Is the time finally right for platform-agnostic cloud gaming?,https://techcrunch.com/2017/12/19/is-the-time-finally-right-for-platform-agnostic-cloud-gaming/,https://tctechcrunch2011.files.wordpress.com/2017/12/parsec-fighter-gif-1.gif,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"After five months in public beta, Google today announced that its Cloud Memorystore for Redis, its fully managed in-memory data store, is now generally available. The service, which is fully compatible with the Redis protocol, promises to offer sub-millisecond responses for applications that need to use in-memory caching. And because of its compatibility with Redis, developers should be able to easily migrate their applications to this service without making any code changes. Cloud Memorystore offers two service tiers — a basic one for simple caching and a standard tier for users who need a highly available Redis instance. For the standard tier, Google offers a 99.9 percent availability SLA. Since it first launched in beta, Google added a few additional capabilities to the service. You can now see your metrics in Stackdriver, for example. Google also added custom IAM roles and improved logging. As for pricing, Google charges per GB-hour, depending on the service level and capacity you use. You can find the full pricing list here.","After five months in public beta, Google today announced that its Cloud Memorystore for Redis, its fully managed in-memory data store, is now generally available. The service, which is fully compatible with the Redis protocol, promises to offer sub-millisecon…",2018-09-20T00:00:12Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google’s Cloud Memorystore for Redis is now generally available,http://techcrunch.com/2018/09/19/googles-cloud-memorystore-for-redis-is-now-generally-available/,https://techcrunch.com/wp-content/uploads/2018/09/memorystore.max-1000x1000.png?w=721,techcrunch,TechCrunch,cloud,1
Ron Miller,"While we might like to think all of our applications are equal in our eyes, in reality some are more important than others and require an additional level of security. To meet those requirements, Google introduced shielded virtual machines at Google Next today. As Google describes it, “Shielded VMs leverage advanced platform security capabilities to help ensure your VMs have not been tampered with. With Shielded VMs, you can monitor and react to any changes in the VM baseline as well as its current runtime state.” These specialized VMs run on GCP and come with a set of partner security controls to defend against things like rootkits and bootkits, according to Google. There are a whole bunch of things that happen even before an application launches inside a VM, and each step in that process is vulnerable to attack. That’s because as the machine starts up, before you even get to your security application, it launches the firmware, the boot sequence, the kernel, then the operating system — and then and only then, does your security application launch. That time between startup and the security application launching could leave you vulnerable to certain exploits that take advantage of those openings. The shielded VMs strip out as much of that process as possible to reduce the risk. “What we’re doing here is we are stripping out any of the binary that doesn’t absolutely have to be there. We’re ensuring that every binary that is there is signed, that it’s signed by the right party, and that they load in the proper sequence,” a Google spokesperson explained. All of these steps should reduce overall risk. Shielded VMs are available in Beta now","While we might like to think all of our applications are equal in our eyes, in reality some are more important than others and require an additional level of security. To meet those requirements, Google introduced shielded virtual machines at Google Next toda…",2018-07-25T16:00:27Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud introduces shielded VMs for additional security,http://techcrunch.com/2018/07/25/google-cloud-introduces-shielded-vms-for-additional-security/,https://techcrunch.com/wp-content/uploads/2018/07/GettyImages-871464138.jpg?w=764,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"These days, no cloud platform is complete without support for GPUs. There’s no other way to support modern high-performance and machine learning workloads without them, after all. Often, the focus of these offerings is on building machine learning models, but today, Google is launching support for the Nvidia P4 accelerator, which focuses specifically on inferencing to help developers run their existing models faster. In addition to these machine learning workloads, Google Cloud users can also use the GPUs for running remote display applications that need a fast graphics card. To do this, the GPUs support Nvidia Grid, the company’s system for making server-side graphics more responsive for users who log in to remote desktops. Since the P4s come with 8GB of DDR5 memory and can handle up to 22 tera-operations per second for integer operations, these cards can handle pretty much anything you throw at them. And since buying one will set you back at least $2,200, if not more, renting them by the hour may not be the worst idea. On the Google Cloud, the P4 will cost $0.60 per hour with standard pricing and $0.21 per hour if you’re comfortable with running a preemptible GPU. That’s significantly lower than Google’s prices for the P100 and V100 GPUs, though we’re talking about different use cases here, too. The new GPUs are now available in us-central1 (Iowa), us-east4 (N. Virginia), Montreal (northamerica-northeast1) and europe-west4 (Netherlands), with more regions coming soon.","These days, no cloud platform is complete without support for GPUs. There’s no other way to support modern high-performance and machine learning workloads without them, after all. Often, the focus of these offerings is on building machine learning models, but…",2018-08-06T16:23:50Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud gets support for Nvidia’s Tesla P4 inferencing accelerators,http://techcrunch.com/2018/08/06/google-cloud-gets-support-for-nvidias-tesla-p4-inferencing-accelerators/,https://techcrunch.com/wp-content/uploads/2018/08/GettyImages-944347542.jpg?w=600,techcrunch,TechCrunch,cloud,1
Lucas Matney,"Whether the action cam business can alone sustain the GoPro business (in light of the recent Karma drone program shutdown ) is going to at least rely on them continuing to dominate it and find new revenue streams within it. Part of this strategy will be continuing to refine the company’s $4.99 per month subscription service, GoPro Plus. Today, the company is announcing a new change to the service which should reduce the friction of freeing up space on your camera and getting your footage into the cloud. One of the Plus’s biggest selling points, its no questions asked return policy for getting newer GoPro models replaced, obviously has strong appeals for the more extreme users who are pushing the limits of the hardware. In addition to discounts on accessories and better phone support, the big appeal for more mainstream consumers has largely centered on the automatic storage space the company allows, which gives users unlimited photo storage and space for about 35 hours of video footage. In the past this has only been possible through connecting the GoPro to your computer and uploading that way, next month the company will let users do all of this wirelessly through the GoPro app. The company’s top-of-the-line GoPro Hero6 Black brought 5ghz Wi-Fi support which sped up the still incredibly lengthy upload times, this 3x improvement brings a lot more utility to this feature even though you’ll still have to be patient. The feature arrives for Plus subscribers on iOS on February 20 and comes to Android in the spring.",Whether the action cam business can alone sustain the GoPro business (in light of the recent Karma drone program shutdown) is going to at least rely on them continuing to dominate it and find new revenue streams within it. Part of this strategy will be contin…,2018-01-31T21:02:45Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",GoPro adds mobile cloud backup to its Plus subscription service,http://techcrunch.com/2018/01/31/gopro-adds-mobile-cloud-backup-to-its-plus-subscription-service/,https://tctechcrunch2011.files.wordpress.com/2017/04/screen-shot-2017-04-18-at-10-54-22-am.png,techcrunch,TechCrunch,cloud,1
David Murphy,"We’re turning the lens around for this week’s Tech 911. Lifehacker Managing Editor Virginia Smith posed a question in our internal Slack channel that cuts wide and deep: “It’s safe to delete photos from my iPhone, right?” The answer is a bit more complicated than you might think—and can have disastrous consequences if you get it wrong. Do you have a tech question keeping you up at night? Tired of troubleshooting your Windows or Mac? Looking for advice on apps, browser extensions, or utilities you can use to accomplish a particular task? Let us know! Tell us in the comments below or email david.murphy@lifehacker.com. It’s incredibly safe to delete photos from your iPhone. If your iPhone’s space is filling up, if you don’t like the composition of a shot, or if you’ve offloaded your photos to another service or storage device, delete away. Your smartphone won’t mind. Of course, by “safe,” you’re actually asking the Prestige -like question, “Am I deleting a copy or deleting an original.” And like the movie, the answer gets a little complicated. If you’ve set up iCloud on your iPhone and you’re automatically uploading photos to Apple’s service, as shown below: then it’s critical to realize that your “synchronization” isn’t a one-way street. Delete a photo on your iPhone and you’ll remove it from iCloud the next time your device has an internet connection. Apple warns you about this when you delete a photo or video in the Photos app, noting that deleting the object will remove it from iCloud Photos on “all your devices.” It’s implied that this removes it from iCloud as well, but Apple could stand to be more explicit, given the apparent confusion. (Virginia isn’t the only one who has been tripped up by this wording.) The same logic applies if you’re synchronizing iCloud Photos on your Mac. Delete a photo there, and the item will disappear from connected iPhones, iPads, and iCloud itself the next time that Mac hops online. Of course, if you delete a photo from your iPhone and didn’t mean to remove it from everything, it’s easy to reverse the process. Just look for the “Recently Deleted” Album under “Other Albums” on the iOS Photos app, on the left sidebar under “Library” on the macOS Photos app, or within the Albums listing at iCloud.com. Don’t dawdle; you only have up to 40 days before your photos and videos disappear forever. If your many Apple devices are all set to synchronize iCloud Photos, you can restore deleted photos from any of these devices. You don’t need to have the actual iPhone you used to originally delete a photo, for example. What makes Apple’s iCloud treatment confusing isn’t anything Apple has done. Rather, it’s the fact that many cloud services—like Google Photos, for example—treat photo synchronization differently. That’s to be expected, but I think a number of people assume that each service does the same thing. They don’t. For example, when you back up photos to the cloud using Google Photos on an iPhone, you can go ahead and delete these photos—using the iOS Photos app—without issue. Your photos will still live in Google’s cloud unless you delete them in the Google Photos app or photos.google.com. This is the main reason why Google’s “Free Up Space” Feature is so handy. Once you’ve uploaded your photos to Google’s service, you can safely delete them off your device without losing them forever, as long as you use the specific feature in Google’s Photos app, shown below: Now, if you were to delete these photos directly from the Google Photos app’s stream, you’d be deleting them from the cloud as well. This “Free Up Space” option deletes your device’s photos—and only your device’s photos—when you press that button. Delete photos the regular way, and you’re removing them from Google’s cloud and app, but not your device. Got it? If you want Google Photos to also remove an item on your iOS device (which removes it from the iOS Photos app and iCloud), you’ll have to pick the option once you’ve deleted a photo or video in the Google Photos app: And since I’m just focusing on iOS to answer Virginia’s question, I should note that this process is more streamlined if you’re using an Android device that features Google Photos as the primary photo-storage app—like Google’s Pixel 2. Delete a photo there, and it’s gone on the device and gone in your cloud storage—unless you restore it via the Trash in either the app or the online site. (And you can still use the “Free Up Space” feature on Android; Google won’t delete those same photos and videos from the cloud, don’t worry.) In summary: Deleting items in the Photos app removes them from iCloud, if you’re signed in and synchronizing, as well as any other device that’s set up similarly. Deleting items the Photos app won’t remove them from Google Photos if you’ve already uploaded them to Google’s cloud storage. Deleting items in the Google Photos app will delete them from Google’s cloud storage and any other device that also uses Google Photos—but only in the Google Photos app. Deleting items in the Google Photos app won’t delete them off your iOS device unless you explicitly tell Google Photos to do that when you’re deleting the item. Maybe... consider making a regular backup of your device’s photos on an external hard drive (or uploaded to another cloud service ) in case this is all just too confusing.","We’re turning the lens around for this week’s Tech 911. Lifehacker Managing Editor Virginia Smith posed a question in our internal Slack channel that cuts wide and deep: “It’s safe to delete photos from my iPhone, right?” Read more...",2018-07-27T13:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",Why Did iCloud Delete All of My Photos?,https://lifehacker.com/why-did-icloud-delete-all-of-my-photos-1827908714,"https://i.kinja-img.com/gawker-media/image/upload/s--iD8V-5jr--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/wk7llctnrhhe7uw8pyzn.png",,Lifehacker.com,cloud,1
Ron Miller,"Building a GPU-fueled infrastructure service is not a simple matter for a startup to undertake, but that’s precisely what Paperspace has set out to do. Today, it took it to the next level when it announced Gradient, a service platform that eliminates the need to deploy servers for AI and machine learning projects. Like any serverless architecture, the servers don’t really go away, but the need to deploy them manually does. Gradient provides the means to simply deploy code and Paperspace will take care of all the allocation and management, removing a big piece of the complexity associated with building machine learning models. Dillon Erb, company co-founder and CEO, says that when they launched the company several years ago, GPUs were not as commonly available as a cloud service as they are today. They initially provided a way to launch GPU instances in virtual machines, something they still do, but they saw a problem around a lack of tooling. Erb explained that large companies tend to build their own tool sets, but most companies or teams for that matter, don’t have the resources to spend the time to build the underlying plumbing. “Just having raw compute is not sufficient. You need a software stack,” he said. They spent the last year building Gradient to provide that structure for developers to concentrate on building models and code and collaborating around a project, while leaving the management to Paperspace. It removes the need to have a DevOps team to manage the interactions between the team, the code and the underlying infrastructure. “Just give us code, a Docker container. You don’t have to schedule a VM because we do it for you. You never have to fire up a machine,” he said. Paperspace has been trying to solve hard problems around deploying GPUs in the cloud, since it graduated from the Y Combinator Winter 2015 class. It has raised over $11 million in funding since it launched in 2014 including a $4 million seed round in 2016.","Building a GPU-fueled infrastructure service is not a simple matter for a startup to undertake, but that’s precisely what Paperspace has set out to do. Today, it took it to the next level when it announced Gradient, a service platform that eliminates the need…",2018-03-21T13:13:02Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Paperspace goes serverless to simplify AI deployment in the cloud,http://techcrunch.com/2018/03/21/paperspace-goes-serverless-to-simplify-ai-deployment-in-the-cloud/,https://techcrunch.com/wp-content/uploads/2018/03/gettyimages-935787754.jpg?w=600,techcrunch,TechCrunch,cloud,1
Brian Heater,"Once a seemingly unstoppable retail juggernaut, Walmart’s been scrambling to define its digitally in this Amazon-defined era. This morning, the company announced that it’s struck a five-year deal with Microsoft, Amazon’s chief cloud competitor. These sorts of partnerships are a regular occurrence for AWS — in fact, it announced one with Fortnite maker Epic Games, just this morning. The companies involved tend to put on a big show, in return for a discount on services, but Walmart and Microsoft are happily playing into the concept of teaming up to take on Amazon. Microsoft’s certainly not making any bones about the competition. In an interview, Satya Nadella told The Wall Street Journal that the fight against Amazon “is absolutely core to this,” adding, “How do we get more leverage as two organizations that have depth and breadth and investment to be able to outrun our respective competition?” Of course, neither Walmart nor Microsoft can be framed as an underdog in any respect, but Amazon’s stranglehold on online retail also can’t be understated. Not even a massive outage at the height of Prime Day could do much to ruffle the company’s feathers. Included in the deal are AI/ML technologies design to help optimize the in-store experience — one of the key factors Walmart brings to the table in a battle against Amazon, which has mostly just dabbled in brick and mortar. For its part, Walmart has been testing cashier-less stores along the lines of Amazon’s, but the company has just to officially unveil its plans in that space.","Once a seemingly unstoppable retail juggernaut, Walmart’s been scrambling to define its digitally in this Amazon-defined era. This morning, the company announced that it’s struck a five-year deal with Microsoft, Amazon’s chief cloud competitor. These sorts of…",2018-07-17T15:45:33Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Walmart enlists Microsoft cloud in battle against Amazon,http://techcrunch.com/2018/07/17/walmart-enlists-microsoft-cloud-in-battle-against-amazon/,https://techcrunch.com/wp-content/uploads/2018/01/gettyimages-463871936.jpg?w=600,techcrunch,TechCrunch,cloud,1
Danny Crichton,"Data sovereignty is on the rise across the world. Laws and regulations increasingly require that citizen data be stored in local data centers, and often restricts movement of that data outside of a country’s borders. The European Union’s GDPR policy is one example, although it’s relatively porous. China’s relatively new cloud computing law is much more strict, and forced Apple to turn over its Chinese-citizen iCloud data to local providers and Amazon to sell off data center assets in the country. Now, it appears that India will join this policy movement. According to Aditya Kalra in Reuters, an influential cloud policy panel has recommended that India mandate data localization in the country, for investigative and national security reasons, in a draft report set to be released later this year. That panel is headed by well-known local entrepreneur Kris Gopalakrishnan, who founded Infosys, the IT giant. That report would match other policy statements from the Indian political establishment in recent months. The government’s draft National Digital Communications Policy this year said that data sovereignty is a top mission for the country. The report called for the government by 2022 to “Establish a comprehensive data protection regime for digital communications that safeguards the privacy, autonomy and choice of individuals and facilitates India’s effective participation in the global digital economy.” It’s that last line that is increasingly the objective of governments around the world. While privacy and security are certainly top priorities, governments now recognize that the economics of data are going to be crucial for future innovation and growth. Maintaining local control of data — through whatever means necessary — ensures that cloud providers and other services have to spend locally, even in a global digital economy. India is both a crucial and an ironic manifestation of this pattern. It is crucial because of the size of its economy: public cloud revenues in the country are expected to hit $2.5 billion this year, according to Gartner’s estimates, an annual growth rate of 37.5%. It is ironic because much of the historical success of India’s IT industry has been its ability to offer offshoring and data IT services across borders. Indian Prime Minister Narendra Modi has made development and rapid economic growth a top priority of his government. (Krisztian Bocsi/Bloomberg via Getty Images) India is certainly no stranger to localization demands. In areas as diverse as education and ecommerce, the country maintains strict rules around local ownership and investment. While those rules have been opening up slowly since the 1990s, the explosion of interest in cloud computing has made the gap in regulations around cloud much more apparent. If the draft report and its various recommendations become law in India, it would have significant effects on public cloud providers like Microsoft, Google, Amazon, and Alibaba, all of whom have cloud operations in the country. In order to comply with the regulations, they would almost certainly have to expend significant resources to build additional data centers locally, and also enforce data governance mechanisms to ensure that data didn’t flow from a domestic to a foreign data center accidentally or programmatically. I’ve written before that these data sovereignty regulations ultimately benefit the largest service providers, since they’re the only ones with the scale to be able to competently handle the thicket of constantly changing regulations that govern this space. In the India case though, the expense may well be warranted. Given the phenomenal growth of the Indian cloud IT sector, it’s highly likely that the major cloud providers are already planning a massive expansion to handle the increasing storage and computing loads required by local customers. Depending on how simple the regulations are written, there may well be limited cost to the rules. One question will involve what level of foreign ownership will be allowed for public cloud providers. Given that several foreign companies already exist in the marketplace, it might be hard to completely eliminate them entirely in favor of local competitors. Yet, the large providers will have their work cut out for them to ensure the market stays open to all. The real costs though would be borne by other companies, such as startups who rely on customer datasets to power artificial intelligence. Can Indian datasets be used to train an AI model that is used globally? Will the economics be required to stay local, or will the regulations be robust enough to handle global startup innovation? It would be a shame if the very law designed to encourage growth in the IT sector was the one that put a dampener on it. India’s chief objective is to ensure that Indian data benefits Indian citizens. That’s a laudable goal on the surface, but deeply complicated when it comes time to write these sorts of regulations. Ultimately, consumers should have the right to park their data wherever they want — with a local provider or a foreign one. Data portability should be key to data sovereignty, since it is consumers who will drive innovation through their demand for best-in-class services.","Data sovereignty is on the rise across the world. Laws and regulations increasingly require that citizen data be stored in local data centers, and often restricts movement of that data outside of a country’s borders. The European Union’s GDPR policy is one ex…",2018-08-04T19:34:53Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",India may become next restricted market for U.S. cloud providers,http://techcrunch.com/2018/08/04/india-may-become-next-restricted-market-for-u-s-cloud-providers/,https://techcrunch.com/wp-content/uploads/2018/08/GettyImages-535734188.jpg?w=689,techcrunch,TechCrunch,cloud,1
Jon Russell,"Alibaba is bringing its cloud computing business into India as it continues to expand its first-growing business unit. The Chinese firm said today that its first data center on Indian soil will come online in January and be based out of Mumbai. The business already has clients in India, but a local presence will allow it to better service customers in the country, it added. Beyond offering standard cloud products — like large scale computing, storage and big data capabilities — India-based customers will get access to elastic computing, database, storage and content delivery, networking, analytics and big data, containers, middleware, and security. The new center will give Alibaba Cloud 33 availability zones, which covers regions including China, Hong Kong, Singapore, Japan, Australia, the Middle East, Europe, and the U.S.. “As we build out the Alibaba Cloud network globally, India is another important piece that is now firmly in place. This continues our commitment to India, helping it to develop trade opportunities with other markets in the region and beyond,” Simon Hu, Senior Vice President of Alibaba Group and President of Alibaba Cloud, said in a statement. Alibaba previously revealed plans for the India data center — and another in Indonesia — in June. Earlier this year, we wrote that Alibaba is aiming to compete with the likes of AWS, Microsoft Azure and Google Cloud in the long run. For now, it is seeing most of its traction in China, but revenue is increasing at the rate of nearly double for every quarter this year. Alibaba Cloud hit net sales of RMB 3 billion ($447 million) in the most recent quarter, up from RMB 2.4 billion in the previous three-month period when it hit one million customers for the first time. Overall, the cloud computing business recorded a RMB 697 million loss, or negative $105 million, which it put down to investment in R&amp;D having nearly reached breakeven in the previous quarter. India is also a key focus for Alibaba generally. The company has invested heavily in payment and e-commerce firm Paytm — helping it push into digital banking services, as Alibaba affiliate Ant Financial has done in China — so it makes sense that Alibaba Cloud is also putting focus there. In evidence of its global focus, Alibaba Cloud made its first investment outside of China last month when it led a $27 million round for open source startup MariaDB.",Alibaba is bringing its cloud computing business into India as it continues to expand its first-growing business unit. The Chinese firm said today that its first data center on Indian soil will come online in January and be based out of Mumbai. The business a…,2017-12-20T10:43:26Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Alibaba Cloud is opening its first data center in India,https://techcrunch.com/2017/12/20/alibaba-cloud-india/,https://tctechcrunch2011.files.wordpress.com/2017/02/alibaba-cloud-in-hong-kong.jpg,techcrunch,TechCrunch,cloud,1
Ron Miller,"How many times do you hear about a company exposing sensitive data because they forgot to lock down a data repository on Amazon? It happens surprisingly often. Chef wants to help developers and operations teams prevent that kind of incident. Today, the company released InSpec 2.0, which is designed to help automate applications security and compliance in the cloud. InSpec is a free open source tool that enables development teams to express security and compliance rules as code. Version 1.0 was about ensuring that applications were set up properly. The new version extends this capability to the cloud where companies are running the applications, allowing teams to test and write rules for compliance with cloud security policy. It supports AWS and Azure and comes with 30 common configurations out of the box including Docker, IIS, NGINX and PostgreSQL. Companies running multiple applications across multiple clouds face challenges in today’s continuous development environment. It’s actually fairly easy to leave that database exposed when it’s up to humans to continuously monitor if it’s in compliance or not. Chef wants to help with that problem by offering a tool to automate compliance. It takes some work in getting the security, development and operations teams together to discuss what needs to be locked down, but once they come to an agreement, they can to use InSpec to write rules to validate proper cloud configurations using the InSpec scripting language. Chef’s director of product marketing Julian Dunn says that anyone used to using scripting languages should be able to pick it up. “A language like InSpec allows customers to customize and write the rules specific to the cloud they are in and specific to their cloud deployment and check things they care about it,” he said. Scripting language example. Code sample: Chef “The language is designed to be easy to read and write. It’s intended for security engineering folks who don’t have programming background, but have scripting experience,” Dunn added. Once you write these scripts, you can run tests against your code, see which areas out of compliance and take steps to fix them. InSpec was created via the acquisition of VulcanoSec, a German compliance and security firm that Chef purchased in 2015. InSpec 2.0 is open source and available for download on Github. Featured Image: Roy Scott/Getty Images","How many times do you hear about a company exposing sensitive data because they forgot to lock down a data repository on Amazon? It happens surprisingly often. Chef wants to help developers and operations teams prevent that kind of incident. Today, the compan…",2018-02-20T13:00:16Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Chef InSpec 2.0 helps automate security compliance in cloud apps,http://techcrunch.com/2018/02/20/chef-inspec-2-0-wants-to-help-companies-automate-security-compliance-in-cloud-apps/,https://tctechcrunch2011.files.wordpress.com/2018/02/gettyimages-737369061.jpg,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Bandai Namco Entertainment announced the latest entrant in its series of Dragon Ball games this week. Dragon Ball Legends is a player versus player (PvP) mobile game that has players from all over the world battle with each other in real time by using their move cards. From all I’ve seen, it looks like a pretty fun game, though I know nothing about Dragon Ball and I have an unreasonable disinterest in card-based games. What made me perk up, though, was when I heard that Bandai Namco opted to use Google’s Cloud Network to host all the infrastructure for the game and that one of the main components of this system is Cloud Spanner, Google’s globally distributed database. To make a real-time game work at all is hard enough, but Bandai Namco wanted players from all over the world to be able to play against each other. There’s a reason most games distribute players into regions based on their geography, though. In a real-time game, latency matters, as every hardened PUBG player will tell you, and the farther you get away from the game server, the higher your latency will likely be. As Bandai Namco’s Keigo Ikeda and Toshitaka Tachibana told me ahead of the launch, the team opted to divide every game second into 250ms intervals, so while the game looks like it’s real-time to users, it’s actually a really fast turn-based game at its core. “Technically speaking, to the user’s eye, it’s real-time, but on the server, players have their own turn,” said Tachibana. By opting for the Google Cloud Platform and Cloud Spanner as the database to keep track of all moves, the average latency the team has seen during its tests is 138ms, which allows for plenty of wiggle room. To make all of this work, the team spent almost two and a half years building out the necessary infrastructure, and Tachibana admitted that the team learned quite a bit more than it expected about network latency. During early tests, the team wanted to create a peer-to-peer connection to have players battle each other, for example, but depending on the carriers, the difference in user experience varied too much. The team also had to learn how to best route traffic between players, something that most gaming developers don’t really have to think about most days. “We were pretty frustrated with everyone who wasn’t Google,” said Tachibana. Indeed, Cloud Spanner is the core service here, and the team says it opted for it because it gives it a globally distributed strongly consistent database to work with. Because any change propagates across the global network within milliseconds, Cloud Spanner is actually a really interesting option for game developers who need low latencies and a ground truth that can be distributed between a global player base. Cloud Spanner is not a cheap service, and the team acknowledged as much, though, as Google Cloud Director of Solutions Miles Ward noted, providing the service isn’t cheap either. “Spanner does things from a consistency standpoint that you can’t get from anybody else, so it’s a place where we have to spend more, too,” he said (and before my friends at Microsoft email me: yes, Cosmos DB also offers features that are comparable to Cloud Spanner, as well as a wider range of consistency options). The Bandai Namco team also noted that Google’s vast private network was another major factor behind its decision. Because Google owns its own network, the data can jump between fewer networks to reach both the central database and the opposing player. To make Dragon Ball Legends run smoothly, the team is also using BigQuery to manage and analyze its data, as well as some of the company’s Firebase services. Tachibana noted that Bandai Namco is placing a big bet on this new game, but that the team also wanted to create a benchmark for what a globally distributed PvP game can look like. “We hope that when other developers look to a similar gameplay, they’ll say that it’s hard to top,” he said. “And also, from a technical side, we know that the people who are part of the industry will understand how amazing it is to realize this entire process.” If you are not a developer and just want to play a new PvP Dragon Ball game, I’m afraid you’ll have to wait a little bit longer, though. The game will arrive in Apple’s App Store and the Google Play Store later this year. [gallery ids=""1609628,1609627,1609629""]",Bandai Namco Entertainment announced the latest entrant in its series of Dragon Ball games this week. Dragon Ball Legends is a player versus player (PvP) mobile game that has players from all over the world battle with each other in real time by using their m…,2018-03-20T18:30:43Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",The new Dragon Ball game is powered by Google’s cloud,http://techcrunch.com/2018/03/20/bandai-namco-bets-on-google-cloud-to-power-its-new-pvp-dragon-ball-game/,https://techcrunch.com/wp-content/uploads/2018/03/gettyimages-621132178.jpg?w=599,techcrunch,TechCrunch,cloud,1
Devin Coldewey,"The strange new breed of malicious cryptocurrency miners spares no one, it seems: Tesla is the latest to be struck by this trendy form of hackery. A poorly secured cloud computing setup let them waltz right in. It’s only the latest example of several detected by cloud security outfit RedLock, which has tracked a series of Kubernetes admin consoles wide open to anyone looking. Not even password-protected. If RedLock could find them, so could hackers — and they did. By logging in and carefully disguising the cloud computing usage and associated traffic, they managed to quietly mine using Tesla’s AWS pod for… well, it’s anybody’s guess how long. And given the volatility of cryptocurrency markets these days, it’s also anybody’s guess how much and of what coin. Obviously, the solution here is to have literally any kind of security on your infrastructure. But hackers are clever and companies should also be watching for unusual levels of traffic and other usage indicators, and also monitor for non-standard user behaviors. But seriously, at least a password. Featured Image: Brian A Jackson / Shutterstock","The strange new breed of malicious cryptocurrency miners spares no one, it seems: Tesla is the latest to be struck by this trendy form of hackery. A poorly secured cloud computing setup let them waltz right in. Read More",2018-02-20T22:47:18Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Poor cloud security let hackers mine cryptocurrency on Tesla’s dime,http://techcrunch.com/2018/02/20/poor-cloud-security-let-hackers-mine-cryptocurrency-on-teslas-dime/,https://tctechcrunch2011.files.wordpress.com/2015/10/shutterstock_1719293211.jpg,techcrunch,TechCrunch,cloud,1
Ron Miller,"Salesforce hasn’t wasted any time turning the MuleSoft acquisition into a product of its own, announcing the Salesforce Integration Cloud this morning. While in reality it’s too soon to really take advantage of the MuleSoft product set, the company is laying the groundwork for the eventual integration into the Salesforce family with this announcement, which really showcases why Salesforce was so interested in them that they were willing to fork over $6.5 billion. The company has decided to put their shiny new bauble front and center in the Integration Cloud announcement, so that when they are in the fold, they will have a place for them to hit the ground running The Integration Cloud itself consists of three broad pieces: The Integration Platform, which will eventually be based on MuleSoft; Integration Builder, a tool that lets you bring together a complete picture of a customer from Salesforce tools, as well as across other enterprise data repositories and finally Integration Experiences, which is designed to help brands build customized experiences based on all the information you’ve learned from the other tools. For now, it involves a few pieces that are independent of MuleSoft including a workflow tool called Lightning Flow, a new service that is designed to let Salesforce customers build workflows using the customer data in Salesforce CRM. It also includes a dash of Einstein, Salesforce’s catch-all brand for the intelligence layer that underlies the platform, to build Einstein intelligence into any app. Salesforce also threw in some Trailhead education components to help customers understand how to best make use of these tools. But make no mistake, this is a typical Salesforce launch. It is probably earlier than it should be, but it puts the idea of integration out there in the minds of its customers and lays a foundation for a much deeper set of products and services down the road when MuleSoft is more fully integrated into the Salesforce toolset. For now, it’s important to understand that this deal is about using data to fuel the various pieces of the Salesforce platform and provide the Einstein intelligence layer with information from across the enterprise wherever it happens to live, whether that’s in Salesforce, another cloud application or some on-prem legacy systems. This should sound familiar to folks attending the Adobe Summit this week in Las Vegas, since it’s eerily similar to what Adobe announced on stage yesterday at the Summit keynote. Adobe is calling it a customer experience system of record, but the end game is pretty much the same: bringing together data about a customer from a variety of sources, building a single view of that customer, and then turning that insight into a customized experience. That they chose to make this announcement during the Adobe Summit, where Adobe has announced some data integration components of its own could be a coincidence, but probably not.","Salesforce hasn’t wasted any time turning the MuleSoft acquisition into a product of its own, announcing the Salesforce Integration Cloud this morning. While in reality it’s too soon to really take advantage of the MuleSoft product set, the company is laying …",2018-03-28T14:59:24Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Salesforce introduces Integration Cloud on heels of MuleSoft acquisition,http://techcrunch.com/2018/03/28/salesforce-introduces-integration-cloud-on-heels-of-mulesoft-acquisition/,https://techcrunch.com/wp-content/uploads/2018/03/gettyimages-802264004-1.jpg?w=600,techcrunch,TechCrunch,cloud,1
Ron Miller,"One of the primary advantages of cloud computing has always been the ability to scale up to meet short-term needs and scale back when that need has been met. In other words, you don’t have to pay for infrastructure that you don’t need to hedge against heavy usage. That has mostly meant server capacity, but over time it has been applied to other cloud services. Now, developers can configure auto scaling across a set of AWS services in a single unified interface. Companies typically use multiple AWS services to build their applications, and up until now that required some work to make sure the various pieces were scaling as the application required it. In a company blog post announcing the new feature, Amazon’s Jeff Barr explained how the new service greatly simplifies auto-scaling for developers. “You no longer need to set up alarms and scaling actions for each resource and each service. Instead, you simply point AWS Auto Scaling at your application and select the services and resources of interest. Then you select the desired scaling option for each one, and AWS Auto Scaling will do the rest, helping you to discover the scalable resources and then creating a scaling plan that addresses the resources of interest,” Barr wrote in the blog post. The new service gives developers a set of auto-scaling options, offering a way to balance cost, availability or a combination of the two, depending on your company’s and the application’s specific requirements. As you set up a given scaling threshold, AWS Auto Scaling actually can create a set of scaling policies automatically, based on your configuration. Screenshot: Amazon The way it works is you select an application, then create a scaling configuration plan where you set scaling targets for each resource. This new feature is available immediately in US East (Northern Virginia), US East (Ohio), US West (Oregon), EU (Ireland), and Asia Pacific (Singapore) regions.","One of the primary advantages of cloud computing has always been the ability to scale up to meet short-term needs and scale back when that need has been met. In other words, you don’t have to pay for infrastructure that you don’t need to hedge against heavy u…",2018-01-17T13:40:40Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Amazon launches autoscaling service on AWS,http://techcrunch.com/2018/01/17/amazon-launches-autoscaling-service-on-aws/,https://tctechcrunch2011.files.wordpress.com/2018/01/mvimg_20171128_084917.jpg,techcrunch,TechCrunch,cloud,1
Ron Miller,"Diane Greene, who heads up Google’s cloud unit, often marvels at how much her company invests in Google Cloud infrastructure. It’s with good reason. Over the past three years since Greene came on board, the company has spent a whopping $30 billion beefing up the infrastructure. Today it announced three new undersea cables scheduled to go online next year. In a blog post penned by Ben Treynor, vice president of Google’s cloud platform, he announced the three undersea cables along with five new regions. Map: Google Let’s start with the cables. The Curie cable, named after Nobel prize-winning scientist Marie Curie, is the first intercontinental cable built by a company that’s not in telecommunications. It will run from LA to Chile, crossing over 6200 miles, according to published reports. Google boasts that it will be the first new cable to land in Chile in almost 20 years and will serve Google customers across Latin America. The second cable, called the Havfrue (Danish for “mermaid”) cable will run from the east coast of the US to Denmark and come online at the end of next year. The last one has the rather unimaginative name, HK-G cable. Google is partnering to build this one with RTI-C and NEC to improve its coverage in the Pacific region. This is a 2400 mile cable that will run from Hong Kong to Guam, according to a report in the Wall Street Journal. “While we haven’t hastened the speed of light, we have built a superior cloud network as a result of the well-provisioned direct paths between our cloud and end-users,” Traynor wrote. Google now has a direct investment in 11 under-sea cables and it leases capacity on several others. It announced an undersea cable that runs from Singapore to Australia last spring. In addition to the undersea cables, the company also announced five new regions including The Netherlands and Montreal coming on line in the first quarter this year. Three others in Los Angeles, Finland, and Hong Kong will come online later this year. Google promises they are not done yet and there will be additional announcements of other regions. Featured Image: Gerard Koudenburg/Getty Images","Diane Greene, who heads up Google’s cloud unit, often marvels at how much her company invests in Google Cloud infrastructure. It’s with good reason. Over the past three years since Greene came on board, the company has spent a whopping $30 billion beefing up …",2018-01-16T15:17:16Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud infrastructure expansion to continue with three new undersea cables,http://techcrunch.com/2018/01/16/google-cloud-infrastructure-expansion-to-continue-with-three-new-undersea-cables/,https://tctechcrunch2011.files.wordpress.com/2018/01/gettyimages-516856330.jpg,techcrunch,TechCrunch,cloud,1
Ingrid Lunden,"Just days after Google announced that it would acquire Velostrata to help customers migrating more of their operations into cloud environments, HPE under its new CEO Antonio Neri is also upping its game in the same department. Today the company announced that it would acquire Plexxi, a specialist in software-defined data center solutions, aimed at optimising application performance for enterprises that are using hybrid cloud environments. A spokesperson confirmed that the companies are not currently revealing the terms of the deal, which is expected to close in the third quarter of 2018 (ending July 31). For some book-ended context, Plexxi was last valued at around $267 million as of its last financing round, more than two years ago in January 2016, according to PitchBook, and the previous cloud infrastructure acquisition HPE made, of SimpliVity over a year ago, was for $650 million. Plexxi’s investors included GV (formerly Google Ventures), Lightspeed Venture Partners, Matrix and more. Ric Lewis, the VP &amp; GM of HPE’s software-defined and cloud group, said that the plan will be to integrate Plexxi into HPE’s existing products in two areas. The first of these is in the company’s hyperconverged solutions business, where HPE’s acquisition of SimpliVity also sites. “Plexxi will enable us to deliver the industry’s only hyperconverged offering that incorporates compute, storage and data fabric networking into a single solution, with a single management interface and support,” he wrote in a blog post. The second of these will be to bring Plexxi’s HCN tech to HPE Synergy and its composable infrastructure business. This, Lewis explained, is “a new category of infrastructure that delivers fluid pools of storage and compute resources that can be composed and recomposed as business needs dictate.” Plexxi will enable this approach to extend also to rack-based solutions in private clouds. It’s not clear what roles Plexxi execs, which include CEO Rich Napolitano — who had been the president of EMC before joining the startup — will have at HPE. We’re asking and will update as we learn more. “Plexxi and HPE’s values and vision for the future are closely aligned,” Napolitano wrote in his own announcement. “We share the same mission, to help the enterprise effectively leverage modern IT to accelerate their business in the digital age.” While the two wait for the deal to close, it seems to be business as usual for Plexxi. Just earlier today, the company announced an expansion of its integrations with VMware.","Just days after Google announced that it would acquire Velostrata to help customers migrating more of their operations into cloud environments, HPE under its new CEO Antonio Neri is also upping its game in the same department. Today the company announced that…",2018-05-15T14:20:09Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",HPE buys Plexxi to expand its hybrid cloud solutions,http://techcrunch.com/2018/05/15/hpe-plexxi/,https://techcrunch.com/wp-content/uploads/2018/03/gettyimages-906499922.jpg?w=600,techcrunch,TechCrunch,cloud,1
Devin Coldewey,"Between Microsoft Build and Google I/O, there are probably more people saying “AI” this week than any previous week in history. But the AI those companies deploy tends to live off in a cloud somewhere — XNOR puts it on devices that may not even be capable of an internet connection. The startup has just pulled in $12 million to continue its pursuit of bringing AI to the edge. I wrote about the company when it spun off of Seattle-based, Paul Allen–backed AI2; its product is essentially a proprietary method of rendering machine learning models in terms of operations that can be performed quickly by nearly any processor. The speed, memory, and power savings are huge, enabling devices with bargain-bin CPUs to perform serious tasks like real-time object recognition and tracking that normally take serious processing chops to achieve. Since its debut it took $2.6 million in seed funding and has now filled up its A round, led by Madrona Venture Group, along with NGP Capital, Autotech Ventures and Catapult Ventures. “AI has done great,” co-founder Ali Farhadi told me, “but for it to become revolutionary it needs to scale beyond where it is right now.” The fundamental problem, he said, is that AI is too expensive — both in terms of processing time and in money required. Nearly all major “AI” products do their magic by means of huge banks of computers in the cloud. You send your image or voice snippet or whatever, it does the processing with a machine learning model hosted in some data center, then sends the results back. For a lot of stuff, that’s fine. It’s okay if Alexa responds in a second or two, or if your images get enhanced with metadata over a period of hours while you’re not paying attention. But if you need a result not just in a second, but in a hundredth of a second, there’s no time for the cloud. And increasingly, there’s no need. XNOR’s technique allows things like computer vision and voice recognition to be stored and run on devices with extremely limited processing power and RAM. And we’re talking Raspberry Pi Zero here, not just like an older iPhone. If you wanted to have a camera or smart home type device in every room of your home, monitoring for voices, responding to commands, sending its video feed in to watch for unauthorized visitors or emergency situations — that constant pipe to the cloud starts getting crowded real fast. Better not to send it at all. This has the pleasant byproduct of not requiring what might be personal data to some cloud server, where you have to trust that it won’t be stored or used against your will. If the data is processed entirely on the device, it’s never shared with third parties. That’s an increasingly attractive proposition. Developing a model for edge computing isn’t cheap, though. Although AI developers are multiplying, comparatively few are trying to run on resource-limited devices like old phones or cheap security cameras. XNOR’s model lets a developer or manufacturer plug in a few basic attributes and get a model pre-trained for their needs. Say you’re the cheap security camera maker; you need to recognize people and pets and fires, but not cars or boats or plants, you’re using such and such ARM core and camera, and you need to render at 5 frames per second but only have 128 MB of RAM to work with. Ding — here’s your model. Or say you’re a parking lot company and you need to recognize empty spots, license plates, and people lurking suspiciously. You’ve got such and such a setup. Ding — here’s your model. These AI agents can be dropped into various code bases fairly easily and never need to phone home or have their data audited or updated, they’ll just run like greased lightning on the platform. Farhadi told me they’ve established the most common use cases and devices through research and feedback, and many customers should be able to grab an “off the shelf” model just like that. That’s Phase 1, as he called it, and should be launching this fall. Phase 2 (in early 2019) will allow for more customization, so for example if your parking lot model becomes a police parking lot model and needs to recognize a specific set of cars and people, or you’re using proprietary hardware not on the list. New models will be able to be trained up on demand. And Phase 3 is taking models that normally run on cloud infrastructure and adapting and “XNORifying” them for edge deployment. No timeline on that one. Although the technology lends itself in some ways to the needs of self-driving cars, Farhadi told me that they aren’t going after that sector — yet. It’s still essentially in the prototype phase, he said, and creators of autonomous vehicles are currently trying to prove the idea works fundamentally, not trying to optimize and deliver it at lower cost. Edge-based AI models will surely be increasingly important as the efficiency of algorithms improves, the power of devices rises, and the demand for quick-turnaround applications grows. XNOR seems to be among the vanguard in this emerging area of the field, but you can almost certainly expect competition to expand along with the market.","Between Microsoft Build and Google I/O, there are probably more people saying “AI” this week than any previous week in history. But the AI those companies deploy tends to live off in a cloud somewhere — XNOR puts it on devices that may not even be capable of …",2018-05-08T15:02:11Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","XNOR raises $12M for its cloud-free, super-efficient AI",http://techcrunch.com/2018/05/08/xnor-raises-12m-for-its-cloud-free-super-efficient-ai/,https://techcrunch.com/wp-content/uploads/2017/01/ai-platforms1.png?w=711,techcrunch,TechCrunch,cloud,1
Ron Miller,"It’s almost becoming boring reporting that the Amazon cloud had a monster quarter. It’s not news at this point, because of course they did. Yesterday, it once again blew away analyst expectations with 49 percent revenue growth for the quarter. Oh ya, and that revenue? Well that was $5.44B for the quarter, a ways above the projected $5.26B. Ho hum. Another day in paradise for the Amazon cloud. That’s a $21.76 billion run rate for a business that is but one piece of Amazon’s vast empire. Amazon’s cloud arm, AWS, has been running away with infrastructure services marketshare for a long time. When you consider companies the likes of Microsoft, IBM, Google and Alibaba are chasing them, it makes their run even more remarkable. Conventional economic wisdom would suggest that the bigger you get, the harder it is to maintain big growth numbers, yet AWS has defied that wisdom and just keeps on growing, quarter after quarter after quarter. It says something about the way Amazon as whole operates. It never backs down and it never gives in. Meanwhile across the lake, Microsoft was also reporting its earnings yesterday as well, and the cloud numbers were also quite good with what Microsoft calls ‘The Intelligent Cloud’ up 17 percent and Azure earnings up an impressive 93 percent. That was compared to 15 percent growth for Intelligent Cloud and 98 percent Azure growth in the previous quarter. Microsoft clearly represents the best hope to give Amazon a run for its money and it’s doing its part to make that happen, but even with all of that growth, Amazon just keeps growing too– albeit at a smaller rate at this point, but certainly strong enough to maintain its hefty marketshare advantage. Canalys, a firm that tracks cloud marketshare numbers says that Amazon’s revenue is nearly double that of its nearest competitor, Microsoft, and far ahead of Google. While Google did not break out its cloud numbers in its earnings report this week, Canalys ranked it third, up 89% for the quarter to US$1.2 billion. In terms of how that translated into marketshare, AWS continues to own about third of the market, while Microsoft is around 15 percent and Google around 5 percent, according to Canalys’ latest numbers. That tracks fairly consistently with Synergy Research Group, another firm that tracks the market. It had Amazon at 33%, Microsoft at 13% with IBM 8%, Google 6% and Alibaba 4%. Synergy’s John Dinsdale says the growth we have seen the last two quarters has been quite remarkable. “Normal market development cycles and the law of large numbers should result in growth rates that slowly diminish – and that is what we saw in late 2016 and through most of 2017. But the growth rate jumped by three percentage points in Q4 and by another five in Q1,” Dinsdale said in a statement. Overall, while the cloud market continues to grow as companies shift more workloads there, the revenue numbers increase, but the marketshare percentages have held relatively steady. Amazon continues to control the vast majority of marketshare, and while there are others chasing them with deep pockets and large investments, it appears that none of these companies is threat to Amazon’s dominance for now.","It’s almost becoming boring reporting that the Amazon cloud had a monster quarter. It’s not news at this point, because of course they did. Yesterday, it once again blew away analyst expectations with 49 percent revenue growth for the quarter. Oh ya, and that…",2018-04-27T15:45:49Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Yawn: Amazon cloud business just keeps rolling along,http://techcrunch.com/2018/04/27/yawn-amazon-cloud-business-just-keeps-rolling-along/,https://techcrunch.com/wp-content/uploads/2018/04/gettyimages-769718385.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google today announced that Nvidia’s high-powered Tesla V100 GPUs are now available for workloads on both Compute Engine and Kubernetes Engine. For now, this is only a public beta, but for those who need Google’s full support for their GPU workloads, the company also today announced that the slightly less powerful Nvidia P100 GPUs are now out of beta and publicly available. The V100 GPUs remain the most powerful chips in Nvidia’s lineup for high-performance computing. They have been around for quite a while now and Google is actually a bit late to the game here. AWS and IBM already offer V100s to their customers and on Azure, they are currently in private preview. While Google stresses that it also uses NVLink, Nvidia’s fast interconnect for multi-GPU processing, it’s worth noting that its competitors do this, too. NVLink promises GPU-to-GPU bandwidth that’s nine times faster than traditional PCIe connections, resulting in a performance boost of up to 40 percent for some workloads, according to Google. All of that power comes at a price, of course. An hour of V100 usage costs $2.48, while the P100 will set you back $1.46 per hour (these are the standard prices, with the preemptible machines coming in at half that price). In addition, you’ll also need to pay Google to run your regular virtual machine or containers. V100 machines are now available in two configurations with either one or eight GPUs, with support for two or four attached GPUs coming in the future. The P100 machines come with one, two or four attached GPUs.","Google today announced that Nvidia’s high-powered Tesla V100 GPUs are now available for workloads on both Compute Engine and Kubernetes Engine. For now, this is only a public beta, but for those who need Google’s full support for their GPU workloads, the comp…",2018-04-30T16:01:16Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google brings Nvidia’s Tesla V100 GPUs to its cloud,http://techcrunch.com/2018/04/30/google-brings-nvidias-tesla-v100-gpus-to-its-cloud/,https://techcrunch.com/wp-content/uploads/2018/04/nvidia-telsa-v100.jpg?w=711,techcrunch,TechCrunch,cloud,1
Ron Miller,"Yesterday Oracle announced a new online transaction processing database service, finally bringing its key database technology into the cloud. The company, which has been around for over four decades made its mark selling databases to the biggest companies in the world, but as the world has changed, large enterprise customers have been moving increasingly to the cloud. These autonomous database products could mark Oracle’s best hope for cloud success. The database giant, which has a market cap of over $194 billion and over $67 billion in cash on hand certainly has options no matter what happens with its cloud products. Yet if the future of enterprise computing is in the cloud, the company needs to find some sustained success there, and what better way to lure its existing customers than with its bread and butter database products. Oracle has demonstrated a stronger commitment to the cloud in recent years after showing it much disdain for it. In fact, it announced it would be building 12 new regional data centers earlier this year alone, but it wasn’t always that way. Company founder and executive chairman Larry Ellison famously made fun of the cloud as “more fashion driven than women’s fashion.” Granted that was in 2008, but his company certainly came late to the party. A different kind of selling The cloud is not just a different way of delivering software, platform and infrastructure, it’s a different way of selling. While switching databases might not be an easy thing to do for most large companies, the cloud subscription payment model still offers a way out that licensing rarely did. As such, it requires more of a partnership between vendor and customer. After years of having a reputation of being aggressive with customers, it may be even harder for them to make this shift. Salesforce exec Keith Block (who was promoted to Co-CEO just yesterday ), worked at Oracle for 20 years before joining Salesforce in 2013. In an interview with TechCrunch in 2016, when asked specifically about the differences between Oracle and Salesforce, he contrasted the two company’s approaches and the challenges a company like Oracle, born and raised in the open prem world, faces as it shifts to the cloud. It takes more than a change in platform, he said. “You also have to have the right business model and when you think about our business model, it is a ‘shared success model’. Basically, as you adopt the technology, it’s married to our payment schemes. So that’s very, very important because if the customer doesn’t win, we don’t win,” Block said at the time. John Dinsdale, chief analyst and managing director at Synergy Research, a firm that keeps close watch on the cloud market, agrees that companies born on-prem face adjustments when moving to the cloud. “In order to survive and thrive in today’s cloud-oriented environment, any software company that grew up in the on-prem world needs to have powerful, cost-effective products that can be packaged and delivered flexibly – irrespective of whether that is via the cloud or via some form of enhanced on-prem solution,” he said. Database as a Service or bust All that said, if Oracle could adjust, it has the advantage of having a foothold inside the enterprise. It also claims a painless transition from on-prem Oracle database to its database cloud service, which if a company is considering moving to the cloud could be attractive. There is also the autonomous aspect of its cloud database offerings, which promises to be self-tuning, self-healing with automated maintenance and updates and very little downtime. Carl Olofson, an analyst with IDC who covers the database market sees Oracle’s database service offerings as critical to its cloud aspirations, but expects business could move slowly here. “Certainly, this development (Oracle’s database offerings) looms large for those whose core systems run on Oracle Database, but there are other factors to consider, including any planned or active investment in SaaS on other cloud platforms, the overall future database strategy, the complexity of moving operations from the datacenter to the cloud, and so on. So, I expect actual movement here to be gradual.” he said. Adam Ronthal, an analyst at Gartner sees the database service offerings as Oracle’s best chance for cloud success. “The Autonomous Data Warehouse and the Autonomous Transaction Processing offerings are really the first true cloud offerings from Oracle. They are designed and architected for cloud, and priced competitively. They are strategic and it is very important for Oracle to demonstrate success and value with these offerings as they build credibility and momentum for their cloud offerings,” he said. The big question is can Oracle deliver in a cloud context using a more collaborative sales model, which is still not clear. While it showed some early success as it has transitioned to the cloud, it’s always easier easier to move from a small market share number to a bigger one, and the numbers (when they have given them ) have flipped in the wrong direction in recent earnings reports. As the stakes grow ever higher, Oracle is betting on what it’s known best all along, the databases that made the company. We’ll have to wait and see if that bet pays off or if Oracle’s days of database dominance are numbered as business looks to public cloud alternatives.","Yesterday Oracle announced a new online transaction processing database service, finally bringing its key database technology into the cloud. The company, which has been around for over four decades made its mark selling databases to the biggest companies in …",2018-08-08T15:32:09Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Oracle’s database service offerings could be its last best hope for cloud success,http://techcrunch.com/2018/08/08/oracles-database-service-offerings-could-be-its-last-best-hope-for-cloud-success/,https://techcrunch.com/wp-content/uploads/2018/08/GettyImages-170754365.jpg?w=599,techcrunch,TechCrunch,cloud,1
Ron Miller,"Oracle announced yesterday that it intends to acquire Zenedge, a 4-year old hybrid security startup. They didn’t reveal a purchase price. With Zenedge, Oracle gets a security service to add it to its growing cloud play. In this case, the company has products to protect customers whether in the cloud, on-prem or across hybrid environments. The company offers a range of services from web application firewalls to distributed denial of service (DDoS) attack mitigation, bot management, API management and malware prevention. In addition, they operate a Security Operations Center (SOC) to help customers monitor their infrastructure against attack. Their software and the SOC help keep watch on over 800,000 websites and networks across the world, according to information supplied by Oracle. Oracle says it will continue to build out Zenedge’s product offerings. “Oracle plans to continue investing in Zenedge and Oracle’s cloud infrastructure services. We expect this will include more functionality and capabilities at a quicker pace,” Oracle wrote in an FAQ on the deal (.pdf) published on their website. Oracle’s recent acquisition history. Source: Crunchbase Just this week Oracle announced that it was expanding its automation capabilities on its Platform as a Service offerings from databases to a range of areas including security. Ray Wang, founder and principal analyst at Constellation Research says the company is a good match as it also uses automation and artificial intelligence in its solution. “Oracle is beefing up its security offerings in the cloud. They have one of the strongest cyber security platforms,” Wang told TechCrunch. “They also have a ton of automation that fits Oracle’s theme of autonomous,” he added. Oracle is far behind cloud rivals as it came late to the game. Just this week, the company announced plans to build a dozen data centers around the world over the next two years. They are combining an aggressive acquisition strategy and rapid data center expansion in an effort to catch up with competitors like AWS, Microsoft and Google. Zenedge launched in 2014 and has raised $13.7 million, a modest amount for a cloud-based security service. Oracle says customers and partners can continue to deal with Zenedge using their existing contacts. Featured Image: Justin Sullivan/Getty Images","Oracle announced yesterday that it intends to acquire Zenedge, a 4-year old hybrid security startup. They didn’t reveal a purchase price. With Zenedge, Oracle gets a security service to add it to its growing cloud play. In this case, the company has products …",2018-02-16T19:19:29Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Oracle grabs Zenedge as it continues to beef up its cloud security play,http://techcrunch.com/2018/02/16/oracle-grabs-zenedge-as-it-continues-to-beef-up-its-cloud-security-play/,https://tctechcrunch2011.files.wordpress.com/2018/02/gettyimages-460559814.jpg,techcrunch,TechCrunch,cloud,1
Ron Miller,"There are now two Dianes running the show at Google Cloud. The company announced that Diane Bryant has been hired as the COO of the division. She joins Diane Greene, who came on board as Senior VP of Google Cloud in November 2015. Greene appeared to be excited about the prospect of her joining the team. “I can’t think of a person with more relevant experience and talents. She is an engineer with tremendous business focus and an outstanding thirty-year career in technology,” Greene wrote in a company blog post announcing the appointment. Google would not comment further beyond the blog post on the nature of her role or if Bryant would be reporting to Greene or vice versa. Bryant, who is also on the Board of United Technologies, most recently ran Intel’s Data Center Group, which itself generated a whopping $17 billion in revenue last year. That experience with data center technology and her great success in generating big bucks at Intel could be one of the reasons Google Cloud brought her in. The company is working to build its cloud business, which is languishing in fourth place behind IBM and Microsoft and well behind market leader AWS. In fact, two years after Greene joined the company to boost its enterprise credentials, it still has just single digit market share, according to data generated by Synergy Research. While Google Cloud experienced huge growth in the 75 percent range in Q3 2017. So far, that hasn’t translated into much market share movement with AWS continuing to maintain its massive lead and Microsoft also growing more quickly. Featured Image: Getty Images","There are now two Dianes running the show at Google Cloud. The company announced that Diane Bryant has been hired as the COO of the division. She joins Diane Greene, who came on board as Senior VP of Google Cloud in November 2015. Greene appeared to be excite…",2017-11-30T19:17:01Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud brings in former Intel exec Diane Bryant as COO,https://techcrunch.com/2017/11/30/google-cloud-brings-in-former-intel-exec-diane-bryant-as-coo/,https://tctechcrunch2011.files.wordpress.com/2017/11/gettyimages-607644238.jpg,techcrunch,TechCrunch,cloud,1
Brendan Hesse,"Rumors of Googles entry into the game-streaming space have circulated for ages, but today those rumors were finally confirmed when the company announced it will be teaming up with game publishers for its newly announced Project Stream service. Project Stream will allow users to play fully-fledged console and PC games through their Chrome browser without the need for a gaming PC or console hardware, all thanks to the cloud. The first of these partnerships is with French publisher Ubisoft and its upcoming title, Assassins Creed Odyssey. While Google says the full, public version of Project Stream wont be released for some time, those interested in helping Google test the service can sign up for its upcoming closed beta. Selected beta users will be able to play Assassins Creed Odyssey via Chrome, provided they meet all the technical requirements. How to sign up for the Project Stream Beta Heres what youll need in order to be selected for the beta: An Ubisoft Account. You can sign up for one here. A Google Account (not counting some managed Google accounts, such Google for Work or Google for Education) Chrome, updated to version 69 at minimum. (Why arent you always running the latest version of Chrome, anyway?) An internet connection with at least 25mbps download speeds. (You can test your speeds here.) Either a USB gaming controller or a mousenot necessary, but Google recommends these over using your laptops touchpad. You wont have to own a copy of Assassins Creed Odyssey in order to participate in the Project Stream betaassuming you get in. According to Google, beta test spots are limited, and Google may limit how many users can access the service at any given time. And thats not all. Once the beta ends around mid-January or so, you wont be able to access the game any more. Should you get in, youll also be playing a more limited version of the game compared to those who went out and purchased it for real: Dont expect to be able to make microtransactions for in-game items, for example (which isnt the worst problem to have). As for the games quality, early footage reveals that Googles service can achieve good-looking 1080p gameplay at 60fps. Whether users will be able to push past that into higher resolutions or frame rates, well let you know if (or when) we get into the beta.","Rumors of Google’s entry into the game-streaming space have circulated for ages, but today those rumors were finally confirmed when the company announced it will be teaming up with game publishers for its newly announced Project Stream service. Project Stream…",2018-10-01T20:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",Play 'Assassin's Creed Odyssey' for Free by Beta Testing Google's Cloud Streaming Service,https://lifehacker.com/play-assassins-creed-odyssey-for-free-by-beta-testing-g-1829446196,"https://i.kinja-img.com/gawker-media/image/upload/s---Vbrr8LT--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/hopxm74fxts7qdvymco5.jpg",,Lifehacker.com,cloud,1
Ron Miller,"It has long been believed that the big three in the cloud consisted of AWS, Microsoft and Google with IBM not doing too badly either, but in its earnings call with analysts today, the company revealed it’s pulling in a billion dollars a quarter in combined cloud revenue. That’s a figure that Google’s Diane Greene says already puts her company on elite footing, but which is substantially below what competitors have been reporting. “We are saying we crossed a billion a quarter in 2017 and according to publicly available numbers, we are the fastest growing cloud. If you step back and think about someone offering services and that’s revenue, that’s pretty darn impressive. Not too many companies can make a claim like that. It already puts you in the elite of companies,” Greene told TechCrunch It’s worth noting that in Q4, Canalys reported that Microsoft had grown the fastest with 98 percent growth with Google second at 85 percent growth, still quite brisk, but not the fastest. While Greene wouldn’t share specific data on how they came up with their number, she did say the company compared a range of publicly available data with their own internal numbers to come up with the “fastest growing cloud” label. That may be so, but it’s hard to ignore that a $4 billion run rate is not even equal to a quarter of revenue for any Google’s main cloud competitors. While it’s hard to do a pure comparison of cloud revenue because there is no standard way of measuring it, we do know that Amazon reported AWS revenue today of $4.331 billion. Meanwhile Microsoft passed a $20 billion total cloud run rate last year and IBM reported revenue of $17 billion for the year in its most recent earnings report, which breaks down to over $4.25 billion a quarter. For her part, Greene sees progress. Besides the the rapid growth she cited, Google Cloud has passed major milestones like 4 million customers paying for G Suite and it has tripled the number of sales of million dollars or more since 2016. All progress she says that points to a company that’s growing more quickly than the billion dollar revenue number would suggest in isolation. Google has spent $30 billion in infrastructure investments over the last three years to build its data center presence around the world. It has also made a concerted effort to be a more developer-friendly cloud vendor and has contributed key software like Kubernetes to open source, a technology that surged in popularity in 2017. Greene says this has removed a lot of market obstacles for Google, but that it takes some time for revenue to catch up with customers. “Think about how the cloud works and they start moving over and the revenue takes awhile to start come in,” she explained. She cited a long list of big-name customers who have come on board during her tenure like enterprise technology players Salesforce, Cisco and SAP to a range of other industries including Disney, Rolls Royce and PayPal. It’s also worth pointing out that customers don’t typically choose a single cloud vendor, which means that each one can share the same customer. This is not necessarily a zero sum game for any of these vendors. Yet its reported results of a billion dollars in revenue per quarter is far less than its competitors. That could change over time of course if the company can continue to grow market share, but it’s a lot lower than any reasonable market observer would have expected from a major cloud player.","It has long been believed that the big three in the cloud consisted of AWS, Microsoft and Google with IBM not doing too badly either, but in its earnings call with analysts today, the company revealed it’s pulling in a billion dollars a quarter in combined cl…",2018-02-02T01:21:45Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google’s Diane Greene says billion dollar cloud revenue already puts them in elite company,http://techcrunch.com/2018/02/01/googles-diane-greene-says-billion-dollar-cloud-revenue-already-puts-them-in-elite-company/,https://tctechcrunch2011.files.wordpress.com/2016/09/disrupt_sf16_diane_greene-3752.jpg,techcrunch,TechCrunch,cloud,1
Kate Clark,"Talkdesk, the provider of cloud-based contact center software, has raised $100 million in new funding from Viking Global Investors, a Connecticut-based hedge fund, and existing investor DFJ. The round values the company at north of $1 billion, Talkdesk co-founder and chief executive officer Tiago Paiva confirmed to TechCrunch, but he declined to disclose the exact figure. The company, which uses artificial intelligence and machine learning to improve customer service, targets mid-market and enterprise businesses, counting IBM, Dropbox, Stitch Fix and Farfetch as customers. “Imagine a company has a million customers and they want to reach out for support, what Talkdesk does is allow the customer to connect with a company in the best way possible,” Paiva told TechCrunch. “If you call into Farfetch, they will be using Talkdesk so they can see what products you’ve bought, what your tastes are, what you’ve complained about before. It gives them the history of everything so they can take care of your problem faster.” Founded in Portugal in 2011, Talkdesk has offices in San Francisco and Lisbon. With the latest investment, it plans to expand to the U.K., as well as double down on its investment in AI. The company has previously raised about $24 million in equity funding, including a $15 million round in mid-2015. It also was a Startup Battlefield contestant at TechCrunch Disrupt NY in 2012. Todays digital-first customers expect immediate and personalized answers, yet the majority of companies have not yet adopted a flexible, cloud-native platform to enable this level of agility and service, DFJ partner Josh Stein said in a statement. “We believe that 2019 will be the year that cloud-based contact centers become the rule, not the exception.”","Talkdesk, the provider of cloud-based contact center software, has raised $100 million in new funding from Viking Global Investors and DFJ.",2018-10-03T17:05:00Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Talkdesk nabs $100M at more than $1B valuation for its smart call centers,http://techcrunch.com/2018/10/03/talkdesk-nabs-100m-at-more-than-1b-valuation-for-its-smart-call-centers/,https://techcrunch.com/wp-content/uploads/2018/10/GettyImages-507833343.jpg?w=600,techcrunch,TechCrunch,cloud,1
Jon Fingas,"The subscription will reportedly make its debut later in August, although it'll be limited to American gamers. There are no guarantees All Access will come to fruition, although job openings have hinted that Microsoft might unify Game Pass and Live Gold in the future. However, it'd be consistent with Microsoft's overall strategy of pushing cloud services wherever possible. An all-in-one subscription would make you to commit to Game Pass and Live Gold for longer than you might otherwise. The company might also be laying the groundwork for a cloud-only Xbox console where a subscription like All Access would be virtually necessary to enjoy the system as intended.","Microsoft's vision for the Xbox as a service might become clearer in the very near future. Windows Central has heard that Microsoft is prepping a Xbox All Access subscription that would bundle an Xbox One console, Xbox Live Gold and an Xbox Game Pass for a si…",2018-08-22T22:49:00Z,"{'id': 'engadget', 'name': 'Engadget'}","'All Access' Xbox subscription could bundle games, Live and a console",https://www.engadget.com/2018/08/22/xbox-all-access-rumor/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D1600%252C972%252C0%252C0%26quality%3D85%26format%3Djpg%26resize%3D1600%252C972%26image_uri%3Dhttp%253A%252F%252Fo.aolcdn.com%252Fhss%252Fstorage%252Fmidas%252F3fe250587307ea4af14197bccbc18c31%252F205826069%252FXbox%252BOne%252BX%252Breview%252Bgallery%252B3.jpg%26client%3Da1acac3e1b3290917d92%26signature%3Dbcb05a1c35f8636d7efad192e42a0b642e553b7a&client=amp-blogside-v2&signature=2afba4fb7703e1a4622200a9e49bf5d4156c7d96,engadget,Engadget,cloud,1
Tom Warren,"Microsoft is officially unveiling the name for its next major Windows 10 update today. Previously codenamed Redstone 5, the “ Windows 10 October 2018 Update ” will arrive at some point in October. It will include a number of new features for devices, like a new cloud clipboard that syncs across machines, a dark File Explorer, an updated snipping tool, improvements to Microsoft Edge, and performance information in the Xbox Game Bar. Microsoft’s naming follows the Windows 10 April 2018 Update that was released earlier this year. Microsoft is expected to conclude development of the October update by the end of September, and it should be available to Windows Insiders by early October followed by regular consumers. Microsoft is now focusing on its next Windows 10 update, codenamed 19H1. This update will likely arrive in April 2019, and the company has not yet revealed which major features will be included. Testing for 19H1 begun in late July, and it’s possible we might see the return of the Sets feature that won’t be included in the Windows 10 October 2018 Update.","Microsoft is releasing its next major Windows 10 update in October, and it will include features like cloud sync, a new snipping tool, and improvements to the Microsoft Edge browser",2018-08-31T15:14:08Z,"{'id': 'the-verge', 'name': 'The Verge'}",Windows 10’s next major update arrives in October,https://www.theverge.com/2018/8/31/17805372/microsoft-windows-10-october-2018-update,https://cdn.vox-cdn.com/thumbor/l-AKD4uXuRUTOCgZ4dqk37yzjvs=/0x147:2040x1215/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/3919716/mswindows2_2040.0.jpg,the-verge,The Verge,cloud,1
Mike Epstein,"Skype announced this week that it has added a cloud-based recording feature to its video chat service, which you can now access on almost any version of Skype save for the Windows 10 version. It’ll gain the call-recording feature “in the coming weeks,” according to the Skype blog. To record a call, simply press the plus symbol (+) in the bottom-left corner of a Skype call select “Start recording.” Everyone in the call will get a warning that you flipped on the recording feature, which will capture audio as well as any videos (or shared screens) during the call. When you’re done recording, either hang up or go back to the same place and select the “Stop recording” option. I’m a journalist. (Kind of.) I have to record calls a lot, to supplement my notes. When I make a… Read more Read Once your recording ends, Skype sends the recording to everyone on the call to view, download, or share directly with other Skype users. It also stores the recording in the cloud for 30 days. The recording looks a little different than the call itself, as Skype strips away a lot of the UI and places the shots side by side or in formation for a more pleasant viewing experience. To save your new recording on a desktop or laptop, click the ellipses next to the recording and select “save as.” You’ll also see an option to “forward” the recording, which is Skype’s built-in sharing feature. To download it on iOS or Android, touch and hold the recording and select “save.” As before, the same menu will also give you the option to forward the recording to other Skype users. As someone who generally uses Skype for recording interviews, having a recorder built into Skype makes the service feel more like the complete package. Previously, you needed a third-party app to record calls in Skype, and some of these could get finicky—especially when dealing with video. Whether you’re just having fun, taking notes, or recording a podcast (because who wants to talk to someone without seeing them?), Skype’s new recording features couldn’t have arrived fast enough.","Skype announced this week that it has added a cloud-based recording feature to its video chat service, which you can now access on almost any version of Skype save for the Windows 10 version. It’ll gain the call-recording feature “in the coming weeks,” accord…",2018-09-05T19:45:00Z,"{'id': None, 'name': 'Lifehacker.com'}",How to Record Calls on Skype,https://lifehacker.com/how-to-record-calls-on-skype-1828832338,"https://i.kinja-img.com/gawker-media/image/upload/s--C2RhsTQz--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/fx3ayq9q4fii9vcr4qpk.png",,Lifehacker.com,cloud,1
Matt Simon,"As the glam metal band Cinderella once said, you don’t know what you’ve got till it’s gone. I’m relatively certain they weren’t talking about snow, but let’s pretend they were anyway: Global warming threatens to wreak havoc on snowpack. The American west in particular already has a snowpack problem, which means less water for drinking and powering hydroelectric plants. Unfortunately you can’t just force snow to fall out of the air. Or can you? For over half a century now, scientists have been toying with the idea of cloud seeding for snow—that is, inducing clouds to spit out more moisture. Think Olympics that never run low on snow (in PyeongChang, almost every bit of snow on the ground was made by machines ), and snowpacks that are forever … packed. Problem is, for all these decades, it’s been tough to prove that cloud seeding is actually working. So, let’s talk about how snow forms in the first place. In a cloud you’ve got a whole mess of gas and water molecules running around bumping into each other. “Getting them to actually start to freeze, they have to bump into each other in the right way to lock into an ice lattice,” says Andrew Gettelman, who studies cloud microphysics at the University Corporation for Atmospheric Research. “That's much easier if there's a solid substrate.” A closeup of the generator. But clouds aren’t known for being very solid. However, they do contain particles like dust, which forms a sort of foundation on which ice can grow, ice that then falls to the ground as snow. Cloud seeders can theoretically hijack this process by tricking the ice to form around a compound called silver iodide. They go about this one of two ways: by flying through a cloud and spraying the stuff, or by using ground-based generators to burn a silver iodide solution that rises into the clouds on a raft of hot air. “It turns out that silver iodide has a molecular structure that's very similar to ice,” says University of Wyoming atmospheric scientist Jeff French, who studies cloud seeding. “What that means is that if a liquid water droplet were to capture a silver iodide particle, or were to perhaps grow initially on a silver iodide particle as a liquid drop, it would be able to freeze more readily at higher temperatures than it would naturally.” What’s important to understand here is that cloud seeding doesn’t work by creating moisture, but by encouraging it to form into ice. So if cloud seeding works, the results wouldn’t be dramatic: You can only coax so much moisture out of clouds, and this certainly isn’t about creating new clouds out of nothing. And that only makes it more difficult to prove that the process produces extra snow. The problem is that as we all know, weather is finicky. You can seed clouds to your heart’s content and see maybe a 10 percent bump in snowfall over the course of 5 years, but it’s tough to prove that it wasn’t a natural fluctuation in weather that did it. What French and his colleagues have been able to do, though, is confirm the hypothesis of how silver iodide would work to form snow, thanks to some fancy technology. They got a plane loaded with sensors—radar and lidar and all kinds of good stuff. As they flew through a seeded cloud, particles passed through a laser beam. This allowed the researchers to capture a 2-D image of the particle and determine if it was liquid or ice. It also allowed them to quantify the concentration of particles over a certain distance, say 50 or 100 meters, in parts of a cloud that had been seeded versus parts of a cloud that hadn't. These flares are ignited as a plane flies through a snowstorm, releasing silver iodide particles into the clouds. “Under certain conditions, we have demonstrated unambiguously that the chain of events that are hypothesized to occur when you add silver iodide to a cloud do indeed occur,” says French. That is, silver iodide seems to facilitate the freezing of liquid into ice particles. “We can quantify how many particles were being produced, how fast those particles grew, etc.,” he adds. “Now, does that mean that over the course of a year you can produce 15 percent more snow? We're a long way from being able to answer that question.” The problem is that it’s tough to run a controlled experiment with cloud seeding. “That gives it a little bit of a bad reputation in the field,” says Gettelman, “because it's hard to do a really good statistical study that shows you can make it snow more than if you didn’t seed the cloud. How do you know what would have happened if you didn't do it?” Still, that hasn’t stopped a lot of folks, particularly in the western US, from seeding anyway. Take Idaho Power, which runs 17 hydroelectric projects. Idaho Power is in the business of water, so in the early ‘90s, amid a multiyear drought, a shareholder suggested the utility explore cloud seeding to boost snowpack. The utility’s program went fully operational in 2005, and today consists of 55 ground-based generators in southern Idaho, as well as three planes. Idaho Power says it’s seen an annual increase in snowpack of 12 percent in one of its regions (other regions tally somewhere between that and 5 percent). “Everything that we've done points to a benefit,” says atmospheric scientist Derek Blestrud of Idaho Power. “We've never had something that points the other way.” Even if cloud seeding is effective, is this something we should be mucking with? “If we find out that it works, that doesn't mean we should do it,” says Nicholas Anderson, an associate program director at the National Science Foundation, which funded some of French’s seeding research. “It means we should understand what happens downstream of it too.” If seeding helps coax moisture out of a cloud and dump it in one region, that may mean another region down the road loses out on snow. But we’re also dealing with relatively small amounts of moisture here. “When you step back and you look at the scales we're talking about,” says French, “in best case scenarios that might actually fall out as a result of cloud seeding, I think you're having a very tiny impact on the overall water balance in the atmosphere.” Wouldn’t want to upset the neighbors, after all. Engineering the Earth What happens when we can't keep the Earth from heating up? Some say the solution is geoengineering —intentionally altering the atmosphere. It's a relatively unresearched idea, but representatives in the US government are making moves to support studies. That research will be essential before we try any of these ideas, because the side effects could be dire.","You can theoretically \""seed\"" snow in the atmosphere, but it's really hard to tell if it actually works.",2018-02-19T17:00:00Z,"{'id': 'wired', 'name': 'Wired'}",Could Scientists Use Silver Iodide to Make Snow for the Olympics?,https://www.wired.com/story/could-scientists-use-silver-iodide-to-make-snow-for-the-olympics/,https://media.wired.com/photos/5a876a26915fc539b509aa20/191:100/pass/snowseeding.jpg,wired,Wired,cloud,1
Sidney Fussell,"Google released a new AI tool on Wednesday designed to let anyone train its machine learning systems on a photo dataset of their choosing. The software is called Cloud AutoML Vision. In an accompanying blog post, the chief scientist of Google’s Cloud AI division explains how the software can help users without machine learning backgrounds harness artificial intelligence. All hype aside, training the AI does appear to be surprisingly simple. First, you’ll need a ton of tagged images. The minimum is 20, but the software supports up to 10,000. Using a meteorologist as an example for their promotional video was an apt choice by Google—not many people have thousands of tagged HD images bundled together and ready to upload. (Anime fans excluded, of course.) A lot of image recognition is about identifying patterns. Once Google’s AI thinks it has a good understanding of what links together the images you’ve uploaded, it can be used to look for that pattern in new uploads, spitting out a number for how well it thinks the new images match it. So our meteorologist would eventually be able to upload images as the weather changes, identifying clouds while continuing to train and improve the software. Being able to recognize patterns at enormous scales has immense interdisciplinary value. Oncologists have trained machine learning systems on images of breast cancer cells so they can spot the disease earlier. Neuroscientists have used algorithms on MRI scans to predict language development in children. And Stanford researchers have applied similar software to predict race and voting patterns in cities by matching census data to the frequency of specific brands of cars. Hopefully AutoML Vision will energize more projects like these. Because while early detection is potentially life-saving, this AI could also unearth new, as of yet unproven patterns and correlations. In the hands of citizen scientists or investigative journalists, this could be transformative. With AutoML Vision, the barrier to entry is primarily data collection—that is, capturing and correctly tagging thousands of images for training. There’s more ways to capture images than ever (via drones, cell phones, live feeds, or social media), but the means of capturing data is far from democratized. Hidden in the usual marketing speak of Google’s blog post, there’s a clear understanding that democratizing the technology could, eventually, reverberate through a number of fields. [ TechCrunch ]","Google released a new AI tool on Wednesday designed to let anyone train its machine learning systems on a photo dataset of their choosing. The software is called Cloud AutoML Vision. In an accompanying blog post, the chief scientist of Google’s Cloud AI divis…",2018-01-17T20:15:00Z,"{'id': None, 'name': 'Gizmodo.com'}",Google Has Made It Simple for Anyone Tap Into Its Image Recognition AI,https://gizmodo.com/google-has-made-it-simple-for-anyone-tap-into-its-image-1822161131,"https://i.kinja-img.com/gawker-media/image/upload/s--19BhC_o4--/c_fill,fl_progressive,g_center,h_450,q_80,w_800/uuscxid9fobckrwjepfk.png",,Gizmodo.com,cloud,1
Ryan F. Mandelbaum,"Our Milky Way galaxy isn’t alone in this corner of space—it’s orbited by a few smaller dwarf galaxies, including the Large Magellanic Cloud. Inside that cloud is 30 Doradus (or the Tarantula Nebula), a “starburst” where stars are formed at a much higher rate than the surrounding area. And 30 Doradus has too many massive stars. Scientists use a mathematical equation called the stellar initial mass function to determine how different mass stars are distributed in a given starry place. A lot of thought goes into whether star masses are generally distributed the same way regardless of where you are in the universe, or if the amount of heavy stars can vary based on the local environment. Scientists have thought that areas with more star formation, like 30 Doradus, might have a higher percentage of heavy stars—but haven’t been able to check for sure. An international team of researchers have now taken new measurements of 800 stars in 30 Doradus, and they published the results in a new paper published in Science. They used an experiment called the “Fibre Large Array Multi Element Spectrograph” on the Very Large Telescope in Chile to observe these stars’ light spectra, or the colors of light they give off. They could robustly ascertain 452 of these single stars’ temperatures, rotational velocities, surface gravity, and brightness. And yeah, they learned some stuff. Of those 452 stars, 247 of them weighed over 15 times the mass of our own sun. And they found 32 percent more stars above 30 solar masses and 73 percent more stars above 60 solar masses than well-accepted models of stellar mass distribution predict. They found evidence of stars bigger than a previously suggested maximum stellar birth mass of 150 solar masses. There’s still a lot of uncertainty in the data, the authors admit—which leaves the question of just how weird 30 Doradus is. “If there were more areas like 30 Doradus in the early universe, then the early universe could have contained many more big stars like this than our canonical model says,” Mark Krumholz at Australian National University in Canberra told New Scientist. That would have other implications: more heavy stars in the universe could mean more black holes and more supernovae, which would lead to a higher rate of black hole collisions and gravitational waves as well as more heavier metals in the universe, for example. So yeah, it looks like the universe could be stocked with some pretty hefty balls of gas. [ Science via New Scientist ]","Our Milky Way galaxy isn’t alone in this corner of space—it’s orbited by a few smaller dwarf galaxies, including the Large Magellanic Cloud. Inside that cloud is 30 Doradus (or the Tarantula Nebula), a “starburst” where stars are formed at a much higher rate …",2018-01-05T15:15:00Z,"{'id': None, 'name': 'Gizmodo.com'}",Nearby Starburst Surprises Scientists By Having Way Too Many Massive Stars,https://gizmodo.com/nearby-starburst-surprises-scientists-by-having-way-too-1821789200,"https://i.kinja-img.com/gawker-media/image/upload/s--aAEyBMCx--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/fkatexiserqs0ideup5z.jpg",,Gizmodo.com,cloud,1
Alex Cranz,"The France-based cloud PC service Shadow is finally making its way to America, and with it comes the choice to have a cloud-based gaming-level Windows machine on any computer you own—from your aging Windows desktop to your iPad to your Chromebook. The cost is pricey, but the promise is enticing. Shadow originally launched last year in France, where its parent company, the equally terribly named Blade, housed its first server farm. By the end of 2017, Shadow claimed to have over 11,000 subscribers in France, with over 53 percent of those subscribers claiming to use the service as a desktop replacement. Those numbers are small but impressive for a French startup, which, according to Venture Beat, secured $86 million in venture capital last year and opened a new office in Palo Alto. It suggests there’s a market there for a costly Windows 10 streaming service, even in a country with a fifth the population of the US. Their service itself is relatively simple, and in the carefully controlled demos I witnessed, it seemed to work remarkably well. You pay your $35 a month (when you subscribe for 12 months; three months is $40 a month, and month-by-month is an insane $50) and get instant access to your very own Windows 10 machine hosted on Blade’s server. While Blade wouldn’t detail the specific guts, a spokesperson said it would be a Windows 10 machine with a 4 core, 8 thread CPU, 12GB of RAM, and an 8.2 TFLOP video card with 16GB of VRAM. That suggests something like an Intel Core i5 or i7 processor and a Nvidia 1070 graphics card. Once you’re subscribed you use an app to access your PC. The app is available on Chrome OS, Android, MacOS, Windows, and Sony and Samsung smart TVs (iOS should also be available, but Blade is waiting on approval from the App Store). And, at least in the demo, you use the exact same PC, regardless of the device used to access it. The Blade spokesperson launched Rise of the Tomb Raider from Windows 10 on one device and the switched to another device, the game picking up exactly where he left off on the last device. He claimed that games will play back at up to 144fps at 1080p and 60fps on 4K—and require a minimum 15Mbps download speed—which is nothing short of incredible for cloud-based PC gaming. By comparison, Nvidia’s GeForce service, which is currently available on Nvidia Shield set-top boxes and in beta on MacOS, goes for $7.99 a month and requires 25Mbps. GeForce has the equivalent of a Nvidia 1080 at the core of its cloud-based PCs, so it should, theoretically, match Shadow for streaming quality. But it’s also limited to specific games (there are a little over 100 now), and there’s now big Windows machine at its core. You couldn’t edit a video in Adobe Premiere or blaze through spreadsheets in Microsoft Excel. Blade has also considered the UI, especially the touch-based UI that it wants to be unified across platforms—meaning interacting on an Android phone should feel the same as a Window or Apple tablet or even a Chromebook. And if you have a MacBook Pro with Touch Bar, Shadow has promised Touch Bar integration. If this all sounds too amazing, if pricey, to be true, then know Blade is aware of the potential problems, particularly maintain quality as the company grows across a nation significantly larger than France and working with an internet infrastructure that isn’t nearly as good. Blade president and co-founder Asher Kagan told Gizmodo, “We actually stopped sales because we didn’t want to degrade the quality to users [in France].” The company halted subscriptions when demand began to outpace supply because it didn’t want to force users to share resources and potentially diminish the quality of individual streams. Which also explains why rollout will be slow going in the US. It will launch in California only in mid-February with nationwide coverage planned for this summer. Kagan isn’t worried about the US’s notoriously shoddy internet, though, citing Shadow needing just 15Mbps to operate. He even boasted to seeing it work with speeds as low as 5Mbps—although he was quick to point out that wasn’t a typical result. Whether Shadow will be as good as Kagan promises remains to be seen, but if you’re willing to shell out the cash, you can get in the subscription queue now if you’re based in California. And if you, somehow, don’t have a single computing device already at your disposal, then Blade can rent you a Shadow Box for $10 a month or sell you one for a flat $140. The Box supports both 4K resolution and 144Hz refresh rate, and Freund claims more than 50 percent of French subscribers currently use the device. Personally, I’m curious to see how it works on a cheap Chromebook. Having top-tier games playing on a $500 laptop is a dang appealing offer. We’ll be able to provide more details, and a hopefully interact with the service in a far less controlled environment, next month when it becomes available in the US.","The France-based cloud PC service Shadow is finally making its way to America, and with it comes the choice to have a cloud-based gaming-level Windows machine on any computer you own—from your aging Windows desktop to your iPad to your Chromebook. The cost is…",2018-01-04T14:00:00Z,"{'id': None, 'name': 'Gizmodo.com'}",You Can Soon Use Windows 10 on Any Computer or Phone You Own—But It’ll Cost You,https://gizmodo.com/you-can-soon-use-windows-10-on-any-computer-or-phone-yo-1821607850,"https://i.kinja-img.com/gawker-media/image/upload/s--MNNq3LoU--/c_fill,fl_progressive,g_center,h_450,q_80,w_800/ftbs8eq0yz6skjhiwixp.jpg",,Gizmodo.com,cloud,1
Kris Holt,"Meanwhile, we're about to turn the calendar page into a new month, which means Sony is set to refresh the free games lineup. From February 5th, you can grab a couple of significant PS4 hitters in the form of action brawler For Honor and Hitman: The Complete First Season, the well-regarded reboot of the long-running assassin series.
It's also the last month Sony will offer free games for PS3 and Vita through PS Plus. The PS3 lineup is getting a notable swansong with Metal Gear Solid 4: Guns of the Patriots. You can also snag Divekickfor both platforms, along with Vita games Gunhouse and Rogue Aces (both are also playable on PS4). Since it's the last opportunity for free PS3 and Vita games, Sony's extending the availability window for them by a few days to March 8th. You'll have until March 5th to claim For Honor and Hitman.","One of the major selling points of PlayStation Plus (beyond ""free"" games and online play) is it offers cloud backups for your game saves and profiles. From early next month, members will have vastly more room to play with, as Sony's boosting the storage capac…",2019-01-30T18:49:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Sony bumps up PlayStation Plus cloud storage limit to 100 GB,https://www.engadget.com/2019/01/30/ps-plus-free-games-february-cloud-storage/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D755%252C425%252C0%252C0%26quality%3D85%26format%3Djpg%26resize%3D1600%252C901%26image_uri%3Dhttps%253A%252F%252Fs.yimg.com%252Fos%252Fcreatr-uploaded-images%252F2019-01%252F5aeff7e0-24b4-11e9-afcf-86fffa5a272d%26client%3Da1acac3e1b3290917d92%26signature%3Da3d0cddf7fd2c0a1d0a52a535342d0da597a5601&client=amp-blogside-v2&signature=6dc2feec4a79a92c346e3be67592280c935e1570,engadget,Engadget,cloud,1
Rachel England,"And just like any reality TV show competition, there's been a good chunk of drama getting to this point. IBM and Oracle also bid for the contract, but ultimately didn't cut the mustard. That didn't stop Oracle from launching a lawsuit accusing an Amazon employee -- who had previously worked on JEDI -- of having undue influence in proceedings. The Pentagon, however, claims this had ""no adverse impact on the integrity of the acquisition process.""
Both Amazon and Microsoft have been focusing heavily on cloud computing in recent times. And while a government contract of this size would be a significant boon to both, motivation for success must certainly be driven by a degree of rivalry between the two competing companies. We'll find out in mid-July at the earliest who comes out on top.","Amazon and Microsoft are the two final companies in the running for the Defense Department's $10 billion cloud computing contract. The Pentagon's migration to the cloud, known as the JEDI project, was announced in 2017, with some of the biggest companies in t…",2019-04-11T10:20:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Microsoft and Amazon will fight for the Pentagon's $10B cloud contract,https://www.engadget.com/2019/04/11/microsoft-amazon-fight-pentagon-cloud-jedi/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D2983%252C1921%252C0%252C0%26quality%3D85%26format%3Djpg%26resize%3D1600%252C1030%26image_uri%3Dhttps%253A%252F%252Fs.yimg.com%252Fos%252Fcreatr-images%252F2019-04%252F044daa70-5c3d-11e9-be76-b16ac4bc4fd0%26client%3Da1acac3e1b3290917d92%26signature%3Dfca4f71903eee16be66c250a8851affbd2c4fa2f&client=amp-blogside-v2&signature=7b1fba595d4b4e1b2470529d658f7e7fe16a0d2a,engadget,Engadget,cloud,1
Mariella Moon,"Back in May, Mountain View conjured up a set of rules to help guide the company when figuring out which AI projects to develop and be involved with. While it will still work with the military, the guidelines would prohibit the use of AI in weaponry. The company wrote those ethical principles after employees strongly opposed its contract renewal for a separate Pentagon program called Project Maven, which aims to develop algorithms that can flag drone images for human review. Around 4,000 employees signed a petition asking the company to end its involvement with the project and some even left Google completely. Due to such a strong pushback, the tech giant didn't renew its contract -- employees were reportedly refusing to work on the tool anyway. According to the Tech Workers Coalition, Google withdrew from the JEDI competition due to ""sustained pressure"" from tech workers, possibly also from employees. The JEDI program is expected to award the full $10 billion contract to a single bidder, making Amazon the leading contender due to its extensive experience as a cloud provider. Oracle filed a complaint a few months ago to challenge that, telling the government that dividing a massive contract between several companies ""promotes constant competition, fosters innovation and lowers prices."" The government has yet to make a decision about the complaint, but Google said that if the contract had been open to multiple vendors, it would have ""submitted a compelling solution for portions of it."" As it is, the tech giant can't go any further in the competition. In addition to its concerns that some of the contract's terms might not align with its principles, Google also lacks some military clearances that would allow it to take up the assignment on its own.",Google has dropped out of a competition that could've won the company a $10 billion contract with the Pentagon. The Joint Enterprise Defense Infrastructure cloud (JEDI) contest's purpose is to find a solution for the military to transfer massive amounts of da…,2018-10-09T12:16:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Google sits out $10 billion Pentagon cloud contest over AI principles,https://www.engadget.com/2018/10/09/google-jedi-pentagon/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D1257%252C782%252C0%252C0%26quality%3D85%26format%3Djpg%26resize%3D1600%252C996%26image_uri%3Dhttp%253A%252F%252Fo.aolcdn.com%252Fhss%252Fstorage%252Fmidas%252F48e5c28b8f1590318b6c732613774e9a%252F200962921%252Fdv1132024.jpg%26client%3Da1acac3e1b3290917d92%26signature%3D30e30f2be21ceb79fe4efceb2b3543a490d7aa86&client=amp-blogside-v2&signature=f3de93be2b25af742ad9a4f548ef03650cd8a462,engadget,Engadget,cloud,1
Richard Lawler,"Earlier this year Uber sold off its ride-hailing business in southeast Asia to a competitor, Grab, which is now raising $3 billion to further expand operations. Today Microsoft announced it's making a ""strategic investment"" in Grab, as the two launch a ""broad partnership"" to use Microsoft's machine learning and AI tech. The first step is adopting Microsoft's Azure servers as the cloud platform backing Grab's ride-hailing and digital wallet. After that the plans get bigger, as it anticipates using machine learning and image recognition to let passengers share their location with a driver by taking a picture of their surroundings that the system recognizes and converts into an address. Otherwise it could handle recommendations, improve fraud detection, improve its maps or power facial recognition to identify drivers and passengers. There are also non-AI powered parts of the arrangement, like in-car entertainment systems, linked rewards programs and integration with Outlook. Like Uber, Grab is building a platform to do a lot more than have a stranger come pick you up in their car, while for Microsoft, all of this seems similar to its efforts to compete with Amazon in building more-connected grocery and retail stores and dominate developing back-end technology to control everything. Microsoft didn't say how much it's investing in Grab, but Japan's Softbank is reportedly investing $500 million, while Toyota already announced it's in for $1 billion.","Earlier this year Uber sold off its ride-hailing business in southeast Asia to a competitor, Grab, which is now raising $3 billion to further expand operations. Today Microsoft announced it's making a ""strategic investment"" in Grab, as the two launch a ""broad…",2018-10-09T05:26:00Z,"{'id': 'engadget', 'name': 'Engadget'}","Microsoft deal with Grab brings its AI, cloud tech to ride-hailing",https://www.engadget.com/2018/10/09/microsoft-grab-ai/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D3499%252C2342%252C0%252C0%26quality%3D85%26format%3Djpg%26resize%3D1600%252C1071%26image_uri%3Dhttp%253A%252F%252Fo.aolcdn.com%252Fhss%252Fstorage%252Fmidas%252F9b3a14053f12900fa9461fe581e1ba48%252F206722518%252FRTX3V0MI.jpeg%26client%3Da1acac3e1b3290917d92%26signature%3D7a1417ab5bedcf398b16a9a8272277bdc47643b2&client=amp-blogside-v2&signature=55592e623cfcc2c41211530dd8f5699a4d56a802,engadget,Engadget,cloud,1
JULIE TURKEWITZ,"Ryan Zinkes time in the Trump cabinet is ending, but his legal troubles are likely far from over. When Mr. Zinke was forced to resign as interior secretary on Saturday, he joined a line of officials who have left the Trump administration under a cloud of ethics inquiries. But the investigations into Mr. Zinkes actions are likely to continue, according to Delaney Marsco, the ethics counsel at the Campaign Legal Center, a nonpartisan watchdog group. And if those inquiries turn out badly for him, Mr. Zinke still faces the threat of criminal penalties that could hobble his political future. Its not a Get Out of Jail Free card to just quit, Ms. Marsco said. The most damaging could be a Justice Department examination of a real estate deal in Montana involving Mr. Zinkes family and a development group backed by David J. Lesar, the chairman of Halliburton, the giant energy services company. If the department finds that Mr. Zinke willfully used his official position to influence the deal and benefit himself, he could be prosecuted under a federal conflict of interest law and, if convicted, face a sentence of up to five years in prison and a $50,000 fine for each violation. The attorney general has discretion over whether to bring the charges. Before President Trump announced his departure on Twitter, Mr. Zinke, 57, had been a man accustomed to winning. As a politician, he had leapt up the Republican pole like a fire on a hot day, rising from freshman state senator to cabinet secretary in a matter of eight years. As a youth, he was president of his high school class and the broad-shouldered captain of an undefeated football team; in college, he was a well-regarded athlete; in the Navy, he had a 23-year career as a SEAL. Friends in Montana have long said they expected him to run for governor in 2020.","Investigations into the departing interior secretary’s actions will probably not end with his resignation, and may cloud his political future.",2018-12-17T01:30:39Z,"{'id': 'the-new-york-times', 'name': 'The New York Times'}",Ryan Zinke’s Legal Troubles Are Far From Over,https://www.nytimes.com/2018/12/16/us/ryan-zinkes-legal-troubles.html,https://static01.nyt.com/images/2018/12/17/us/17Zinke/17Zinke3-facebookJumbo.jpg,the-new-york-times,The New York Times,cloud,1
Leigh Anderson,"Sometimes certain words and phrases bubble up in the right-wing word cloud—Jeremiah Wright, birth certificate, Vince Foster, Kenya, Benghazi. The latest to surge to the front is “Uranium One,” the latest story on alleged Clintonian malfeasance to make headlines on both right-wing and mainstream media. A quick primer: A Russian company, Rosatom, acquired a majority stake in Uranium One, a Canadian company with rights to mine U.S. uranium, in 2010, and bought the rest in 2013. As the Associated Press explains : “Because Uranium One had holdings in American uranium mines, which at the time accounted for about 20 percent of America’s licensed uranium mining capacity, Rosatom’s 2010 purchase had to be approved by the Committee on Foreign Investment in the United States. That committee, known as CFIUS, is made up of officials from nine federal agencies, including the State Department, which [Secretary] Clinton ran at the time.” President Trump, mired in his own Russia scandal, has recently made this an issue, raising questions about some kind of quid pro quo deal, as people associated with Uranium One had donated millions of dollars to the Clinton Foundation. The connection was first noted in a book by Peter Schweizer, who frequently collaborates on films and books with Steve Bannon of Breitbart News. So Why Now? And Is There Anything to This? On Monday, the Justice Department said prosecutors were investigating whether a special prosecutor should be appointed to look into the Uranium One deal and ties to the Clinton Foundation. This appears to be a response to the president’s statement that Jeff Sessions, the attorney general, should be investigating the Clintons and the Obama administration. This is troubling for several reasons—the first of which is that presidents are not supposed to use their platform to punish political rivals. Secondly, as the Times notes: “Any such investigation would raise questions about the independence of federal investigations under Mr. Trump. Since Watergate, the Justice Department has largely operated independently of political influence on cases related to the president’s opponents.” The third issue—and this is the kicker—is that the F.B.I. investigated the Clinton Foundation in 2015, and did not uncover enough evidence to mount a case. The story has a strong, strong whiff of distraction—if the media is once more whipped up about Secretary Clinton, perhaps it will devote less air time to Special Counsel Robert Mueller’s investigation of the Trump campaign’s alleged ties to Russian in the 2016 election, an investigation that issued its first indictments two weeks ago. Interested in further reading? Check out the New York Times ’ story What is the Uranium One Deal and Why Does the Trump Administration Care So Much? or Shepherd Smith’s thorough explainer on Fox News:","Sometimes certain words and phrases bubble up in the right-wing word cloud—Jeremiah Wright, birth certificate, Vince Foster, Kenya, Benghazi. The latest to surge to the front is “Uranium One,” the latest story on alleged Clintonian malfeasance to make headlin…",2017-11-15T15:45:00Z,"{'id': None, 'name': 'Lifehacker.com'}",What Is Uranium One and Why Is It Suddenly a Big Story?,https://lifehacker.com/what-is-uranium-one-and-why-is-it-suddenly-a-big-story-1820468007,"https://i.kinja-img.com/gawker-media/image/upload/s--JjsZPU08--/c_fill,fl_progressive,g_center,h_450,q_80,w_800/i58cy3twnmiqcf5raftv.jpg",,Lifehacker.com,cloud,1
Tom Warren,"Microsoft first acquired Wunderlist almost three years ago, promising the to-do app would help the company “reinvent productivity for a mobile-first, cloud-first world.” At the time, Wunderlist founder and CEO Christian Reber joined Microsoft to continue leading the app forward. Instead, Microsoft launched its own To-Do app to replace Wunderlist last year, and fans of Wunderlist haven’t been happy it’s going away. At the time of Microsoft’s To-Do launch, the company announced the new app would eventually replace Wunderlist and incorporate most of its best features. That hasn’t really happened yet, and fans of Wunderlist are labelling Microsoft To-Do a “ half-cooked scrap.” Wunderlist found Christian Reber left Microsoft in September just a few months after the new app launched, and he’s now taken to Twitter ( spotted by Dr Windows ) to reveal why Microsoft’s To-Do app is taking so long to get to feature parity with Wunderlist. “There were technical porting challenges,” explains Reber. “Wunderlist’s API runs on Amazon Web Services, and should then be ported to Azure. But that was extremely time-consuming, so everything had to be rewritten.” The process of rewriting an entire app is clearly complicated, and as Reber puts it “easier said than done.” He also reveals that the original goal was to run Wunderlist for a year and then have Microsoft To-Do ready, but the porting from AWS to Azure means Wunderlist has remained online. Microsoft’s To-Do app Wishing the To-Do team success, Reber also mentions it “was not a pleasant experience,” in response to a Wunderlist fan who isn’t happy with its successor. It’s clear that Microsoft’s acquisition of Wunderlist and its integration into the company has been messy, and perhaps even avoidable. Microsoft has been acquiring mobile productivity apps at a surprising pace, and the company successfully turned Accompli into its popular Outlook mobile app. Microsoft also acquired Sunrise to shutter it and move its features into Outlook mobile. Wunderlist’s complications do sound all too similar to Microsoft’s struggles in overhauling Skype’s backend infrastructure. Microsoft has been using its own Messenger backend for Skype services for years now, but it was a rocky transition. Skype has also been redesigned so many times it’s hard to even recall the number, and if Microsoft isn’t careful with To-Do then it could end up with a Skype-like experience that’s desperately in need of being fixed.","Microsoft first acquired Wunderlist almost three years ago, promising the to-do app would help the company “reinvent productivity for a mobile-first, cloud-first world.” At the time, Wunderlist...",2018-03-21T09:45:05Z,"{'id': 'the-verge', 'name': 'The Verge'}",Microsoft’s Wunderlist acquisition is getting complicated,https://www.theverge.com/2018/3/21/17146308/microsoft-wunderlist-to-do-app-acquisition-complicated,https://cdn.vox-cdn.com/thumbor/mBjxdWD82lblh7-vP2zbOnQisTw=/0x146:2040x1214/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/3468188/DSCF1179.0.jpg,the-verge,The Verge,cloud,1
Mike Epstein,"Sometimes, its a lot easier to dump a file (or a ton of files) into a cloud storage service like Box or Dropbox instead of trying to attach them all in an email. That, or you hunt around the web for any number of free file upload services that promise to temporarily host data on your behalf.
This week, the folks at Mozilla launched a neat new service called Firefox Send, which makes it easy to send larger files using end-to-end encryption at no cost. Send allows you to send up to 1GB of encrypted data directly to another person by simply dragging and dropping files onto the page. If you want, you can also easily protect the transfer with a password. 
You dont have to sign up for a Firefox account to use Firefox Send, but if you log in with one, the transfer limit goes up to 2.5GB, and you can create a longer-lasting transfer packet that expires after a certain amount of time, (up to a week) or downloads (up to 100). Despite the branding, Firefox Send works with any browser.
Mozillas offering is hardly innovative, but its easy to use and feels less cumbersome than other services. While other services like WeTransfer give you more space without any required signup (2GB, in this case), a number of their features are typically locked behind a paid service. The account-based version of Firefox Send is more versatileand still free.","Sometimes, it’s a lot easier to dump a file (or a ton of files) into a cloud storage service like Box or Dropbox instead of trying to attach them all in an email. That, or you hunt around the web for any number of “free file upload” services that promise to t…",2019-03-12T19:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",Share up to 2.5GB of Files at Once (for Free) with Firefox Send,https://lifehacker.com/share-up-to-2-5gb-of-files-at-once-for-free-with-fire-1833232502,"https://i.kinja-img.com/gawker-media/image/upload/s--hfbxd-l---/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/kq65a9pvbmpgu7pqtsxt.png",,Lifehacker.com,cloud,1
Devindra Hardawar,"On the Surface Laptop -- a great ultrabook marred only by its weak integrated graphics -- running over our office's WiFi, PUBG felt almost as smooth as it does on my dedicated gaming rig. It ran at a steady 60 frames per second, even though I cranked the graphics settings to ""Ultra"" and the resolution to 2,560 by 1,400. After a few minutes of running around the game's apocalyptic European town and taking out other players, I almost forgot I was playing something that was running on a server hundreds of miles away. The game's excellent audio design also survived -- I had no trouble pinpointing people sneaking around a house while wearing headphones, and the bomb strikes in ""Red Zones"" still rattled my skull. Mostly, though, I was surprised that I didn't feel any lag while I was using the Surface Laptop's keyboard and a Logitech wireless G903 gaming mouse. Moving the camera around and aiming my weapons felt incredibly responsive, and I was surprised that I was able to outgun some players in some heated shootouts. That lack of latency as even more impressive with Overwatch, an even faster-paced game. Characters like Tracer and Genji, both of whom would be tough to play with any noticeable lag, felt as nimble as they do on my desktop. I didn't even have trouble landing shots with snipers like Hanzo and Ana. I was simply able to enjoy playing the game as I normally do. And, even more so than PUBG, I was impressed by how well GeForce Now handled Overwatch's vibrant and colorful graphics. Gorgeous maps like Ilios and Dorado appeared as detailed as ever, and the same goes for the game's imaginative character models and costumes. GeForce Now easily handled graphically intensive titles like Destiny 2 and The Witcher 3, which felt even more impressive to play on the Surface Laptop. Both games managed to run at 60 FPS at a 2,560 by 1,400 resolution (the service supports up to 2,560 by 1,600), with all of their graphics settings turned all the way up. Even though Destiny 2 isn't exactly a fast-paced shooter, it still benefited from the service's low latency, which helped me mow down waves of enemies without much trouble. And with the Witcher 3, I was impressed that its graphically rich world didn't lose any fidelity while being streamed. Perhaps because these games are particularly demanding, I occasionally experienced connection hiccups while playing them. They only lasted a few seconds, but if I were fighting against tough bosses, they could have easily led to my doom. Those disruptions also made it clear that your experience with GeForce Now will depend largely on your internet connection. I had a mostly trouble-free experience in our office and at home, where I have 100 Mbps cable service. But if you don't have a steady 25 Mbps connection, Ethernet access or strong wireless reception, you'll likely see more gameplay-disrupting issues. I wasn't able to run any games at Starbucks locations around NYC, and based on my terrible experiences with hotel WiFi, I'd wager you'd have trouble using GeForce Now while traveling, too. (The service is only supported in the US and Europe, at the moment.)","A year ago, NVIDIA's GeForce Now game streaming service let me play The Witcher 3, a notoriously demanding PC-only title, on a MacBook Air. This year, NVIDIA finally unveiled the Windows version of the service, and it was even more impressive. I was able to p…",2018-01-31T20:30:00Z,"{'id': 'engadget', 'name': 'Engadget'}",NVIDIA proves the cloud can replace a high-end gaming rig,https://www.engadget.com/2018/01/31/nvidia-geforce-now-pc/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D1600%252C1090%252C0%252C0%26quality%3D85%26format%3Djpg%26resize%3D1600%252C1090%26image_uri%3Dhttp%253A%252F%252Fo.aolcdn.com%252Fhss%252Fstorage%252Fmidas%252Fc7a1b4935293ff764af5171d3329262c%252F206083436%252FNVIDIA%252BGeforce%252BNow%252Bgallery%252B2.JPG%26client%3Da1acac3e1b3290917d92%26signature%3Dc8a311355faee8452cc562d261a042c73cb0a24e&client=cbc79c14efcebee57402&signature=1b035575baca677227b7bbd736d8d2baa3a1d7d7,engadget,Engadget,cloud,1
Dieter Bohn,"Exclusive: How Windows is changing to work with everything ”We are the Windows company, after all,” Microsoft CEO Satya Nadella told me. I was at Microsoft’s campus in Redmond a week before the Build developer conference, and I wanted to know what was going on with Windows after a reorg split the team into different divisions. Was Microsoft really preparing itself for a world without Windows ? Nadella was ready to tell me that Windows isn’t going away — of course Windows isn’t going away — but he also wanted to explain his latest buzzwordy vision for the future of the Microsoft: AI, Intelligent Cloud, and Intelligent Edge. Windows might still be here, but after talking to Nadella, I did get the sense that Windows is no longer as central to the company’s future plans as it once was. Instead of trying to make everything run on Windows (as his predecessor Steve Ballmer was trying to do), Nadella wants to ensure that everything can work with Windows. It’s a big shift following Nadella’s rise to CEO in 2014. Nadella’s Microsoft is fighting fewer losing battles, and instead, it’s trying to be just a little more open. New features in Windows 10 — like a timeline that works across multiple devices and a “Your Phone” system that lets users text and access phone data from their desktop — show a measure of pragmatism, if not humility. Of course, openness has its limits: Microsoft is still aggressively pushing users toward its Cortana assistant and Edge browser in Windows. Microsoft is a different kind of company under Nadella than it was just a few years ago. It’s working to get its software in IoT devices like drones, support companies with cloud services, and improve its enterprise software. Retail consumers aren’t exactly an afterthought, but Microsoft is starting to look more like IBM than Apple. The demos that Microsoft will be showing onstage at its Build developer conference are a good example of this shift. I got an early look at a few of them in Microsoft’s “Batcave,” a room filled with a tangle of cables, laptops, drones, and wildly expensive Surface Hub displays. It’s located in the same building as Nadella and the other executives, and it’s where new tech is tested and honed into something that could be shown onstage — and eventually (hopefully) shipped to businesses and consumers. In one demo, a DJI drone had Microsoft software loaded on it so it could recognize faults in an oil pipeline without the need for an internet connection. An off-the-shelf consumer drone can stream video to a Windows laptop to do the same. In both cases, the idea is that Microsoft thinks its customers will be better off if they happen to have Windows around to augment the gear they’re already using. Windows is important, but it’s not the only thing. But if you are entirely in a Microsoft corporate setup, the company is hinting at big things ahead. It’s integrating Cortana into its Slack competitor Teams. Cortana can be “invited” into your chats and automatically suggest stuff based on your conversation, like a meeting or document you need. I also sat in on a demo of a futuristic meeting: a speaker topped with a 360-degree camera recognized me and greeted me as I walked into the room. As we spoke, it transcribed our conversation live using far-field microphones. (Microsoft also says it could also translate between different languages live.) If one of us said something like “I will follow up with you on that,” a separate box logged an action item, correctly assigning the right person with the context of the conversation. Of course, HoloLens was involved. Microsoft is researching ways to make its augmented reality headset more useful, and visualizing data in 3D is a good use case. As one person used the HoloLens to manipulate a 3D map of a building’s temperature sensors, I was able to see it on the screen in real time. We were getting the same data displayed in multiple different ways: in AR on the HoloLens, on the display, and remotely to somebody else using a Surface tablet. The demo was just a bunch of existing Microsoft products working well together in a surprising way. And there’s no better metaphor for Nadella’s Microsoft: instead of naïvely hoping it will take over everything, it’s just working on the stuff it’s good at. And instead of promising a future that we know isn’t going to happen anytime soon, it’s building on what it has now. That’s a different kind of Microsoft than what we’re used to thinking of. It’s a little less flashy, yes, but it has the benefit of being a lot more likely to succeed. Photo by Vjeran Pavic / The Verge This interview has been edited for clarity and length. I just reread your book, and I’m curious about your effort to remake the culture of Microsoft. A s you said, the C in CEO stands for culture, and I’m wondering how that’s going. One of the things that I think about for any company, including ours, is that you really need to anchor yourself on that sense of purpose and identity, a culture that allows you to express that identity and sense of purpose with changing technology. In fact, [the Build conference] is actually a good place to start, because, if you think about it, our sense of identity is all in the story of our creation itself. We first were a tools company. Bill [Gates] and Paul [Allen] created the BASIC interpreter for the Altair. So that means in the very genesis of this company, it’s creating technology so that others can create technology. This notion of empowerment of people and organizations is very ingrained in our mission. And the best place to start that is by talking about developers. Then, on the culture side, you need a culture that allows you to learn and to be open to new ideas. Otherwise, it’s lethal in this business. If you think about tech and tech transitions, [you need to have] what we describe as this “growth mindset.” You have to really confront your fixed mindset. The feedback cycle from the developers to platform companies is probably a great place for us to keep renewing our culture. So what sorts of fixed mindsets does that feedback loop help you break? What sorts of changes are happening within the culture in that way? We built operating systems all our life, but what is an operating system? In a world where every person is going to use multiple devices in their life, they’re going to collaborate with many people in their family or at work. What does it even mean to build a platform? […] Now, we need to reconceptualize and build something for the person, not something that is about the device. So that’s what Microsoft 365 is all about, [to] allow anyone to have an experience that’s multidevice. We are the Windows company, after all. Windows and multi-device experiences are not at odds. In fact, “let’s make every Windows application a multi-device application” is something that we’re going to talk a lot about [at Build]. “Windows and multi-device experiences are not at odds.” Similarly, take Azure. We’re having amazing momentum. We’ll talk about customers using it. Azure is available across 50 regions. Having started in that group, it’s stunning to see the momentum and the scale. But guess what? Right as the cloud has become mainstream, the edge of the cloud is where the action is. In every factory and every hospital, you now need edge computing. And so you’ll hear us talk about everything from the microcontroller that runs Azure Sphere to Azure Stack, to the DJI drone, to the Qualcomm camera. These are the things that are exciting. There’s this swing between the cloud and the edge, but it’s basically local computing. Do you think of it that way? Do you think of it as a pendulum? [It’s fair to say] it’s swinging back, but it’s a very different form of swing-back. It’s not like, “Okay, let’s throw back to client-server computing.” This is very different event-driven, distributed computing, which will support the needs of the experiences. When you have an autonomous drone with the ability to see things, the amount of data that’s being generated, the amount of compute power that needs to be local, and how those events are processed, how inference happens, it’s a very different compute architecture. [It’s] not something that we did when you had either a phone or a PC connected to a server. So yes, it is swinging back, but it’s swinging back because the insatiable need for more compute and more data at the edge is what’s driving it. You just did a reorganization of Windows, and your memo did a really good job of explaining it from a Microsoft insider context. I think it really explained it to your staff and your team really well. But can you simply explain to an average consumer who w alks into a Microsoft Store to buy a Surface what’s coming next for Windows? Take the latest update to Windows we just did. It’s all about being able to recognize that every Windows user also happens to have a phone as well. So that means they already have multiple devices. They have a Windows device and perhaps a non-Windows device. How do we make sure that both those devices can work in concert to help the user get the most out of their computers? For example, the Timeline feature is a fantastic feature for continuity between devices. So, from a Windows user perspective, we want to continue to innovate in the computers we build like the Surface Book or the Surface or the HoloLens, give it new features like the Timeline or My Phone, and then on top of it, also make sure that we recognize the fact that they have multiple devices and that there’s task completion and task continuity for the user across all of these devices. That shift sounds like you are planning to not have Windows be as central to Microsoft. We were talking earlier about w hat an operating system is. It might even be a reasonable thing to ask : What is Windows? What is Windows in a year or two ? I s it still how I think of it? If you look at the capability that we have — whether it’s from innovation in the silicon itself to the natural user interface that depends on some silicon innovation to all the way to the cloud — we now have more of that capability than we ever had. So what is conceptually Windows: it’s always about managing a bunch of hardware resources, whether on the server or on the client, and creating an application model on top of it. We have that now a lot more than ever. The thing that we also want to recognize is just like on the cloud side. We did not say we’re building Azure only for Windows Server, but we’re going to welcome Linux as first class, Java as first class. When it comes to the users, we want to recognize that the only thing that will matter for Microsoft is us serving our users well across all of their devices. So this is not about taking away anything from Windows. If it’s anything, it’s adding to Windows the capability to span more devices. And that’s why we talk about Microsoft 365, so we’re not confused that our job is to serve our customers who have multiple devices. Photo by Vjeran Pavic / The Verge You wrote in your book that you see the world as going from a Windows runtime to a web runtime to an AI runtime, and that people are going to be building on top of it. But what exactly does that mean? What kind of stuff is being built on that? Xiaomi, for example, built a device using our cognitive capabilities, both speech recognition and machine translation. Chinese travelers worldwide are going to be the largest population of travelers. Just imagine if they could have a far-field device that’s just there in their pocket. And they can just put it in front, and the two of us could be conversing, one in Mandarin and the other in English, and in real time have language translation. That’s a real application of AI. The DJI drone where you now have a drone that’s flying over oil pipelines. We have a machine learning model that can recognize any breaks in the pipeline that’s being downloaded to run on the DJI drone itself. That means Azure Edge is in the DJI drone. Or Uber using our face recognition for their driver identification, Twitter using our AI capabilities to do machine translation. These are all practical applications. AI has gone sideways for a bunch of companies. We all know how badly it went for Facebook with the last election. Uber does not have a great history of treating i ts customers well. What sort of responsibility do you have to build protections into the AI capabilities you’re providing to developers s o that it won’t cause completely insane sideways effects? I would say we have a very broad responsibility as tech companies and tech platform companies, in particular. “You’ve got to recognize that privacy is a human right and that you need to treat it as such.” I think, when I look at it, there are three issues on which you have to have strong principles guiding you. One is on privacy. Fundamentally, where the world is going, you’ve got to recognize that privacy is a human right and that you need to treat it as such. Second, you have to treat security and cybersecurity as important for everyone. And third, you have to not just ask, “What can computers do?” But you should ask, “What should computers do?” So those are the three issues. For example, take privacy. In our case, [we embraced] some of the new regulation. Take GDPR. Let’s make sure that we’re doing all our work so that we’re ensuring GDPR compliance. And this is the start of our journey, but we take that very seriously. Or even what we did with the Cloud Act, right? Cloud Act is a breakthrough because it creates, for the first time, a framework of law that governs public safety and privacy. And I hope that the world comes to an equilibrium around it. Similarly, when it comes to security or cybersecurity, some of the work that we led, which is the Tech Accord, is making sure that tech companies will come together … ensuring that nobody attacks anybody, and especially civilians and small businesses don’t get impacted. That’s a huge breakthrough. [ Editor’s note: The Tech Accord includes a pledge to not help governments — including the United States — “launch cyberattacks against innocent citizens and enterprises.” ] On ethics, I think that there’s a lot that we need to do, but there’s a choice. Long before regulation, we as companies, we as designers of AI, should have a set of ethical principles. And so we’ve created even an ethics board internally. It’s called AETHER ? “You have to not just ask, ‘what can computers do?’ but you should ask, ‘what should computers do?’” That’s right. It’s a diverse group of people guiding design choices so that we do not run afoul of any unintended consequences. When you start allowing people to build on top of the AI, how much responsibility falls on you to limit their capabilities, and how much of it do you think is just encouraging people to not be terrible? Having been in the platform business for all these years at Microsoft, I think the key is for us to be able to guide people to make those design choices. I mean, take good UI, right? We’ve always had books about what it means to create good UI. I think we all now have to start reading about what it means to create good AI. And good AI is not just the technology frameworks. It is also the ethical principles that guide good AI, and I think that’s where we have to start. Video by Christian Mazza, Felicia Shivakumar, and Vjeran Pavic. Lead photo by Vjeran Pavic. Disclosure: Dieter Bohn’s wife works for Oculus, a division of Facebook. More about his ethics policy is here.","In this exclusive interview with Dieter Bohn, Microsoft CEO Satya Nadella kicks of the Build developer conference by speaking with Dieter Bohn about Windows, AI, and the ethics of cloud computing.",2018-05-07T15:30:02Z,"{'id': 'the-verge', 'name': 'The Verge'}",Exclusive: Satya Nadella on the future of Microsoft,https://www.theverge.com/2018/5/7/17324920/microsoft-ceo-satya-nadella-interview-windows-10-build-2018,https://cdn.vox-cdn.com/thumbor/iKiipyyEgcOgY2bPbLPOVJpx5Wo=/0x38:1920x1043/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/10792123/Microsoft_Build_v02.jpg,the-verge,The Verge,cloud,1
Ron Miller,"Remember how VMware was supposed to be disrupted by AWS? Somewhere along the way it made a smart move. Instead of fighting the popular cloud platform, it decided to make it easier for IT to use its products on AWS. Today, at the opening of the AWS re:invent customer conference, it announced plans to expand that partnership with some new migration and disaster recovery services. As Mark Lohmeyer, VP of the VMware cloud platform business unit pointed out, it’s only been a quarter since they made the original product generally available. The new products are designed to help ease the transition to the cloud. One of the issues VWmare customers have struggled with, is simply moving their mission critical applications from their datacenters to the cloud. Lohmeyer says the new migration product is designed to move applications quickly with no down time and for a reasonable cost (although that’s probably in the eye of the beholder). If it works as advertised, it really gives IT the best of both worlds. They can move their applications lock, stock and barrel from the datacenter to AWS and continue to monitor and manage them in the way they have been. If there truly is no downtime, this makes for an easy cloud transition, whether a company wants to start slowly and move an application or two or has plans to shut down the company datacenter and needs to move everything. What’s more, VMware is sweetening the pot by offering reserved pricing discounts over the traditional hourly pricing model. That means if you sign up for a year or three years you get a discount, although the amount will vary depending on volume and other factors. They are also introducing hybrid licenses, which allow customers to continue to use their on-prem licenses when they move to the cloud. All of these factors could reduce the overall cost of transitioning to the cloud over standard pricing approaches. VMware also announced a new disaster recover product. This enables IT to keep a copy of applications and data in the cloud on AWS. In the event of a disaster like a massive hurricane, power outage or other misfortune they can keep going with minimal disruption. “If you have a failure in your on-prem cloud, you can organize a restart on AWS,” Lohmeyer explained. The disaster recovery tool also helps those companies that may be slow to move to the cloud to get more comfortable with the idea. If you can store your disaster recovery backup in the cloud, you may be ready to move the actual applications at some point too. VMware was the company that popularized the virtual machine concept and today it is the premiere datacenter product. The problem for VMware has been that the future is moving out of the datacenter and into the cloud. Its first idea was to try to build its own public cloud service and compete with AWS. When that idea fizzled, VMware had a Plan B and that was to partner with the other cloud vendors. The biggest of those is of course AWS. As Lohmeyer sees it, this partnership is good for everyone. “It’s good for customers because they can readily take advantage of the cloud model. It’s good for VMware because we are keeping the customers on our platforms and growing the workloads. And it’s good for AWS because there are more workloads running on their platform.” If it works as described, that’s the ingredients for an ideal partnership.","Remember how VMware was supposed to be disrupted by AWS? Somewhere along the way it made a smart move. Instead of fighting the popular cloud platform, it decided to make it easier for IT to use its products on AWS. Today, at the opening of the AWS re:invent c…",2017-11-28T13:00:28Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",VMware expands AWS partnership with new migration and disaster recovery tools,https://techcrunch.com/2017/11/28/vmware-expands-partnership-with-aws-with-new-migration-and-disaster-recovery-tools/,https://tctechcrunch2011.files.wordpress.com/2017/11/aws-vmware-1.png,techcrunch,TechCrunch,cloud,1
Klint Finley,"Like most 12-year-old boys, Mitchell Hashimoto played a lot of videogames. But he never liked the repetitive parts of games like Neopets, where players feed and care for virtual animals. ""I used a lot of bot software that other people wrote to play the more mundane parts for me, so I could do the fun stuff,"" he says. Those bots were often blocked by gamemakers, so Hashimoto taught himself to program and created his own bot. When the creators of Neopets ordered him to stop using that bot, he was done with the game.
Along the way, he discovered that creating bots was more fun. ""It's the dream of every child programmer to create an army of robots,"" Hashimoto, now 29, says. Soon he was writing scripts to automatically set up web forum software. As a student at the University of Washington in Seattle, he wrote course registration software so he didn't have to wake up early to sign up for classes.
But even as automation let Hashimoto avoid some tedious tasks, he learned that programming came with its own time-consuming drudgery. So in 2012 Hashimoto and college friend Armon Dadgar cofounded HashiCorp, which makes open-source software designed to free programmers and system administrators from grunt work. ""The consistent thread of everything I've ever done is automating the things I don't want to do,"" Hashimoto says. ""Humans are good at creativity; computers should be doing the repetitive work.""
HashiCorp's flagship product, Terraform, has become the de facto standard for setting up, or ""provisioning,"" cloud infrastructure since the products launch in 2014, says Forrester analyst Charles Betz. Many software development tools simply assume that you use Terraform. The software is used by companies like Barclays, Capital One, and General Motors' self-driving car company, GM Cruise. Along the way, HashiCorp has grown to more than 400 employees, raised $174.2 million, and was most recently valued at $1.9 billion.
Building and running applications requires programmers and system administrators to install and configure programming languages, database systems, and a host of other tools. Cloud computing made some of this easier, but theres still plenty of rote work involved in setting up and configuring cloud servers and ensuring that applications have all the components they need to function. Terraform automates these sorts of tasks. Manuel Kiessling, a software architect in Cologne, Germany, likens the experience of using Terraform to ordering food from a restaurant: You don't have to give the chef explicit instructions on how to cook it.
The upshot is that it's much easier to get cloud applications up and running. ""We've gone from minutes rather than days to provision infrastructure,"" says Kieran Broadfoot, head of developer experience at Barclays.
Developer Focus
Much of Terraform's success stems from HashiCorp's focus on developers experience. HashiCorp releases open source versions of its products that anyone can use without charge. The open source versions are usually used by individuals, smaller companies, or tests. The company sells versions of its products with advanced features for teams in larger organizations.
HashiCorp's first product, which Hashimoto built before starting the company, was a tool called Vagrant. Vagrant helps developers build ready-to-use ""virtual machines"" that bundle up all the software a developer needs for a particular project. Once a virtual machine is built, it can be reused for other projects: A developer doesn't need to again install or configure the software it contains. Vagrant was an instant hit with programmers, who shared virtual machines to save each other the effort, and helped HashiCorp attract a loyal following of developers who were happy to check out subsequent HashiCorp products like Terraform or its security product Vault.
""It's like Apple devices,"" Kiessling says. ""You hold them in your hand and you're not sure if they have the features you need, but you can immediately sense that someone has put a lot of effort and love into them. You really sense that HashiCorp are people who know their stuff, people who care about quality, about technology.""
Kiessling started with Vagrant, then used Terraform for some personal projects. So far he's only used it for tests for his work at German retail giant Galeria Kaufhof. But that sort of grassroots interest helped HashiCorp land customers like Barclays. ""We knew many of our employees were using these technologies, so rather than go against the grain, we went with the tools our developers love,"" Broadfoot says.
Potential Threats
Cloud providers typically offer their own provisioning toolsbut they tend to work only with that companys technology. Amazon's CloudFormation tool, for example, only works with Amazon services. Terraform, by contrast, works with many cloud services. It can be configured to run an application's main code from, say, Amazon, but access data from Microsoft Azure.
Forresters Betz says there's a need for more of these sorts of ""multi-cloud"" setups. Many companies fear being locked in to a single cloud, he says. ""There are people out there saying 'I just got out from under IBM 10 years ago, there's no way in hell I'm going to go all in on Amazon,'"" he says. Others need tools that can work with so-called ""hybrid clouds,"" which combine private data centers with public cloud services from companies like Amazon and Google. Acquisitions can also result in companies having software that runs in multiple clouds.
For now, Terraform has few direct competitors, Betz says. But it could eventually be displaced by software that accomplishes the same ends in a new way. Thats what happened to Vagrant. It wasn't displaced by a better virtual machine, but by Docker, which uses a potentially more efficient technology called ""containers"" to create bundles of ready-to-use, self-contained software without the need to virtualize an entire operating system.
Even if Terraform is eventually displaced, HashiCorp has developers' attention. Kiessling mostly uses Docker instead of Vagrant now, but he's an advocate for Terraform. Whatever HashiCorp does next, he and countless other developers will be watching.
More Great WIRED Stories","HashiCorp has won fans among developers, and a billion-dollar valuation, by automating the mundane tasks of setting up and configuring servers.",2019-02-17T12:00:00Z,"{'id': 'wired', 'name': 'Wired'}",This Company Takes the Grunt Work Out of Using the Cloud,https://www.wired.com/story/this-company-takes-grunt-work-using-the-cloud/,https://media.wired.com/photos/5c64494c044c907a34a0946b/191:100/pass/Mitchell-HashiConf-2018-FINAL.jpg,wired,Wired,cloud,1
Jaime Green,"For those of us who don’t have an Amazon smart speaker, visits home for the holidays bring with them a new family member, always being shouted at. Alexa! ALEXA! SKIP SONG! But for anyone living with Amazon’s or Google’s smart home device, day in and day out, you eventually start to wonder: How much is it listening? Is it sending my every word to Big Tech Company HQ? First of all, yes, it’s always listening. It has to be, in order to hear its cue word—“Alexa” or “Hey Google” or “computer” or whatnot. But, according to Wired, it doesn’t record anything until it hears that cue. (The podcast Reply All recently did an episode on a related suspicion, the idea that Facebook is listening in through our phones to target ads according to our conversations. Short version: Facebook isn’t listening to what you say, but it’s tracking you every other way in order to target ads.) Once you trigger your device, however, it is recording, at least in order to send your queries to the cloud. So your queries go to a server in the cloud, the computers do their work, and Alexa or Hey Google gives you what you want. But what happens to your queries in the cloud? They stay there. Unless you clear them out. Here’s how: For Alexa, go into your Alexa app and select Settings, then History, to delete queries individually. To clear out everything at once, go to Manage Your Content and Devices in your Amazon account. Find your Echo under “Devices,” and select “Manage voice recordings.” You’ll get a pop-up that allows for the full purge. For Google, go to myactivity.google.com. Click on the three dots in the top right corner, and select “Delete activity by.” You’ll be able to choose a date range—or all time—and you’ll find “Voice &amp; Audio” under “All Products.” There’s no way to stop your Echo or Google Home from recording your requests—the devices themselves don’t have the information you need, so they have to record your voice to process your commands in the cloud—but you can at least do a regular clean-up. What Amazon Echo and Google Home Do With Your Voice Data | Wired","For those of us who don’t have an Amazon smart speaker, visits home for the holidays bring with them a new family member, always being shouted at. Alexa! ALEXA! SKIP SONG! But for anyone living with Amazon’s or Google’s smart home device, day in and day out, …",2017-11-26T20:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",How to Delete the Voice Data That Amazon Echo and Google Home Are Storing,https://lifehacker.com/how-to-delete-the-voice-data-that-amazon-echo-and-googl-1820737802,"https://i.kinja-img.com/gawker-media/image/upload/s--w5z37Vvb--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/jie4aoq1nehwwsbaudrt.jpg",,Lifehacker.com,cloud,1
Ron Miller,"It used to be that developers built applications with long lead times and development cycles. There was always plenty of time to prepare, but in today’s continuous delivery/continuous deployment (CI/CD) world, new versions could be going out every day. That requires a CI/CD framework, and today at Google Next in San Francisco, the company announced Cloud Build, its new CI/CD framework. As Google describes it, Cloud Build is the company’s “fully-managed Continuous Integration/Continuous Delivery (CI/CD) platform that lets you build, test, and deploy software quickly, at scale.” Cloud Build works across a variety of environments including VMs, serverless, Kubernetes, or Firebase. What’s more it supports Docker containers and it gives developers or operations the flexibility to build, test and deploy in an increasingly automated fashion. Google will allow you to use triggers to deploy, so that when certain conditions are met, the update will launch automatically. You can identify vulnerabilities in your packages before you deploy and you can build locally and deploy in the cloud if you so choose. If there are problems, Cloud Build provides analytics and insights to let you debug via build errors and warnings and filter those warnings to easily identify slow builds or those with other issues you want to see before deploying live. Google is offering a free version of Cloud Build with up to 120 build minutes per day at no cost. Additional build minutes will be billed at $0.0034 per minute.","It used to be that developers built applications with long lead times and development cycles. There was always plenty of time to prepare, but in today’s continuous delivery/continuous deployment (CI/CD) world, new versions could be going out every day. That r…",2018-07-24T16:43:52Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","Google announces Cloud Build, its new continuous integration/continuous delivery platform",http://techcrunch.com/2018/07/24/google-announces-cloud-build-its-new-continuous-integration-continuous-delivery-platform/,https://techcrunch.com/wp-content/uploads/2018/07/GettyImages-697538599-1.jpg?w=600,techcrunch,TechCrunch,cloud,1
Jacob Kleinman,"If you cut the cord to save money but miss your favorite cable channels, Channel Pear is here to help. The cloud-based streaming app offers live TV footage covering everything from MSNBC and CNN to ESPN and Cartoon Network—though it’s limited to just five channels unless you’re willing to pay for the service. Roku recently removed Channel Pear from its devices last month, citing the questionable legality of the app. But if you’re not ready to quit using the service, there are a few different ways to get it back up and running on your Roku at home. Watch Channel Pear on Roku With M3u Playlist Player Channel Pear has an easy workaround of its own for getting the app back up and running on your Roku. All you have to do is download a separate app called M3u Playlist Player and then watch your livestream through that. The first thing you’ll need to do it log into your Roku account in an internet browser. Then follow this link to download M3u Playlist Player and select Install. To launch the new app, pick up your Roku remote (or the app on your phone) and head into Settings and then System Update to check for updates. Once you’ve updated, the channel should appear on your Roku. All that’s left is to sync your Channel Pear playlists with M3u Playlist Player. To do that, head to your online library, click on “pair” to open a dropdown menu, and select Roku. You’ll see a unique URL, which you should select and copy on your phone. Next, launch the M3u Playlist Player on your Roku and hit New Playlist. Give your playlist a name (whatever you want is fine) and hit Next. Then, using the Roku app on your smartphone, copy your custom library URL into the text field. Relaunch M3u Playlist Player and you should be good to go. I tried this one for myself and the entire process was quick and easy. Even if you’re using Channel Pear for the first time, it shouldn’t take more than 30 minutes to get setup and start streaming on your Roku. Use Plex to Watch Channel Pear On Your Roku If you’re a fan of Plex, there’s another way to get Channel Pear back on your Roku. You’ll also need a Windows computer to use this method, which is outlined by The Streaming Advisor. Before we get started, you’ll need to setup a free Plex account (if you haven’t already), install Plex Media Server and download WinRAR. Then, download Channel Pear for Plex and open the zip file with WinRAR. From there, you’ll need to extract Channel Pear. Then change the file type by renaming it from “channelpear.bundle-master” to “channelpear.bundle” before installing it as a Plex plugin to finish the process. I don’t have a PC at home so I wasn’t able to try this method for myself, but it seems pretty straightforward. If you’re struggling, The Streaming Advisor has a step-by-step guide with detailed images to help you through the process.","If you cut the cord to save money but miss your favorite cable channels, Channel Pear is here to help. The cloud-based streaming app offers live TV footage covering everything from MSNBC and CNN to ESPN and Cartoon Network—though it’s limited to just five cha…",2017-11-24T16:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",How to Get Channel Pear Back on Your Roku,https://lifehacker.com/how-to-get-channel-pear-back-on-your-roku-1820686460,"https://i.kinja-img.com/gawker-media/image/upload/s--ygyxrj0P--/c_fill,fl_progressive,g_center,h_450,q_80,w_800/d0gu8vqi34wywlc6x7ul.jpg",,Lifehacker.com,cloud,1
Ron Miller,"Oracle executive chairman and CTO, Larry Ellison, first introduced the company’s autonomous database at Oracle Open World last year. The company later launched an autonomous data warehouse. Today, it announced the next step with the launch of the Oracle Autonomous Transaction Processing (ATP) service. This latest autonomous database tool promises the same level of autonomy — self-repairing, automated updates and security patches and minutes or less of downtime a month. Juan Loaiza SVP for Oracle Systems at the database giant says the ATP cloud service is a modernized extension of the online transaction processing databases (OLTP) they have been creating for decades. It has machine learning and automation underpinnings, but it should feel familiar to customers, he says. “Most of the major companies in the world are running thousands of Oracle databases today. So one simple differentiation for us is that you can just pick up your on-premises database that you’ve had for however many years, and you can easily move it to an autonomous database in the cloud,” Loaiza told TechCrunch. He says that companies already running OLTP databases are ones like airlines, big banks and financial services companies, online retailers and other mega companies who can’t afford even a half hour of downtime a month. He claims that with Oracle’s autonomous database, the high end of downtime is 2.5 minutes per month and the goal is to get much lower, basically nothing. Carl Olofson, an IDC analyst who manages IDC’s database management practice says the product promises much lower operational costs and could give Oracle a leg up in the Database as a Service market. “What Oracle offers that is most significant here is the fact that patches are applied without any operational disruption, and that the database is self-tuning and, to a large degree, self-healing. Given the highly variable nature of OLTP database issues that can arise, that’s quite something,” he said. Adam Ronthal, an analyst at Gartner who focuses on the database market, says the autonomous database product set will be an important part of Oracle’s push to the cloud moving forward. “These announcements are more cloud announcements than database announcements. They are Oracle coming out to the world with products that are built and architected for cloud and everything that implies — scalability, elasticity and a low operational footprint. Make no mistake, Oracle still has to prove themselves in the cloud. They are behind AWS and Azure and even GCP in breadth and scope of offerings. ATP helps close that gap, at least in the data management space,” he said. Oracle certainly needs a cloud win as its cloud business has been heading in the wrong direction the last couple of earnings report to the point they stopped breaking out the cloud numbers in the June report. Ronthal says Oracle needs to gain some traction quickly with existing customers if it’s going to be successful here. “Oracle needs to build some solid early successes in their cloud, and these successes are going to come from the existing customer base who are already strategically committed to Oracle databases and are not interested in moving. (This is not all of the customer base, of course.) Once they demonstrate solid successes there, they will be able to expand to net new customers,” he says. Regardless how it works out for Oracle, the ATP database service will be available as of today.","Oracle executive chairman and CTO, Larry Ellison, first introduced the company’s autonomous database at Oracle Open World last year. The company later launched an autonomous data warehouse. Today, it announced the next step with the launch of the Oracle Auton…",2018-08-07T20:00:44Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Oracle launches autonomous database for online transaction processing,http://techcrunch.com/2018/08/07/oracle-launches-autonomous-transaction-processing-cloud-service/,https://techcrunch.com/wp-content/uploads/2017/10/gettyimages-856460700.jpg?w=600,techcrunch,TechCrunch,cloud,1
Jon Fingas,"The leader of the panel, Infosys' Kris Gopalakrishnan, didn't comment on the recommendation. However, he acknowledged plans to submit the report to the Ministry of Electronics and Information Technology between the end of August and September 15th. As with similar moves by China and Russia, the demand would raise all kinds of practical and ethical concerns. Cloud-dependent companies like Amazon, Apple and Microsoft would have to set up local servers, raising costs that might pass on to users. There's also the matter of privacy: this theoretically makes it easier for authorities to spy on conversations or other sensitive info. This may also increase pressure on services that encrypt data, especially those messaging services that don't store chats on servers in the first place. Would India ask companies to decrypt data (even if they technically couldn't) during an investigation, or make firms hold on to conversation data where they didn't before? This hinges on whether or not the government acts on the proposal, but it could create awkward situations where companies might either have to weaken their privacy protections or give up on one of the world's largest markets.",India might be the next country to insist that internet companies store users' data inside its borders. Reuters has learned that a government cloud policy panel wants locally-generated data (including info about Indians) to be stored on servers within the cou…,2018-08-04T22:54:00Z,"{'id': 'engadget', 'name': 'Engadget'}",India may tell companies to store cloud data inside the country,https://www.engadget.com/2018/08/04/india-may-order-cloud-data-stored-inside-country/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D3614%252C2408%252C0%252C44%26quality%3D85%26format%3Djpg%26resize%3D1600%252C1066%26image_uri%3Dhttp%253A%252F%252Fo.aolcdn.com%252Fhss%252Fstorage%252Fmidas%252Ff09ca3e6afe4df67878865a61cedf4aa%252F204388650%252FRTS6Q3P.jpeg%26client%3Da1acac3e1b3290917d92%26signature%3D02819595f07462345ac361f813a3d3b2a1c7451a&client=cbc79c14efcebee57402&signature=bc1576414d28788257a67c0c1b964fcc6861bbfb,engadget,Engadget,cloud,1
Swapna Krishna,"The analytics that the Google Cloud team will be using to predict which team will win will be based on the interesting facts they've uncovered while analyzing NCAA data: ""everything from who blocks more shots per minute (for the record: juniors) to whether teams with a certain type of animal mascot cause more March Madness upsets (hint: meow),"" according to a blog post. But the team wondered whether they could do more if they used machine learning and their analytics to figure out what might happen during a game, based on how the teams were performing before halftime. That's why they're trying this halftime experiment using Google Cloud Platform tech. You can learn more about exactly what they are trying to do over at their blog. While this is certainly exciting for basketball fans, the Cloud Platform team is hoping that the techniques they use, and what they learn from the experience, will be broadly applicable. The team will break down what their results were after the Final Four.","Google has been working closely with the NCAA during this year's tournament, but now, during the Final Four, the company will be using predictive analytics to figure out who will win games. The wrinkle here is that the team will use data from the first half o…",2018-03-30T17:44:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Google will predict Final Four winners based on in-game data,https://www.engadget.com/2018/03/30/google-cloud-predict-final-four-winners-halftime/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fs.aolcdn.com%2Fhss%2Fstorage%2Fmidas%2F4da638cbb970df78730335e877e8be29%2F206260681%2Fbball-ed.jpg&client=cbc79c14efcebee57402&signature=684e3d8a84b81966b74a765d00bd1c27bc6a7775,engadget,Engadget,cloud,1
Ron Miller,"CoreOS announced Tectonic 1.8, its latest update of the popular Kubernetes container orchestration tool. It features a new open services catalog that enables DevOps personnel to plug in external services into Kubernetes with ease. As Rob Szumski, Tectonic product manager at CoreOS pointed out in a company blog post announcing the new version, public clouds offer lots of benefits around ease of use, but they can end up locking you in, in some cases to a proprietary set of tools. This is precisely what the new Open Cloud Services catalog is designed to resolve. Instead of using those proprietary tools, you get more open choices and that should make it easier to move between clouds or a hybrid environment. “New for Tectonic 1.8, CoreOS Open Cloud Services offer the same near-effortless operations customers have come to expect from managed cloud offerings with a difference. Unlike proprietary cloud services, Open Cloud Services are first-class, fully automated Kubernetes resources running on the CoreOS Tectonic platform,” Szumski wrote in the blog post. CoreOS is aiming to keep the whole process as open and portable as possible, so that customers can choose where and how they want to deploy their applications. What’s more, the Open Cloud Catalog is built right into the Tectonic console, making it simple to enable the external services (or disable them as you choose). Among the earliest additions to the Open Cloud Services offering are etcd, Prometheus and Vault. The Open Cloud Service Catalog was only part of the Tectonic 1.8 release, which puts it in line with the open source version released at the end of September. As CoreOS points out, this is a pure upstream version of Kubernetes, meaning it’s not a fork, something the Cloud Native Computing Foundation, the organization that manages the Kubernetes open source project has been striving for from members. It will also automatically update the Docker container engine, which developers use to create the containers that make up an application. Operations uses Kubernetes to manage and deploy the containers. That means, it’s taking care of both sides of the container DevOps equation for you. The latest version will be shipping towards the end of the year and the company says it will be a smooth and automatic update from Tectonic 1.7 for existing CoreOS customers. Featured Image: Stan Olszewski/SOSKIphoto/ Flickr UNDER A Copyright LICENSE","CoreOS announced Tectonic 1.8, its latest update of the popular Kubernetes container orchestration tool. It features a new open services catalog that enables DevOps personnel to plug in external services into Kubernetes with ease. As Rob Szumski, Tectonic pro…",2017-12-05T14:11:27Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",CoreOS Tectonic 1.8 makes it easy to plug external services into Kubernetes,https://techcrunch.com/2017/12/05/coreos-tectonic-1-8-makes-it-easy-to-plug-external-services-into-kubernetes/,https://tctechcrunch2011.files.wordpress.com/2017/11/30253951384_c484ac85c7_o-1.jpg,techcrunch,TechCrunch,cloud,1
Jamie Rigg,"Hatch is a cloud gaming service for your phone, very much in beta at this point and currently available on Android in 16 European countries including the UK. Your first reaction to this idea, if you think like me, might be: But... why ? Is it not just an over-engineered solution to a problem that doesn't really exist? Is storage space so precious we need to host mobile apps in the ether? These were the questions bouncing around my head when I first came across Hatch, but the method of delivery is just part of the picture. What Hatch really wants to be is a champion of premium mobile games and creator of new social ways to play them. Free-to-play games are more prevalent on mobile than any other platform, but Hatch more or less ignores this vast catalog. Instead, the games it's adopted to date either have an upfront price attached or offer a basic, demo-esque experience ahead of in-app purchases to unlock the full game. No money changes hands on Hatch, though. You can stream the complete version of every game for free, though Hatch doesn't like the word ""free"" since you are paying, to some extent, by putting up with ads dotted around the UI. They're hardly intrusive (at this stage, anyway), but eventually Hatch will offer a subscription tier that strips the ads away. Some special features will also be reserved for paying users, such as a kids mode with parental controls. Hatch is still figuring out exactly what its subscription option will look like because it won't come into play until the app is fully developed. I'm not an avid mobile gamer, but there are several titles even I recognize on Hatch's books, such as Monument Valley, Pac-Man Championship Edition DX, Rocket Riot and Mini Metro. A new partnership with Square Enix Montreal means Hitman GO, Lara Croft GO, Deus Ex GO and Hitman Snipe r will arrive on Hatch later this year, too. There are only around 50 games playable in the UK currently, but users in other regions have access to more than 100. You'd think big developers might shy away from a service like Hatch: Every player is someone that may've otherwise bought these games from the Play Store. But that hasn't stopped studios like Ubisoft, Bandai Namco and Ustwo from pledging their support (another big name will be added to that list in a week or so, I'm told). The cloud service itself was spun out of and is still majority owned by Rovio, which knows a thing or two about mobile gaming. Naturally then, Angry Birds and Bad Piggies have a home on Hatch. Developers showing faith in Hatch, not to mention wanting a cut of ad/subscription revenue, is one thing. Execution is the other part of the puzzle. And if you're still struggling to see the value in a cloud streaming service for mobile games, actually playing around with Hatch is where it all seems to come together. For lack of a more eloquent phrase, it's slick AF. For starters, the main UI is colorful and inviting. You swipe left and right between genre pages, with animated banners up top and visual cards for the different games beneath. There's also a page that lists your recently played games and one for your social feed (more on that later). Tap a game card and you'll see a trailer of sorts above a short description of what it's all about. Hit that play button and after a short loading time you're playing the game as if you'd downloaded it to your device. That's how a cloud gaming service is supposed to operate, of course, but the journey there is worth a mention. The real light bulb moment, though, is when you start exploring Hatch's social features. At any point during your play session, you can jump into the Hatch menu and invite a friend to join your game. Up to four friends, in fact. Not only is there built-in voice chat, but both of you can actually play the game together -- multiplayer for games that don't have multiplayer. During a quick session of OK Golf, for example, a colleague and I were chatting away and taking alternate shots, as if we were sitting on a couch passing one phone between us. According to Hatch, this isn't even hard to do, technically speaking. Since the game is running in the cloud, the servers just push the same instance to both devices. In other words, the game has no idea you're playing it on two different phones. Also, as the host, I have the option of locking my friends' controls if I start seeing too many wayward shots. Multiplayer doesn't have to mean taking turns, though. You could invite a friend in to advise you on how to beat a Bridge Constructor Medieval level, or just have them watch you speedrun Badland while you shoot the shit. In addition to sharing your exploits with friends live, you can also clip special moments. Hatch records your gameplay on a 45-second loop. Do something worthy of remembering you can jump into a simple video editor, complete with cropping and annotation tools. You can then share this moment on Hatch's internal social network or on Instagram. The Hatch feed is actually a lot like Instagram, with captions, likes, comments and the ability to follow other users. These features are what make Hatch feel like more than the sum of its parts, more than the catalog of titles. Mobile gaming is, for the most part, a solitary experience. Hatch opens this up, and it's still early days. The team has broadcasting ( à la Twitch) on its roadmap, so players can livestream to anyone who wants to watch, not just who they invite into their games. ""Technically speaking, it's super easy,"" Hatch co-founder and Head of Content Vesa Jutila told me, since the whole system is built to stream the same game instance to multiple devices. The challenging part is delivering it to a player a million people can tune in to. Competitive and co-op multiplayer is also a priority, but first Hatch wants to finish developing an over-the-top lobby system that make it as seamless as possible. Something a bit like Xbox Live parties. ""One of the powerful things about a service like Hatch is the fact that you can design the multiplayer game experience across all the games at a platform level, instead of designing it for every individual game one-by-one,"" Jutila said. He believes it could even change how developers make games, giving them more time to craft great multiplayer without worrying about the nuts and bolts of putting people together in the same session. Hatch will handle that. Before all these extra social features are realized, though, Hatch still has a lot of polishing to do on the core experience. As much as I came round to the idea, there's still a fair few glitches to iron out. These range from the occasional lag spike that disrupts the flow of a game to voice recording for game clips straight up not working. During my playtests, I found that if I hosted a game, I could hear people I invited clearly on voice chat, while they heard a patchy, robotic voice from my end. Swap over and I would sound clear while the new host became the Dalek. Two-way audio is one of the harder things to get right, I was told, because of the different ways different Android devices handle sound. Hatch is still very much in development at this stage. Finland was the first country to have at the open beta last August and since then it's expanded to a total of 16 European countries. For now the plan is to settle in Europe, stick to Android, complete development and then reach other countries and platforms. I wonder whether Apple would allow Hatch on iOS since it encourages people to stream rather than buy games. But then again the same could be said for Android, and yet Hatch is featured on Google's Early Access program. My crystal ball won't reveal whether mobile cloud gaming will ever be a mass market thing. ""Once you offer an on-demand way to consume entertainment, adoption can happen super fast. When something starts when you press play, you never want to go back,"" Jutila told me. ""With consumers, as always, getting them to move to the next paradigm requires time,"" he added. ""It's a totally new experience and before you've tried it and you understand what it is, it's very difficult to explain."" Assuming you can get a solid catalog together, perhaps the sheer convenience of no downloads or updates will be enough. A potential problem is that Hatch isn't in control of 'the last mile,' by which I mean a crappy connection and resulting sub-par experience might quickly turn you off. But 5G is just around the corner, promising better speeds and lower latency than existing tech. ""Yes, there will always be some possibility that you have a crappy network connection and for that moment, there's some glitches in the game,"" Jutila said. ""So be it, because all in all the on-demand model for gaming is way more powerful and it enables this totally new kind of social experience that wouldn't be possible without it."" He compares it to an odd buffering incident you might suffer while watching Netflix. It's not ideal, but you'd rather that than go to a store to buy a DVD. If you want to game on the go, you've also gotta think about the damage it's doing to your data allowance. While there's still room for optimization, according to Jutila, in my experience the app used the best part of a gig per hour of playtime. All that said, if there's an appetite for cloud gaming on the small screen, it's going to be a service like Hatch that satiates it. Because it's not even really about the streaming -- it's about turning the introverted nature of mobile gaming into a more social, console-like experience.","We're reached a point where cloud gaming finally makes sense. The technology that exists now is beyond what was available to famous failures like OnLive and many others you could say were ahead of their time. Servers, the consoles and computers we have in our…",2018-03-14T14:30:00Z,"{'id': 'engadget', 'name': 'Engadget'}",A cloud service for mobile gaming isn't as dumb as it sounds,https://www.engadget.com/2018/03/14/hatch-mobile-cloud-gaming/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fs.aolcdn.com%2Fhss%2Fstorage%2Fmidas%2F74454e0a83fba514044e56f5187b5b25%2F206211554%2Feng-hatch-edJT.jpg&client=cbc79c14efcebee57402&signature=81bb551829bc5ec09b12f9b7046dc0d84971eb21,engadget,Engadget,cloud,1
Jon Porter,"HyperX has announced its first Bluetooth headset. Called the Cloud MIX, the headset’s use of the universal Bluetooth standard means that it will pair just as easily (or clumsily, as in the case of most Bluetooth connections) to your smartphone as it will to your PC. With an internal microphone in addition to the detachable mic arm, the Cloud Mix have a shot at avoiding the gamer-y look that stops most people using gaming headsets as standalone wireless headphones. The Cloud Mix isn’t HyperX’s first wireless headset, but its pre-existing Cloud Flight relied on a USB dongle to connect to your machine wirelessly. Gaming headset manufacturers have previously relied on these dongles to reduce latency and offer cross-compatibility between PC and consoles. We haven’t had a chance to try out the Mix yet to see how responsive it is, but the change means you’ll need to use it in wired mode if you want to use it with a console. Playback time is rated for 20 hours, which is about average for a pair of wireless headphones in 2018. Once that power’s gone you’ll be charging them over Micro USB — no USB Type-C here, unfortunately. Even if it doesn’t do everything perfectly, the Cloud Mix has the advantage of working in so many different situations that it might not end up mattering. The headset is available now in the US for $199.99.","HyperX has announced its first Bluetooth headset. Called the Cloud MIX, the headset’s use of the universal Bluetooth standard means that it will pair just as easily to your smartphone as it will to your PC.",2018-10-08T13:38:24Z,"{'id': 'the-verge', 'name': 'The Verge'}",The HyperX Cloud Mix wants to be your gaming headset and wireless headphones rolled into one,https://www.theverge.com/circuitbreaker/2018/10/8/17950758/hyperx-cloud-mix-bluetooth-gaming-headset-release-date-price-features,https://cdn.vox-cdn.com/thumbor/8c59ob0VRuQKq29XHmU3U3ndEa0=/148x188:1398x842/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/13235297/PR_images_1000x611.jpg,the-verge,The Verge,cloud,1
David Murphy,"There’s nothing inherently wrong with Dropbox, Google Drive, or whatever popular service you use to back files up to the cloud and keep them synchronized between your multiple desktops and laptops. However, data privacy is becoming a bigger deal, and we wouldn’t question you if you’re ready to make the leap away from these kinds of services. The open-source utility Syncthing is a great way to get all the benefits of local file synchronization without having to give up your data to another company (no matter how much they promise they’ll never look at it). The trade-off is that Syncthing is a bit trickier to manage than a service like Dropbox, which makes it painfully easy to upload, download, and sync your files across all your devices. Access to decent cloud storage is practically a necessity these days whether it’s for work or play, … Read more Read Switching to Syncthing requires some setup To get started, head on over to Syncthing’s site and look for a link to SyncTrayzor —a windows utility that allows you to use the program through a pretty GUI instead of the headache-inducing command line. Download the appropriate version for your Windows PC (32- or 64-bit), or the portable version if you feel like being different, and install (or run) the app. Repeat that process on every computer you want to incorporate into your big file-synchronization plan. When you launch SyncTrayzor, you’ll first want to link together all the different systems that will be sending and receiving files. For simplicity’s sake, we’ll pretend we’re linking together two Windows desktop PCs: System A and System B. On System A, click on the “Add Remote Device” button. On System B, click on the Actions button in the upper-right corner and select “Show ID.” Copy the absurdly long device identification code from System B into the Device ID field on System A. Enter a Device Name for System B (like “Laptop.”) Click on save. Now, repeat this process in reverse—add System A onto System B’s version of SyncTrayzor, using System A’s device ID. Think of this process as creating a friendship between your two devices. If both don’t want to be pals, they won’t trade secrets. If each recognizes the other, though, they’ll just chat and chat forever. Got it? Now, SyncTrayzor on one of your computers (say, System A), and click on the “Add Folder” button in the upper-left portion of the app. Pick the folder you want to synchronize across your multiple systems, give it a Folder Label (a name) you’ll recognize, and select which devices you want to share the folder to. Repeat this process for every folder you want to synchronize (or just create a new folder somewhere called “Sync” and dump all your stuff in there, Dropbox-style). SyncTrayzor should now, in theory, start automatically dumping your synchronized folder across all the systems you selected. If it’s screwing up at all, or doesn’t appear to be working, make sure that you don’t have some kind of internal network issue that’s messing things up. You can also restart SyncTrayzor from the Actions menu, which might clear up your issue on any (or all) systems.","There’s nothing inherently wrong with Dropbox, Google Drive, or whatever popular service you use to back files up to the cloud and keep them synchronized between your multiple desktops and laptops. However, data privacy is becoming a bigger deal, and we would…",2018-05-14T21:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",Syncthing Synchronizes Your Files Without Giving Them Up to a Third-Party Service,https://lifehacker.com/syncthing-synchronizes-your-files-without-giving-them-u-1826017871,"https://i.kinja-img.com/gawker-media/image/upload/s--hY3AwNnt--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/rsn5ezbm27fgiklorkbz.jpg",,Lifehacker.com,cloud,1
Nick Douglas,"Last week, Twitter revealed that it had accidentally stored some user passwords in plain text, and thus suggested that all users change their Twitter password. It was bad. But honestly not that bad, according to Tristan Bolton, founder of enterprise cloud provider BoltonSmith. We talked to him about how it might have happened, and how it could have been worse. How it normally works First, here’s what was supposed to happen to your password. As Twitter CTO Parag Agrawal explained when announcing the mistake, the service normally never stores your actual password. When you make a Twitter account or change your password, Twitter encrypts it by running it through an algorithm to get a “hash”—a long string that’s like a coded translation that only works one way. (It probably also “salts” the hash, so that if two people use the same password, the two stored hashes aren’t also the same.) Between LinkedIn, Zappos, Dreamhost, and other prominent sites recently hacked, you’ve likely been… Read more Read Twitter stores that encrypted hash instead of your actual password. Every time you log in, Twitter turns your entered password into a hash again, and checks it against its stored hash. If they match, it lets you in. If they don’t match, it doesn’t. While you can always turn the password into the hash, you can’t turn the hash back into the password. (It’d be kind of like turning a smoothie back into strawberries and milk.) This means that if someone ever hacked into Twitter’s database of hashes, they still wouldn’t have everyone’s passwords. Because people are hacking into databases all the time, it’s crucial that services don’t save users’ actual passwords. So, says Bolton, it’s become such standard practice that every computer science student learns it. Even small informal services usually turn passwords into hashes. This wasn’t always the case; it became much more common after multiple high-profile breaches that exposed millions of accounts. What can go wrong But Twitter says that at one point, it failed to do this. Bolton explains how that might happen: Developers often run their software in debug mode, which produces detailed logs of everything the software does. “When you are building an application, you often want very detailed logs to see what is going on to easily troubleshoot and/or verify that the app is working as intended,” he says. Debug menus are great little secrets that often hide useful extra settings. Apple’s hidden these… Read more Read But occasionally a developer forgets to turn off debug logging before taking a system live. This means that the system keeps logging data it doesn’t need—or data it’s not supposed to log. And that can include unencrypted passwords. This, Bolton says, could be what happened at Twitter. (We asked Twitter to confirm; they declined to comment.) Twitter’s response According to Bolton, while the mistake was “very unprofessional,” Twitter’s response wasn’t: They alerted their users even though the risk was low, when they theoretically could have just hidden the incident. While Twitter might not be heroes just for doing the right thing, they’re certainly a lot better than Equifax, which tried hiding its data breach for months, while its executives quietly sold their stock in a possible case of insider trading. And the SEC recently fined Yahoo $35 million for hiding data breaches that exposed billions of accounts. It’s the security breach that never ends. Reuters reports that 2.5 million more people were… Read more Read Sometimes, Bolton says, it’s actually appropriate to keep quiet about a data breach or mistake. If Apple discovers a vulnerability and needs a week to fix it, it might be safer to keep it secret until the fix is available. Otherwise hackers will have a free week to exploit the vulnerability. (The ethics of these choices are highly debated in the security world.) But in a case like Twitter’s, where the solution is immediately available, it’s best to inform the public. There’s little risk that the passwords made it anywhere outside of Twitter’s now-deleted internal log, says Bolton. (Otherwise Twitter would need to force everyone to change their password, not just suggest it.) But there’s always a slight risk. You’re probably fine if you leave your front door unlocked today, but why take the chance? So change your password, and if you used it anywhere else, change that too. (And never reuse passwords again.) Make your password long, and store it in a password manager. And turn on two-factor authentication so hackers need more than your password to log into your account.","Last week, Twitter revealed that it had accidentally stored some user passwords in plain text, and thus suggested that all users change their Twitter password. It was bad. But honestly not that bad, according to Tristan Bolton, founder of enterprise cloud pro…",2018-05-08T20:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",How Twitter's Password Screwup Might Have Happened,https://lifehacker.com/how-twitters-password-screwup-might-have-happened-1825861476,"https://i.kinja-img.com/gawker-media/image/upload/s--yLsbx-Di--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/rhia6mfisehxzy7wzja8.jpg",,Lifehacker.com,cloud,1
Shannon Liao,"Today, Amazon reported earnings for the third quarter of 2018, showing that its North American business remains solid, boosted by Echo device sales and cloud services. At the same time, it’s still minimizing its losses in international markets. In North America, Amazon pulled in $34.3 billion, which is a 35 percent improvement from last year. Amazon Web Services continued to bolster numbers, rising 46 percent in sales. International sales trail behind and Amazon is still losing money in the category. Its losses this quarter however were only $385 million, compared to $936 million year over year. That’s the narrative we’re seeing as Amazon trucks along past the $1 trillion valuation number. Although the company doesn’t provide a breakdown of sales for particular devices, we do know that during this past quarter, Amazon Prime Day was a large driver of sales, even if the site momentarily broke down. The company didn’t give sales specifics but did say customers purchased over 100 million products, approaching the size of last year’s Cyber Monday event, where customers bought over 140 million products. The one category that Amazon is still fumbling with is international sales, as it slowly develops its smart assistant Alexa to learn more languages and expands its Echo offerings into additional countries. Overall, net income for the company increased more than 10 times year over year to $2.8 billion, a huge jump due to AWS sales, advertising, and services offered to third-party sellers. Revenue is up 29 percent year over year. Not everything is rosy in CEO Jeff Bezos’ world though. Amazon expects to bring in about $66.5 billion to $72.5 billion during the holiday season, which is below analyst expectations by over $1 billion. The company says it anticipates the “unfavorable impact” of foreign exchange rates to be the main cause.","Today, Amazon reported earnings for the third quarter of 2018, showing that its North American business is still solid, boosted by Echo device sales and cloud services. At the same time, it’s still minimizing its losses in international seas.",2018-10-25T21:01:42Z,"{'id': 'the-verge', 'name': 'The Verge'}",Amazon is doing well in North America but still struggles overseas,https://www.theverge.com/2018/10/25/18024660/amazon-earnings-q3-echo-sales-cloud-services,https://cdn.vox-cdn.com/thumbor/JlFBATViiUK1wNypd_9olPkgUfA=/0x215:3000x1786/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/10745911/acastro_180329_1777_amazon_0001.jpg,the-verge,The Verge,cloud,1
Steve O'Hear,"ContentSquare, which offers cloud-based software that helps businesses understand how and why users are interacting with their app, mobile and web sites, has picked up $42 million in Series B funding. The round is led Canaan, the U.S. VC firm that has previously invested in LendingClub, Ebates, and Match.com, with participation from previous backer Highland Europe, Eurazeo, and H14. The new injection of capital for the Paris startup will be used to grow the business in the U.S. and globally. In a call with co-founder and CEO Jonathan Cherki, he explained that ContentSquare ‘digital experience’ (UX) platform has been developed to go beyond traditional web or mobile app analytics to not only enable companies to see how users interact with their digital experiences but to make the resulting insights actionable. This can include making changes to specific page or content elements (or combinations) to increase engagement and conversion rates. To do this, the software, which is used by content, eCommerce, analytics, acquisition, IT and UX teams, claims to compute billions of touch and mouse movements every day in 191 countries. The longer term vision, explained Cherki, is to become the first “fully automated, Artificial Intelligence (AI)-driven” digital experience platform. It’s no longer enough to know where users or customers are dropping off or stalling, but what should done to rectify this. The thinking behind ContentSquare is that if these actionable insights can be delivered automatically and in an easy to understand way, analytics can be a driver for change in the way digital teams work across the whole company. “This is about people transformation more than digital transformation,” says the ContentSquare CEO. To make this vision more of a reality, the company has recently rolled out new features and technology, including what it describes as an AI engine to analyse behavioural data and offer automatic insights, and its “Auto-Zone” feature that replaces content tagging and tag configuration with automatic element identification. The latter means that ContentSquare automatically recognises different page or app elements and can therefore track changes more easily to feed into the aforementioned AI engine. Meanwhile, the company previously raised a $20 million Series A fundraising in late 2016 led by Highland Europe and supported by business angels from Seed4soft. Since then ContentSquare has expanded globally and now has offices in Paris, Munich, London and New York, and has grown to over 200 employees. It says it has 120 customers worldwide, including Orange, Rakuten, Carrefour, Walmart, Tiffany’s, Clarks, Goldman Sachs, Abbott, SNCF, AccorHotels, and L’Occitane.","ContentSquare, which offers cloud-based software that helps businesses understand how and why users are interacting with their app, mobile and web sites, has picked up $42 million in Series B funding. Read More",2018-01-29T12:00:11Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",ContentSquare raises $42M Series B for its UX insights platform for mobile and web,http://techcrunch.com/2018/01/29/contentsquare/,https://tctechcrunch2011.files.wordpress.com/2018/01/using-contentsquare-1.png,techcrunch,TechCrunch,cloud,1
Ron Miller,"Diane Greene announced in a blog post today that she would be stepping down as CEO of Google Cloud and will be helping transition former Oracle executive Thomas Kurian to take over early next year. Greene took over the position almost exactly three years ago when Google bought Bebop, the startup she was running. The thinking at the time was that the company needed someone with a strong enterprise background and Greene, who helped launch VMware, certainly had the enterprise credentials they were looking for. In the blog post announcing the transition, she trumpeted her accomplishments. “The Google Cloud team has accomplished amazing things over the last three years, and Im proud to have been a part of this transformative work. We have moved Google Cloud from having only two significant customers and a collection of startups to having major Fortune 1000 enterprises betting their future on Google Cloud, something we should accept as a great compliment as well as a huge responsibility,” she wrote. The company had a disparate set of cloud services when she took over, and one of the first Greene did was to put them all under a single Google Cloud umbrella. “Weve built a strong business together — set up by integrating sales, marketing, Google Cloud Platform (GCP), and Google Apps/G Suite into what is now called Google Cloud,” she wrote in the blog post. As for Kurian, he stepped down as president of product development at Oracle at the end of September. He had announced a leave of absence earlier in the month before making the exit permanent. Like Greene before him, he brings a level of enterprise street cred, which the company needs as it continues to try and grow its cloud business. After three years with Greene at the helm, Google, which has tried to position itself as the more open cloud alternative to Microsoft and Amazon, has still struggled to gain market share against its competitors, remaining under 10 percent consistently throughout Greene’s tenure.",Diane Greene announced in a blog post today that she would be stepping down as CEO of Google Cloud and will be helping transition former Oracle executive Thomas Kurian to take over early next year. Greene took over the position almost exactly three years ago …,2018-11-16T18:03:13Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Former Oracle Exec Thomas Kurian to replace Diane Greene as head of Google Cloud,http://techcrunch.com/2018/11/16/former-oracle-exec-thomas-kurian-to-replace-diane-greene-as-head-of-google-cloud/,https://techcrunch.com/wp-content/uploads/2018/07/GettyImages-604493160.jpg?w=601,techcrunch,TechCrunch,cloud,1
Nick Statt,"Google is launching a new form of secure login for business customers it’s calling context-aware access. Instead of relying solely on users to appropriately set up two-factor authentication and manage secure passwords, context-aware access will let system administrators for G Suite businesses put a set of parameters in place that can prevent malicious third parties from accessing secure systems. These parameters, as the name implies, can now be based on a user’s context: where they’re logging in from, the IP of the machine they’re using, the time of day, and other factors. The news was announced this morning on the second day of the company’s Cloud Next conference in San Francisco. Google is expanding the idea of security to include context clues “People increasingly want access to their business critical apps on the devices that make the most sense for how they work,” writes Jennifer Lin, a product management director at Google Cloud, in a blog post published today. “However, traditional access management solutions often put security at odds with flexibility by imposing one-size-fits-all, coarse-grained controls that limit users.” Google is responding mostly to two trends here. The first is that hacks are always getting more sophisticated, with new workarounds like SIM hacking to bypass SMS-based two-factor authentication and advanced forms of phishing that compromise login details. The other is that, with cloud and mobile access to remote systems, modern work is increasingly being spread out across the globe as employees are accessing software on any number of different devices in any number of different locations and network environments. To improve security, it makes sense that Google would want to give system admins a bit more control over the conditions under which a user can access a secure system that contains sensitive data. Right now, context-aware access is reserved for select G Suite customers who are using the company’s VPC Service Controls. Access for customers using Cloud Identity and Access Management (IAM), Cloud Identity-Aware Proxy (IAP), and Cloud Identity is coming soon.","Google is launching a new form of secure login for business customers it’s calling context-aware access. Instead of relying solely on users to appropriately set up two-factor authentication and manage secure passwords, context-aware access will let system adm…",2018-07-25T16:57:26Z,"{'id': 'the-verge', 'name': 'The Verge'}",Google’s new secure login tool for businesses will check for context clues,https://www.theverge.com/2018/7/25/17613376/google-context-aware-access-login-security,https://cdn.vox-cdn.com/thumbor/zecX0cPldoJGxd8xivCIx07sXlA=/0x146:2040x1214/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/10745895/acastro_180427_1777_0001.jpg,the-verge,The Verge,cloud,1
Brendan Hesse,"We might be in the era of cloud streaming and subscription services, but media ownership (both digital and physical) is still alive and well. That said, being able to consolidate and stream your collection of movies, music, and other media from one place is far more convenient than having a fragmented library across multiple hard drives, devices, and stacks of physical copies.
Thats where having a Plex media server comes in handy. Plex has been around for a while, but if youre unfamiliar, the service can turn just about any device with enough power and storage into a network-connected media server that you can access remotely. You can even share access with friends and family to create your own private media network.
That may sound like a complicated project, but if you have the right tools, its actually quite easy. Weve put together this guide to help with major steps and decisions involved. Well be focusing the three aspects of your media server setup: the server device, your media files, and the various apps and add-ons available to customize your experience.
Picking a server device
There are a surprising number of devices that will work as a Plex media server, but picking the right one for your envisioned setup will require more consideration than simply plugging an external hard drive into your PC and calling it good.
Ideally, your server will be turned on and connected to your home network 24/7, so look for devices/components designed to be always-on (we detail several below). Similarly, youll need to think about horsepower. For those looking to extend access to multiple users, youll need beefier equipment since more people accessing the server requires more processing power and network bandwidth. This becomes especially crucial if you plan on streaming 4K UHD and HDR video content or Hi-Fi audio files. However, more power will require both more expensive devices/parts, but also require more power draw in order to run. On the other hand, you may not want to have a super-powerful device as your server.
As for network demands, we recommend you always connect your server to your home network via Ethernet, when possible. If youre connecting over WiFi, youll need to use at least an 802.11n router, or ideally and 802.11ac router.
For more information on Plex server requirements, refer to Plexs support page.
NAS
Networked-attached storage (NAS) is your simplest option hardware-wise, but also the weakest. The smaller horsepower has its benefits though, since NASs have low power consumption requirements and can stay on 24/7. If you buy a pre-made NAS, there will be minimal setup involved, though you can build your own if you want more control over the process. The tradeoff is that DIY NAS servers require more complicated software setup since they do not have graphical user interfaces by default, while the off-the-shelf devices often have the software pre-installed.
Mini PC
While more expensive than a NAS, buying or building a mini PC is the next easiest option, and is probably the median sweet spot between cost, power, and ease of use for most people. For pre-made options, there are tons of NUC builds available online, while those who are comfortable building their own could look into projects using a Raspberry Pi.
Tower PC 
Whether you buy a pre-built PC or build a beefy high-end rig yourself, this is going to be the most expensive option and will require the largest power draw. However, this is going to be best for multi-user support and transcoding large files (like 4K video or Hi-Fi audio).
You also dont have to build a new PC. It is technically possible to use your current PC, or even an old one you have sitting around. Just remember that the server needs to be on and connected to the network to be accessible, and that system resources will be taken up by media playback. Because of this, wed only recommend using your everyday PC if youre the only person who will be accessing the files and can ensure the PC will be on when you want to connect remotely.
Plex supports Windows, macOS, and Linux PCs.
Other devices
Since Plexs media server software can be run on a plethora of devices and operating systems, there are a handful of other devices that can be configured to be a PMS despite that not being its primary use case.
For example, in our testing, we used an Nvidia Shield Pro as our server. There are some limitations to using the Shield, but it was the most accessible option for us since the Plex media server software comes pre-installed on the Shield Pro, and its 500GBs of internal storage can hold an appreciable portion of any media library on its own.
The Plex download page includes a full list of the supported operating systems and devices.
Picking a hard drive
The second part of the server build is selecting the hard drive youll be storing your media on. A few of the options above can use internal HDDs as the media drive, but we recommend using an external, NAS-certified HDD with USB 3.0 support, its own housing, andif possibleits own power supply.
We used a 500GB external drive coupled with the Nvidia Shield Pros internal 500GB storage, for a 1TB server, and had more than enough space for a large music collection and a dozen or so Blu-ray quality movies, but you could easily core a hard drive with four or five times as much storage.
Setting up the server
Prepare your server device. If youre building your own, be sure to follow the guides linked above, which will take you through any hardware assembly or software installation required, and help you connect your server to your network. If youre using a pre-built device, set it up like you would any other PC, turn it on, and follow the first-use instructions.
Once youve connected your server to your home network, download and Install plex media server for your device from Plexs downloads page. If youre using an Android/Android TV device as your media server, you can snag the Plex Media Server Android beta software from Google.
Make a Plex account if you havent already (the Android version of Plex uses your Google account).
Install the Plex app on the device(s) youll be accessing the server from. This is a different application from the media server software and is how youll connect to your server and watch your media. Its available on numerous Smart TVs, Roku and other streaming boxes, plus Windows, Mac, Linux, iOS, Android, Android TV, Xbox One, PlayStation 4, and various other devices.
Transferring media
With the server set up, now its time to move your music, pictures, videos, and any other media onto the server. If youre using an external HDD as the media drive and the media youre looking to move is on a PC, simply plug in the HDD and move the files manually. You can also move the files wirelessly from one device to another by using an FTP client.
If youre looking to back up your physical media collections, you can rip audio CDs using virtually any desktop CD or DVD drive, and you can rip movies as long as you have the correct software and a DVD or Blu-ray drive on your PC. MakeMKV is an excellent choice for ripping movies.
Other guides may recommend using applications like BitTorrent clients or similar software to download media, but this can be a legal quagmire if youre not careful, so be smart, courteous, and legal here. Follow the copyright laws of your region and dont go pirating or distributing content illegally. In the next section, well point out several legal (and in some cases free) ways to watch and record content and bolster your servers library.
Apps, add-ons, and extra Plex features
All Plex users have free access to several cool features that can pad out your library and consolidate other apps/services to make your server an all-in-one streaming solution.
New and Web TV
Plex provides users with free live and curated news streams from various sources, including ABC, AP, Reuters, and more. In addition to this free news content, Plex also includes a free curated library of popular Web series from outlets like Pitchfork and the New Yorker.
Podcasts 
Instead of using your smartphones storage to download podcasts, you can download, store, and stream podcasts from your Plex server. Users can search for and subscribe to shows directly in the Plex app, and the software will auto-update and download the latest episodes when theyre available.
VR mode
If you have a compatible VR headset, you can watch or listen to all your media in VR mode.
Plex Pass and free OTA TV
Plexs basic server functions are free, but the service offers a premium Plex Pass membership that adds tons of perks like metrics tracking and bandwidth stats, better metadata matching for your files, and perks like discounts towards Tidal music subscriptions. Luckily, the Plex Pass subscription is quite affordable compared to other media services and offers a flexible trio of pricing options: either $4.99 monthly, $39.99 annually, or a one-time $119.99 purchase for lifetime access. (Plex also offers a free 30-day trial, if youre curious).
Plex Pass users can also snatch free HDTV signal right from the atmosphere using an HD OTA antenna. Youll even be able to record shows as they air. Plexs interface will even auto-fill-in programming information to create a guide/schedule. We used a Mohu ReLeaf and an HD Home Run to create a makeshift multi-user TV service out of our Plex server without needing cable or a web TV service.
Youre not going to get the breadth of channels, on-demand content, or extra features of cable TV or OTT live TV services like Sling TV, but local channels, sports, and even a handful of cable channels will likely be available in your area. You can check out what channels are available using this handy online tool.","We might be in the era of cloud streaming and subscription services, but media ownership (both digital and physical) is still alive and well. That said, being able to consolidate and stream your collection of movies, music, and other media from one place is f…",2019-03-04T18:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",Use a Plex Media Server to Stream Your Media Collection Anywhere,https://lifehacker.com/use-a-plex-media-server-to-stream-your-media-collection-1832992483,"https://i.kinja-img.com/gawker-media/image/upload/s--7un7TqP8--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/bfokoadlpeay70c4vjtv.jpg",,Lifehacker.com,cloud,1
Lucas Matney,"In January, Microsoft acquired 3D optimization service Simplygon for an undisclosed sum. Today, we’re seeing the fruits of that buy with the launch of Simplygon Cloud, which the company hopes will help game developers integrate the optimization tool into their workflows to make content more friendly to resource-intensive AR and VR platforms. The tool, launching on the Azure Marketplace, works in reducing polygon complexity for 3D objects so that they’re easier to render when compute resources are in high-demand. Simplygon takes the couch on the left (584k polygons) and makes the one on the right with just 5k polygons This past week, Google announced their Poly API, which makes it easier for game developers to import 3D assets into what they’re working on. Google has largely been focusing on stylized low-poly objects for the most part as they look to make objects more friendly to the GPU restrictions of their Daydream VR platform. Simplygon Cloud is operating in a similar vein for Microsoft, the company launch its Mixed Reality platform this fall with a bunch of VR headset makers, and resource constraints are now a much realer thing to Windows game developers in VR (and AR with HoloLens). As the name implies, what Microsoft is now doing is sending this capability to the cloud, so that assets can be delivered across differently specced platforms with unique cloud-based 3D pipelines for object libraries. What makes Simplygon a more attractive option compared to competitors is that it’s obviously easier to reduce complexity than increase it so by allowing game devs to focus on building high-detail 3D objects and allowing Simplygon Cloud to minimize them for the appropriate platform, developers won’t have to rebuild objects as compute capabilities increase over time. Simplygon Cloud is available in the Azure Marketplace now.","In January, Microsoft acquired 3D optimization service Simplygon for an undisclosed sum. Today, we’re seeing the fruits of that buy with the launch of Simplygon Cloud, which the company hopes will help game developers integrate the optimization tool into thei…",2017-12-07T17:13:35Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Microsoft introduces Simplygon Cloud 3D object optimization for AR and VR,https://techcrunch.com/2017/12/07/microsoft-introduces-simplygon-cloud-3d-object-optimization-service/,https://tctechcrunch2011.files.wordpress.com/2015/10/microsoft-hololens-01.jpg,techcrunch,TechCrunch,cloud,1
Steve Dent,"The ER-2 has been scanning the blazes with a couple of interesting instruments that have flown, or will fly aboard the international space station (ISS). One of them is the AVIRIS spectrometer that can penetrate cloud, dust and smoke to see the ground below. While providing a clear image of the ground, it can also measure fine details in vegetation like water content and plant species growing. Eventually, a similar instrument will be launched into space. On the flight pictured above, however, it's carrying another instrument, the Cloud-Aerosol Multi-Angle Lidar (CAMAL). It was originally developed to a validate space-based version of the instrument called CATS, which operated for 33 months aboard the ISS, before going out of service last month. Now, CAMAL is being used for a similar purpose aboard the ER-2. Unlike other types of LiDAR used to scan the ground, CAMAL can study pollution, smoke, clouds and other atmospheric phenomena. Another view of the Thomas Fires from space Ideally, NASA flies the AVIRIS spectrometer over regions before a fire starts to get a base measurement, then overflies the same spot again afterwards. Comparing the before and after images gives researchers an idea about the severity of a fire. Meanwhile, when the blaze is active, the area can be scanned with the CAMAL LiDAR to get a picture of the dust, smoke and cloud cover in the area. Using the space-based version of the instrument, for instance, NASA scanned the October wildfires, finding plumes extending as high as 2-3 miles that created ""the worst air quality ever recorded in many parts of the Bay Area,"" NASA's CATS team said. ""The vision is that these types of measurements could be available from space in the next decade,"" said JPL's Rob Green. ""The resulting information would then be used to develop fuel maps in advance that could be used to make better predictions about where you could mitigate risk by clearing brush and trees."" CAMAL can also be used by researchers to study cloud formations and learn more about climate change, which is helping fuel the wildfires in the first place.","For the second time this year, swaths of California are burning out of control thanks to unseasonably warm and dry temperatures. To better study what's happening and assess the environmental impact, NASA deployed its high-altitude ER-2 aircraft with a host of…",2017-12-11T16:03:00Z,"{'id': 'engadget', 'name': 'Engadget'}",NASA's high altitude ER-2 scans California's wildfires,https://www.engadget.com/2017/12/11/nasa-er-2-lidar-california-fires-the-big-picture/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fs.aolcdn.com%2Fhss%2Fstorage%2Fmidas%2F8928ef0655424ce4cdd0155d17652bf6%2F205939706%2Fcalifornia-thomas-fire-lidar-2017-12-11-02.jpg&client=cbc79c14efcebee57402&signature=4078a440ef209ce4333ec09fb8ea87f695947c0a,engadget,Engadget,cloud,1
Ingrid Lunden,"After spinning out as a standalone security business from Intel earlier this year, McAfee has made its first acquisition. The company has acquired Skyhigh Networks, a specialist in cloud security, the companies announced today. The financial terms of the deal have not been disclosed, but here are a few data points: Skyhigh had raised over $106 million in funding, according to Crunchbase, most recently a Series D round a year ago, with its investors including Sequoia, Greylock and Salesforce. PitchBook, meanwhile, puts its most recent funding round at $400 million, one marker for the potential value of this deal. The deal is a sign of the ongoing trend for consolidation in the security industry, where smaller players are coming together under larger businesses to provide more security services under one roof. This makes sense on a couple of levels. For one, the issue of cybersecurity has become one of the most persistent in the market today, with malicious hacking a nightmare not just for businesses but individuals as more and more of our personal and not-so-personal information becoming digitised and moving to the cloud, putting it in the reach of both criminals and destructive pranksters. In the case of Skyhigh, McAfee — whose legacy business is in endpoint security — is specifically acquiring the company for that cloud expertise. Skyhigh CEO Rajiv Gupta will head McAfee’s cloud business unit. The other is the nature of how security services is evolving: we’re seeing a big shift to the use of data analytics and machine learning and other kinds of AI to be able to identify, track and stop cybercrime. Bringing together different services that can use and improve the bigger data pool makes all of those services stronger, potentially. “Skyhigh Networks had the foresight five years ago to realize that cybersecurity for cloud environments could not be an impediment to, or afterthought of, cloud adoption,” Chris Young, CEO of McAfee, said in a statement. “They pioneered an entirely new product category called cloud access security broker (CASB) that analysts describe as one of the fastest growing areas of information security investments of the last five years – where Skyhigh continues to innovate and lead. Skyhigh’s leadership in cloud security, combined with McAfee’s security portfolio strength, will set the company apart in helping organizations operate freely and securely to reach their full potential.” It’s not clear if Skyhigh was profitable, and where it stood on its funding, but this will be coupled with more investment into its business by McAfee, which was valued at $4.2 billion at the time of its spinout from Intel in April. “Becoming part of McAfee is the ideal next step in realizing Skyhigh Networks’ vision of not simply making the cloud secure, but making it the most secure environment for business,” Gupta said in a statement. “McAfee will provide global scale to further accelerate Skyhigh’s growth, with the combined company providing leading technologies and solutions across cloud and endpoint security – categories Skyhigh and McAfee respectively helped create, and the two architectural control points for enterprise security.” Skyhigh Networks already has produces in the areas of SaaS, PaaS and IaaS and a range of cloud-based security services around policy control both for apps in the cloud and on premises. We should expect to see more of that now being marketed to McAfee’s current roster of customers. The deal is expected to close pending regulatory approvals and other closing conditions. Featured Image: Blue Island / Shutterstock","After spinning out as a standalone security business from Intel earlier this year, McAfee has made its first acquisition. The company has acquired Skyhigh Networks, a specialist in cloud security, the companies announced today. The financial terms of the deal…",2017-11-27T15:13:58Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","McAfee acquires cloud security startup Skyhigh Networks, last valued at $400M",https://techcrunch.com/2017/11/27/mcafee-skyhigh-networks/,https://tctechcrunch2011.files.wordpress.com/2015/10/identity-security.png,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Over the course of the last few years, Cloudflare built a global network of data center locations and partnerships to expand its DDoS protection, security tools and website acceleration services. That kind of expertise is hard to beat, so maybe it doesn’t come as a surprise that even a global technology juggernaut like IBM today announced that it has decided to partner with Cloudflare to offer these services to its customers instead of building a similar product itself. IBM’s new Cloud Internet Services offering, which the company announced today, is powered by Cloudflare and will offer all of that services features for protecting sites and speed them up. IBM’s CTO for Watson and Cloud Bryson Koehler told me that IBM Cloud users will be able to turn these features on with a single click. “ Cloudflare has done a great job of building a set of world-class tool that are easy to use and that have been built against the same standards that we hold our internal teams to,” he said. “In today’s fast changing world of constantly evolving services, we are always making built vs. partner decisions and when you really get into the world of caching and load balancing, […] they have done the work of establishing partnerships.” In addition to this partnership, IBM also announce two additional new security features today: the IBM Cloud Security Advisor and new features for IBM Cloud App ID. The Cloud Security Advisor gives developers and operational teams more and deeper insights into their security posture. Some of these are basic insights like alerting you to the fact that your web server security certificate is about to expire while other features go deeper and alert ops teams of emerging threats that IBM is seeing in its global network and that might affect your application and its data. The tool is also sophisticated enough to ensure that developers who have to stick to certain regulations for managing data don’t accidentally load data from a PCI or HIPAA compliant service and then write it to one that isn’t compliant. The Cloud Security Advisor, which is officially still an experimental product, takes all of this data and presents it in a single dashboard. As for App ID, the idea here is to ensure that users who access a given app or data are indeed authorized to do so. That’s not a new feature, but IBM is now extending this service to containers and the IBM Cloud Container Service as well.","Over the course of the last few years, Cloudflare built a global network of data center locations and partnerships to expand its DDoS protection, security tools and website acceleration services. That kind of expertise is hard to beat, so maybe it doesn’t com…",2018-03-14T11:00:56Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",IBM partners with Cloudflare to launch new security and DDoS protection features,http://techcrunch.com/2018/03/14/ibm-partners-with-cloudflare-to-launch-new-security-and-ddos-protection-features/,https://techcrunch.com/wp-content/uploads/2018/03/gettyimages-527012802.jpg?w=599,techcrunch,TechCrunch,cloud,1
PETER BAKER,"But the House, controlled by Democrats, has the power under the Constitution to decide for itself whether the presidents actions constituted high crimes and misdemeanors that justify impeachment, and it could interpret Mr. Muellers evidence the other way once it sees it.
The next phase of the story, then, will be the fight by House Democrats to force Mr. Barr to turn over Mr. Muellers full report and accompanying evidence, a constitutional battle that could ultimately be resolved in the courts.
Until they read the report for themselves, Democrats are hardly going to agree that the president has been cleared. And they will most likely summon Mr. Mueller to testify, which could provide a public airing of Mr. Trumps actions that, even if not rising to a crime, may not reflect well on the president.
Still, Speaker Nancy Pelosi has already said that she did not favor impeachment unless the evidence was so compelling and overwhelming and bipartisan, a standard that seems even less likely to be met now.
Either way, Mr. Muellers investigation has taken its toll on this presidency, leading to indictments, convictions or guilty pleas for a half-dozen of Mr. Trumps associates, including his campaign chairman and national security adviser, and spawning offshoot investigations. In any other administration, that record alone would be enough to seriously damage a president.","The end of the special counsel’s investigation without findings of collusion with Russia fortified the president for the battles to come, including his campaign for re-election.",2019-03-25T01:19:55Z,"{'id': 'the-new-york-times', 'name': 'The New York Times'}",News Analysis: A Cloud Over Trump’s Presidency Is Lifted,https://www.nytimes.com/2019/03/24/us/politics/trump-robert-mueller.html,https://static01.nyt.com/images/2019/03/24/us/24dc-asses/merlin_152577555_e48fa37e-961a-4f29-9f51-a8e1d161559d-facebookJumbo.jpg,the-new-york-times,The New York Times,cloud,1
Frederic Lardinois,"Google’s Tensor Processing Units (TPUs), the company’s custom chips for running machine learning workloads written for its TensorFlow framework, are now available to developers. The promise of these Google-designed chips is that they can run specific machine learning workflows significantly faster than the standard GPUs that most developers use today. For Google, one of the advantages of these TPUs is that they also use less power, something developers probably don’t care quite as much about, but that allows Google to offer this service at a lower cost. The company first announced Cloud TPUs at its I/O developer conference nine months ago (and gave access to them to a limited number of developers and researchers). Each Cloud TPU features four custom ASICs with 64 GB of high-bandwidth memory. According to Google, the peak performance of a single TPU board is 180 teraflops. Developers who already use TensorFlow don’t have to make any major changes to their code to use this service. For the time being, though, Cloud TPUs aren’t quite available at a click of a button, though. “To manage access,” as Google says, developers have to request a Cloud TPU quota and describe what they want to do with the service. Once they get in, usage will be billed at $6.50 per Cloud TPU and hour. In comparison, access to standard Tesla P100 GPUs in the U.S. runs at $1.46 per hour, though the maximum performance here is about 21 teraflops of FP16 performance. Google’s reputation for machine learning will surely drive a lot of new users to these Cloud TPUs. In the long run, though, what’s maybe just as important is that this gives the Google Cloud a way to differentiate itself from the AWS’s and Azure’s of this world. For the most part, after all, everybody now offers the same set of basic cloud computing services and the advent of containers has made it easier than every to move workloads from one platform to another. With the combination of TensorFlow and TPUs, Google can now offer a service that few will be able to match in the short term.","Google’s Tensor Processing Units (TPUs), the company’s custom chips for running machine learning workloads written for its TensorFlow framework, are now available to developers. The promise of these Google-designed chips is that they can run specific machine …",2018-02-12T16:53:32Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google’s custom TPU machine learning accelerators are now available in beta,http://techcrunch.com/2018/02/12/googles-custom-tpu-machine-learning-accelerators-are-now-available-in-beta/,https://tctechcrunch2011.files.wordpress.com/2017/05/google-io-2017-0113.jpg,techcrunch,TechCrunch,cloud,1
Ron Miller,"Diane Greene announced on Friday that she was stepping down after three years running Google’s cloud business. She will stay on until the first of the year to help her successor, Thomas Kurian in the transition. He left Oracle at the end of September after more than 20 years with the company, and is charged with making Google’s cloud division more enterprise-friendly, a goal that has oddly eluded the company. Greene was brought on board in 2015 to bring some order and enterprise savvy to the company’s cloud business. While she did help move them along that path, and grew the cloud business, it simply hasn’t been enough. There have been rumblings for months that Greene’s time was coming to an end. So the torch is being passed to Kurian, a man who spent over two decades at a company that might be the exact opposite of Google. He ran product at Oracle, a traditional enterprise software company. Oracle itself has struggled to make the transition to a cloud company, but Bloomberg reported in September that one of the reasons Kurian was taking a leave of absence at the time was a difference of opinion with Chairman Larry Ellison over cloud strategy. According to the report, Kurian wanted to make Oracle’s software available on public clouds like AWS and Azure (and Google Cloud). Ellison apparently didn’t agree and a couple of weeks later Kurian announced he was moving on. Even though Kurian’s background might not seem to be perfectly aligned with Google, it’s important to keep in mind that his thinking was evolving. He was also in charge of thousands of products and helped champion Oracle’s move to the cloud. He has experience successfully nurturing products enterprises have wanted, and perhaps that’s the kind of knowledge Google was looking for in its next cloud leader. Ray Wang, founder and principal analyst at Constellation Research says Google still needs to learn to support the enterprise, and he believes Kurian is the right person to help the company get there. “Kurian knows what’s required to make a cloud company work for enterprise customers,” Wang said. If he’s right, perhaps an old-school enterprise executive is just what Google requires to turn its Cloud division into an enterprise-friendly powerhouse. Greene has always maintained that it was still early days for the cloud and Google had plenty of time to capture part of the untapped market, a point she reiterated in her blog post on Friday. “The cloud space is early and there is an enormous opportunity ahead,” she wrote. She may be right about that, but marketshare positions seem to be hardening. AWS, which was first to market, has an enormous marketshare lead with over 30 percent by most accounts. Microsoft is the only company with the market strength at the moment to give them a run for their money and the only other company with double digit market share numbers. In fact, Amazon has a larger marketshare than the next four companies combined, according to data from Synergy Research. While Google is always mentioned in the Big 3 cloud companies with AWS and Microsoft, with around $4 billion revenue a year, it has a long way to go to get to the level of these other companies. Despite Greene’s assertions, time could be running out to make a run. Perhaps Kurian is the person to push the company to grab some of that untapped market as companies move more workloads to the cloud. At this point, Google is counting on him to do just that.","Diane Greene announced on Friday that she was stepping down after three years running Google’s cloud business. She will stay on until the first of the year to help her successor, Thomas Kurian in the transition. He left Oracle at the end of September after mo…",2018-11-18T17:18:52Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google looks to former Oracle exec Thomas Kurian to move cloud business along,http://techcrunch.com/2018/11/18/google-looks-to-former-oracle-exec-thomas-kurian-to-move-cloud-business-along/,https://techcrunch.com/wp-content/uploads/2018/11/GettyImages-181747340.jpg?w=601,techcrunch,TechCrunch,cloud,1
Bryan Menegus,"Tensions are high within Microsoft, as new scrutiny is given to a partnership between the company’s Azure Government cloud computing arm and U.S. Immigration and Customs Enforcement (ICE), according to several Microsoft employees who spoke to Gizmodo on the condition of anonymity. Two were considering leaving the company based on the response. The partnership was first made public in late January, where Microsoft announced it was “proud to support” the agency’s efforts—but given the size of the company, many employees were not even aware any such agreement was in place until recently. A likely catalyst is the recent revelations that ICE separates asylum-seeking families and confines children in cages. In response, the announcement post was “briefly deleted [...] after seeing commentary in social media,” according to a Microsoft spokesperson who refused to divulge the specific nature of the Azure/ICE partner arrangement. “This was a mistake and as soon as it was noticed the blog was reverted to previous language.” Internally, as news of the contract spread, employees expressed their dissent. “This is the sort of thing that would make me question staying,” one employee told Gizmodo. Another echoed, “I’ll seriously consider leaving if I’m not happy with how they handle this.” Microsoft condemned family separation by ICE in a statement to Gizmodo but declined to specify if specific tools within Azure Government, like Face API—facial recognition software—were in use by the agency. The company also did not comment on whether it had assisted in building artificial intelligence tools for ICE, something the agency has been seeking (and courting Microsoft over) for some time. “My sense is that the government cloud group is very much a sales/consulting group, so it’s definitely plausible they could have been working on something specific, but if so then it would likely have been helping them customize existing public product tech,” a current Microsoft employee told Gizmodo. The possibility of Microsoft providing cheap, efficient facial recognition software to ICE comes less than a month after the ACLU discovered Amazon had given law enforcement agencies access to its similar in-house tool, Rekognition, and several months after Gizmodo first revealed Google had agreed to assist in Project Maven, a program to help develop artificial intelligence for drone footage analysis for the Pentagon. Microsoft told Gizmodo it was “dismayed” by ICE’s actions and that it “urge[d] the administration to change its policy and Congress to pass legislation ensuring children are no longer separated from their families.” Absent from its statement was whether it would continue to provide its cloud services to ICE. As more employees become aware of the agreement—and the recent activities of ICE—it remains to be seen what response these internal frustrations will draw from Microsoft’s leadership. Do you work for Microsoft and have thoughts or information on this ICE partnership? Get in touch via email to get my Signal number, chat with me on Keybase, or send documents anonymously to our Secure Drop server.","Tensions are high within Microsoft, as new scrutiny is given to a partnership between the company’s Azure Government cloud computing arm and U.S. Immigration and Customs Enforcement (ICE), according to several Microsoft employees who spoke to Gizmodo on the c…",2018-06-18T19:59:00Z,"{'id': None, 'name': 'Gizmodo.com'}",Microsoft Employees Up in Arms Over Cloud Contract With ICE,https://gizmodo.com/microsoft-employees-up-in-arms-over-cloud-contract-with-1826927803,"https://i.kinja-img.com/gawker-media/image/upload/s--8C6BN2Fs--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/hmxy23igudxwrkw0lsje.jpg",,Gizmodo.com,cloud,1
Taylor Hatmaker,"As the House advances a 2,232 page spending bill meant to avert a government shutdown, privacy advocates and big tech companies aren’t seeing eye to eye about a small piece of legislation tucked away on page 2,212. The Clarifying Lawful Overseas Use of Data Act, a.k.a. the CLOUD Act ( H.R.4943, S.2383 ) aims to simplify the way that international law enforcement groups obtain personal data stored by US-based tech platforms, but the changes to that process are controversial. As it stands, if a foreign government wants to obtain that data in the course of an investigation, a series of steps are necessary. First, that government must have a Mutual Legal Assistant Treaty (MLAT) with the U.S. government in place, and those treaties are ratified by the Senate. Then it can send a request to the U.S. Department of Justice, but first the DOJ needs to seek approval from a judge. After those requirements are met, the request can move along to the tech company hosting the data that the foreign government is seeking. The debate around the CLOUD Act also taps into tech company concerns that foreign nations may move to pass laws in favor of data localization, or the process of storing users’ personal data within the borders of the country they are a citizen of. That trend would prove both costly for cloud data giants and difficult, upending the established model of cloud data storage that optimizes for efficiency rather than carefully sorting out what data is stored within the borders of which country. In a February 6 letter, Microsoft, Apple, Google, Facebook and Oath (TechCrunch’s parent company) co-authored a letter calling the CLOUD Act “notable progress to protect consumers’ rights.” In a late February blog post, Microsoft Chief Legal Officer Brad Smith addressed the issue. “The CLOUD Act creates both the incentive and the framework for governments to sit down and negotiate modern bi-lateral agreements that will define how law enforcement agencies can access data across borders to investigate crimes,” Smith wrote. “It ensures these agreements have appropriate protections for privacy and human rights and gives the technology companies that host customer data new statutory rights to stand up for the privacy rights of their customers around the world.” Today is an important day for privacy rights around the world, for international relations, and for building trust in the technology we all rely on every day. pic.twitter.com/9afiFXmzGn — Brad Smith (@BradSmi) March 22, 2018 In a recent opinion piece, ACLU legislative counsel Neema Singh Guliani argues that the CLOUD Act sidesteps oversight from both the legislative and judicial branches, granting the attorney general and the state department too much discretion in choosing which governments the U.S. will enter into a data exchange agreement with. The Center for Democracy and Technology also opposes the CLOUD Act on the grounds that it fails to protect the digital privacy of American citizens and the Electronic Frontier Foundation dismissed the legislation as “a new backdoor around the Fourth Amendment.” The Open Technology Institute also denounced the CLOUD Act’s provision to “allow qualifying foreign governments to enter into an executive agreement to bypass the human rights protective Mutual Legal Assistance Treaty (MLAT) process when seeking data in criminal investigations and to seek data directly from U.S. technology companies.” Both organizations acknowledge that improvements to the bill do partially address some of the human rights concerns associated with not requiring a MLAT in a data sharing agreement. “While this version of the CLOUD Act includes some new safeguards, it is still woefully inadequate to protect individual rights,” OTI Director of Surveillance &amp; Cybersecurity Policy Sharon Bradford Franklin said of the changes. “Critically, the bill still would permit foreign governments to obtain communications data held in the United States without any prior judicial review, and it would allow foreign governments to obtain U.S.-held communications in real time without applying the safeguards required for wiretapping by the U.S. government. ” The Consumer Technology Association voiced its support of the altered bill in a press release issued Thursday. “CTA thanks the House of Representatives for taking steps to empower America’s digital infrastructure for the 21st century. The inclusion of the CLOUD Act and RAY BAUM’S Act in today’s legislation ensures Americans can safely create, share and collect electronic data while providing them the resources to do so.” While some changes made aspects of the bill more palatable to digital privacy watchdogs, some are objecting to the choice to tack it onto the omnibus spending bill. Oregon Senator Ron Wyden and Kentucky Senator Rand Paul spoke out Thursday against passing the CLOUD Act by attaching it to the spending bill. “Tucked away in the omnibus spending bill is a provision that allows Trump, and any future president, to share Americans’ private emails and other information with countries he personally likes. That means he can strike deals with Russia or Turkey with nearly zero congressional involvement and no oversight by U.S. courts,” Wyden said. “This bill contains only toothless provisions on human rights that Trump’s cronies can meet by merely checking a box. It is legislative malpractice that Congress, without a minute of Senate debate, is rushing through the CLOUD Act on this must-pass spending bill.” But guess what? Congress can’t vote to reject the CLOUD Act, because it just got stuck onto the Omnibus, with no prior legislative action or review. https://t.co/8b6W08goXm — Senator Rand Paul (@RandPaul) March 22, 2018 While the content of the CLOUD Act has evolved away from controversy with some modifications, the choice to pass it as part of the omnibus plan without further opportunity for public debate to examine its potential far-reaching implications is proving just as controversial as earlier forms of the legislation.","As the House advances a 2,232 page spending bill meant to avert a government shutdown, privacy advocates and big tech companies aren’t seeing eye to eye about a small piece of legislation tucked away on page 2,212. The Clarifying Lawful Overseas Use of Data A…",2018-03-23T00:06:11Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","As the CLOUD Act sneaks into the omnibus, big tech butts heads with privacy advocates",http://techcrunch.com/2018/03/22/cloud-act-omnibus-bill-house/,https://techcrunch.com/wp-content/uploads/2018/03/gettyimages-881828548-1.jpg?w=600,techcrunch,TechCrunch,cloud,1
Ron Miller,"Rescale, the startup that wants to bring high performance computing to the cloud, announced a $32 million Series B investment today led by Initialized Capital, Keen Venture Partners and SineWave Ventures. They join a list of well-known early investors that included Sam Altman, Jeff Bezos, Richard Branson, Paul Graham, Ron Conway, Chris Dixon, Peter Thiel and others. Today’s investment brings the total amount raised to $52 million, according to the company. Rescale works with engineering, aerospace, scientific and other verticals and helps them move their legacy high performance computing applications to the cloud. The idea is to provide a set of high performance computing resources, whether that’s on prem or in the cloud, and help customers tune their applications to get the maximum performance. Traditionally HPC has taken place on prem in a company’s data center. These companies often have key legacy applications they want to move to the cloud and Rescale can help them do that in the most efficient manner, whether that involves bare metal a virtual machine or a container. “We help take a portfolio of [legacy] applications running on prem and help enable them in the cloud or in a hybrid environment. We tune and optimize the applications on our platform and take advantage of capital assets on prem, then we help extend that environment to different cloud vendors or tune to best practices for the specific application,” company CEO and co-founder Joris Poort explained. Photo: Rescale Ben Verwaayen, who is a partner at one of the lead investors, Keen Venture Partners, sees a company going after a large legacy market with a new approach. “The market is currently 95% on-premise, and Rescale supports customers as they move to hybrid and eventually to a fully cloud native solution. Rescale helps CIOs enable the digital transformation journey within their enterprise, to optimize IT resources and enable meaningful productivity and cost improvements,” Verwaayen said in a statement. The new influx of cash should help Rescale, well, scale, and that will involve hiring more developers, solutions architects and the like. The company wants to also use the money to expand its presence in Asia and Europe and establish relationships with systems integrators, who would be a good fit for a product like this and help expand their market beyond what they can do as a young startup. The company, which is based in San Francisco, was founded in 2011 and has 80 employees. They currently have 150 customers including Sikorsky Innovation, Boom Aerospace and Trek Bikes.","Rescale, the startup that wants to bring high performance computing to the cloud, announced a $32 million Series B investment today led by Initialized Capital, Keen Venture Partners and SineWave Ventures. They join a list of well-known early investors that in…",2018-07-24T13:00:14Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Rescale reels in $32 million Series B to bring HPC to cloud,http://techcrunch.com/2018/07/24/rescale-reels-in-32-million-series-b-to-bring-hpc-to-cloud/,https://techcrunch.com/wp-content/uploads/2018/07/GettyImages-622242270.jpg?w=711,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Mirantis co-founder Adrian Ionel left his CEO role in October 2015, at a time when the company was still solely focused on OpenStack. Now, he is coming back to reprise his role as the company’s CEO at a time when Mirantis is looking beyond OpenStack and toward the still nascent cloud native ecosystem around Kubernetes for its next big business opportunity. Mirantis’ current CEO Alex Freedland will remain a board member. Like other companies that made an early bet on OpenStack, Mirantis, which once described itself as “the pure play OpenStack company,” went through a series of ups and downs (it now also describes itself as the “#1 pure play open cloud company”). But just like OpenStack has now found its place, Mirantis weathered these storms as an independent company and is now able to take what it learned in the last few years in delivering OpenStack to a wider swath of products. “The company needs to find its path forward, leveraging the present — the enormous assets we have in the cloud infrastructure business — and articulate a compelling and exciting and engaging future vision that brings us into the future” Ionel told me. “That’s quite similar to where we were in 2010.” For now, Mirantis’ foundation will remain its private cloud business, where it will continue to offer its managed cloud solution for OpenStack. “Private cloud is still a very large market and we have a very compelling answer,” Ionel said. And with companies like AT&amp;T (which runs a 10,000 node OpenStack cluster with the help of Mirantis), the Shenzhen Stock Exchange, VW and others on its customer roster, this will likely remain a good part of the company’s business in the near term. But in building out its continuous delivery platform for OpenStack, Mirantis also built the foundation for the future of the company. “We believe that our open source toolbox — our unique approach to open source and the open source technology out there — are central to helping companies build and run applications on any cloud and that we will use the same playbook – the continuous delivery playbook and the open approach that made us successful in OpenStack to help companies run apps in a fully automated fashion in any cloud — public or private,” Ionel explained. Mirantis already started this journey under Freedland. With DriveTrain and its Container-as-a-Service offering, it built the kernel of the continuous delivery platform that Ionel envisions and that Mirantis needs to focus on as its customers start asking for multi-cloud solutions. Ionel also seemed very excited about the fact that this move brings Mirantis closer to the actual applications and toward application delivery in general. “We want to give customers and autopilot for applications they want to deploy,” he said. “We will invest heavily in cloud native continuous deliver that enables developers to deploy their code with full automation and ease.” Returning to Mirantis wasn’t an obvious choice for Ionel and he admitted that it took him a few months to make a decision. In part, that’s because he built another company, Dorsal, after leaving Mirantis. He also noted that he prefers to look forward in life, “so if you rejoin something, you want to think very carefully why you are doing it.” In the end, though, he decided that he would feel very much at home at Mirantis and that he would be able to help the company in this next phase.","Mirantis co-founder Adrian Ionel left his CEO role in October 2015, at a time when the company was still solely focused on OpenStack. Now, he is coming back to reprise his role as the company’s CEO at a time when Mirantis is looking beyond OpenStack and towar…",2018-01-25T17:00:36Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Adrian Ionel returns to CEO role at Mirantis as the company continues to expand beyond OpenStack,http://techcrunch.com/2018/01/25/adrian-ionel-returns-to-ceo-role-at-mirantis-as-it-continues-to-expand-beyond-openstack/,https://tctechcrunch2011.files.wordpress.com/2018/01/2018-01-25_0859.png,techcrunch,TechCrunch,cloud,1
The Google Cloud team,"In November here in the U.S., we felt some Thanksgiving gratitude that theres never a dull moment in cloud technology. Weve been keeping track of whats new and quickly evolving, from AI and ML tools to storage and databases. Here are a few of the highlights from last month in Google Cloud. Theres a new way to make a Google Doc. Heres a new, time-saving (and dare we say, fun?) way to create a Google Doc when youve got to get your ideas down on the page immediately. Type in doc.new, docs.new or document.new into your web browser and itll bring up a new Google Doc. See how it works. The New York Times uses Google Cloud to digitize its photo archive. The New York Times photo archive, nicknamed the morgue, contains more than a hundred years worth of photosfive to seven million in all. The paper built a processing pipeline using Google Cloud Platform (GCP) products to digitize, organize and easily search those photos. See some of the pictures and read more on their plans. Asia Pacific cloud users can access GCP data faster. We were excited to announce the opening of our Hong Kong region last month, and plans for the Jakarta region, to bring faster access to GCP data and apps for users. Locating your companys data closer to a cloud region means you can transmit that data faster, with lower network latency. Find your own location latency here. Non-data scientists can now experiment with AI and ML. Artificial intelligence (AI) and machine learning (ML) are hot topics in tech these daysbut how do you even start using these concepts? Our new central AI Hub is now in its first stage of availability, offering pipelines, modules, and other preconfigured ML content. Check out real-world examples of AI and ML, like using data analytics to predict health problems or predict potential hazardous driving areas in Chicago. We put forth our principles for building ethical AI. AI is a fascinating technology, full of great potential. Its also still a technology built by humans, dependent on us to input data and train models. Were considering AI principles every step of the way, and working to eliminate bias from AI models, use AI for positive results, make sure AI is interpretable by humans, and helping businesses prepare for a future with more automation built in. Find out more about how were creating AI ethics at Google. We described our microservices vision. A microservices architecture is one where discrete, single-purpose software units are the basis to build large, distributed apps that work in both hybrid and on-prem situationsespecially interesting as businesses continue to run their IT operations both in their own data centers and with cloud resources. Using container technology means developers can deploy new apps faster, and lets developers use that microservices architecture more easily. The missing piece has been a management layer. Read more on how Istio fills the gap. For all of what we covered in November, check out the Google Cloud blog.","In November here in the U.S., we felt some Thanksgiving gratitude that there’s never a dull moment in cloud technology. We’ve been keeping track of what’s new and quickly evolving, from AI and ML tools to storage and databases. Here are a few of the highlight…",2018-12-06T19:00:00Z,"{'id': None, 'name': 'Blog.google'}",Cloud covered: What was new in Google Cloud for November,https://www.blog.google/products/google-cloud/cloud-covered-what-was-new-in-google-cloud-for-november/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+blogspot%2FMKuf+%28The+Keyword+%7C+Official+Google+Blog%29,https://storage.googleapis.com/gweb-uniblog-publish-prod/images/BlogHeader_Set2_D_IdY07VY.max-1300x1300.png,,Blog.google,cloud,1
Ron Miller,"Back in 2013, Yelp was a 9-year old company built on a set of internal systems. It was coming to the realization that running its own data centers might not be the most efficient way to run a business that was continuing to scale rapidly. At the same time, the company understood that the tech world had changed dramatically from 2004 when it launched and it needed to transform the underlying technology to a more modern approach. That’s a lot to take on in one bite, but it wasn’t something that happened willy-nilly or overnight says Jason Yellen, SVP of engineering at Yelp. The vast majority of the company’s data was being processed in a massive Python repository that was getting bigger all the time. The conversation about shifting to a microservices architecture began in 2012. The company was also running the massive Yelp application inside its own datacenters, and as it grew it was increasingly becoming limited by long lead times required to procure and get new hardware online. It saw this was an unsustainable situation over the long-term and began a process of transforming from running a huge monolithic application on-premises to one built on microservices running in the cloud. It was a quite a journey. The data center conundrum Yellen described the classic scenario of a company that could benefit from a shift to the cloud. Yelp had a small operations team dedicated to setting up new machines. When engineering anticipated a new resource requirement, they had to give the operations team sufficient lead time to order new servers and get them up and running, certainly not the most efficient way to deal with a resource problem, and one that would have been easily solved by the cloud. “We kept running into a bottleneck, I was running a chunk of the search team [at the time] and I had to project capacity out to 6-9 months. Then it would take a few months to order machines and another few months to set them up,” Yellen explained. He emphasized that the team charged with getting these machines going was working hard, but there were too few people and too many demands and something had to give. “We were on this cusp. We could have scaled up that team dramatically and gotten [better] at building data centers and buying servers and doing that really fast, but we were hearing a lot of AWS and the advantages there,” Yellen explained. To the cloud! They looked at the cloud market landscape in 2013 and AWS was the clear leader technologically. That meant moving some part of their operations to EC2. Unfortunately, that exposed a new problem: how to manage this new infrastructure in the cloud. This was before the notion of cloud-native computing even existed. There was no Kubernetes. Sure, Google was operating in a cloud-native fashion in-house, but it was not really an option for most companies without a huge team of engineers. Yelp needed to explore new ways of managing operations in a hybrid cloud environment where some of the applications and data lived in the cloud and some lived in their data center. It was not an easy problem to solve in 2013 and Yelp had to be creative to make it work. That meant remaining with one foot in the public cloud and the other in a private data center. One tool that helped ease the transition was AWS Direct Connect, which was released the prior year and enabled Yelp to directly connect from their data center to the cloud. Laying the groundwork About this time, as they were figuring out how AWS works, another revolutionary technological change was occurring when Docker emerged and began mainstreaming the notion of containerization. “That’s another thing that’s been revolutionary. We could suddenly decouple the context of the running program from the machine it’s running on. Docker gives you this container, and is much lighter weight than virtualization and running full operating systems on a machine,” Yellen explained. Another thing that was happening was the emergence of the open source data center operating system called Mesos, which offered a way to treat the data center as a single pool of resources. They could apply this notion to wherever the data and applications lived. Mesos also offered a container orchestration tool called Marathon in the days before Kubernetes emerged as a popular way of dealing with this same issue. “We liked Mesos as a resource allocation framework. It abstracted away the fleet of machines. Mesos abstracts many machines and controls programs across them. Marathon holds guarantees about what containers are running where. We could stitch it all together into this clear opinionated interface,” he said. Pulling it all together While all this was happening, Yelp began exploring how to move to the cloud and use a Platform as a Service approach to the software layer. The problem was at the time they started, there wasn’t really any viable way to do this. In the buy versus build decision making that goes on in large transformations like this one, they felt they had little choice but to build that platform layer themselves. In late 2013 they began to pull together the idea of building this platform on top of Mesos and Docker, giving it the name PaaSTA, an internal joke that stood for Platform as a Service, Totally Awesome. It became simply known as Pasta. Photo: David Silverman/Getty Images The project had the ambitious goal of making their infrastructure work as a single fabric, in a cloud-native fashion before most anyone outside of Google was using that term. Pasta developed slowly with the first developer piece coming online in August 2014 and the first production service later that year in December. The company actually open sourced the technology the following year. “Pasta gave us the interface between the applications and development teams. Operations had to make sure Pasta is up and running, while Development was responsible for implementing containers that implemented the interface,” Yellen said. Moving to deeper into the public cloud While Yelp was busy building these internal systems, AWS wasn’t sitting still. It was also improving its offerings with new instance types, new functionality and better APIs and tooling. Yellen reports this helped immensely as Yelp began a more complete move to the cloud. He says there were a couple of tipping points as they moved more and more of the application to AWS — including eventually, the master database. This all happened in more recent years as they understood better how to use Pasta to control the processes wherever they lived. What’s more, he said that adoption of other AWS services was now possible due to tighter integration between the in-house data centers and AWS. Photo: erhui1979/Getty Images The first tipping point came around 2016 as all new services were configured for the cloud. He said they began to get much better at managing applications and infrastructure in AWS and their thinking shifted from how to migrate to AWS to how to operate and manage it. Perhaps the biggest step in this years-long transformation came last summer when Yelp moved its master database from its own data center to AWS. “This was the last thing we needed to move over. Otherwise it’s clean up. As of 2018, we are serving zero production traffic through physical data centers,” he said. While they still have two data centers, they are getting to the point, they have the minimum hardware required to run the network backbone. Yellen said they went from two weeks to a month to get a service up and running before this was all in place to just a couple of minutes. He says any loss of control by moving to the cloud has been easily offset by the convenience of using cloud infrastructure. “We get to focus on the things where we add value,” he said — and that’s the goal of every company.","Back in 2013, Yelp was a 9-year old company built on a set of internal systems. It was coming to the realization that running its own data centers might not be the most efficient way to run a business that was continuing to scale rapidly. At the same time, th…",2018-06-04T12:00:41Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",How Yelp (mostly) shut down its own data centers and moved to AWS,http://techcrunch.com/2018/06/04/how-yelp-mostly-shut-down-its-own-data-centers-and-moved-to-aws/,https://techcrunch.com/wp-content/uploads/2018/06/gettyimages-819173136.jpg?w=533,techcrunch,TechCrunch,cloud,1
Ron Miller,"Last week Chinese eCommerce giant Alibaba announced its its Q3 earnings. Cloud revenue was $553 million, an impressive 104 percent year-over-year increase. That comes out to a run rate in the range of $2.2 billion, well behind Google which announced it is pulling in a billion dollars a quarter and still buried behind the market leaders all of whom reported around $4 billion+ a quarter. While the growth was impressive, keep in mind when you have a small market share, it’s much easier to grow a big number than when you have a larger market share. In other words, it gets harder to grow, the larger you get. It is worth noting, however that the growth spurt allowed Alibaba to show up in the top five of Synergy Research’s most recent Cloud Infrastructure Market Share report for the first time. While the market share was only around 3 or 4 percent. it’s still significant because no longer being lumped together with “next 10” or “rest of market.” Synergy reports that the cloud market grew 46 percent in the fourth quarter, and each of the biggest cloud companies benefited over the smaller ones. “In large part the expansion was driven by aggressive growth of Amazon (AWS), Microsoft, Google and Alibaba, who all increased their share of the worldwide market at the expense of smaller cloud providers,” Synergy wrote in their report. The fact that smaller players continue to struggle in the cloud market isn’t that surprising, but it makes Alibaba’s rapid rise all the more remarkable. That Alibaba has shown up at all is a testimony to how fast it has grown. Even though Alibaba launched its cloud business in 2009, just a few years after AWS, the company really only began taking it seriously as a business in 2015 with a billion dollar investment. That didn’t seem like enough at the time, but it has taken some big leaps to get to this point so quickly. While the majority of its business is in China and Asia more broadly, that is a huge market and gives Alibaba enough lift to grow fast and move up in the market more quickly. In another report from Synergy published last year, it found to no one’s surprise that the Chinese cloud market was dominated by Chinese cloud companies. Alibaba was at the top of the cloud services market tier, which wasn’t the most lucrative, but was the fastest growing. For Alibaba to really expand its share and move up the graph, it probably needs to build a bigger market in the west, particularly in the US. It currently runs 14 data centers worldwide with just two in the US, one in Silicon Valley and one in Virginia. It is expanding rapidly with plans to build several new data centers in China, Malaysia, India and Indonesia. For Alibaba, the explosive growth numbers are positive news, but it still remains far behind market leader AWS and other big players like Microsoft, IBM and Google. Whether it can sustain that level of growth over the long term remains an open question, but for the short term it surely has been impressive. Featured Image: Xinhua/Wang Dingchang via Getty Images","Last week Chinese eCommerce giant Alibaba announced its its Q3 earnings. Cloud revenue was $553 million, an impressive 104 percent year-over-year increase. That comes out to a run rate in the range of $2.2 billion, well behind Google which announced it is pul…",2018-02-06T16:23:01Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}","Alibaba Cloud growing like gangbusters, but still far behind AWS and other market leaders",http://techcrunch.com/2018/02/06/alibaba-cloud-growing-like-gangbusters-but-still-far-behind-aws-and-other-market-leaders/,https://tctechcrunch2011.files.wordpress.com/2017/09/gettyimages-516871766.jpg,techcrunch,TechCrunch,cloud,1
Jacob Kleinman,"Technology is unavoidable, whether it’s the smartphone in your pocket, the computer on your desk, or the emails in your inbox. Unfortunately, all those gadgets and apps also have a pretty terrible impact on the environment. No one is asking you to ditch your technology entirely, but with a little extra thought and a few sacrifices you can reduce the amount of pollution your gadgets are putting out into the world. Here are three ways to start putting the planet first. Don’t Replace Your Phone Every Year It’s tempting to swap out your smartphone every year or two as shiny new gadgets get released, but if you’re looking to minimize your environmental footprint, you should hold on to your current device for as long as possible. According to a study from McMaster University (via Fast Company ), up to 95 percent of a smartphone’s total CO2 emissions over two years of use come from mining the materials needed to build it. In fact, the amount of energy required to create a new phone is pretty much equal to what it takes to continuously use and recharge an old device for 10 years. No one’s asking you to wait 10 years for your next upgrade, but another year or two won’t kill you, right? Batteries don’t last forever. All too soon, you’ll notice your laptop and/or smartphone don’t last… Read more Read To keep your phone running for that long, you’ll probably need to replace the battery at least once. If you have an iPhone, you can take advantage of Apple’s current offer of $29 for a new battery. For Android, start by asking your phone-maker how much they charge for a new battery if you’re out of warranty. You could also try going with a third-party service like iCracked, which offers battery replacements and screen repairs. Finally, if all else fails, you can do it yourself with a little help from one of iFixit’s guides. Recycle Your Old Gadgets Once it’s time to finally part ways with your current device, you’ll want to make sure it gets recycled. If it still works, you can probably sell it on eBay or Craiglist and make a little cash. If not you may to cut your losses and give it away for free. Do you have an old but functioning PC collecting dust in your closet? An iPhone 6 that you no… Read more Read Plenty of retailers offer their own recycling services, including Best Buy, Amazon, Apple, Samsung, Verizon, AT&amp;T, T-Mobile, and Sprint (you may even get some store credit out of the deal). You could also try Call 2 Recycle, which collects smartphones, or just list it on Freecycle and see if someone online wants your junk. If you’re feeling charitable, there are plenty of organizations that will take your old gadgets for a good cause. Reconnect (a collaboration between Dell and Goodwill), is a reliable option. The Wireless Foundation is another great service that donates refurbished phones to domestic abuse organizations. In honor of Earth Day, Yahoo put together a ""Free Is Good"" web site promoting previously… Read more Read With all those options, it shouldn’t be too difficult to find a good home for your old smartphone, tablet, or computer once it’s finally time to say goodbye. Use the Cloud Responsibly Every time you store data online, whether that’s an email sent, a photo saved, or a new Facebook post, data centers are hard at work powering the cloud. In 2010, the cloud used up 76 billion kilowatt-hours (that’s roughly two percent of the country’s electricity) according to The New York Times, and it’s only gotten worse. A recent report from the National Resources Defense Council found that these data centers could use up to 140 billion kilowatt-hours annually by the year 2020. Unfortunately, avoiding the cloud is nearly impossible. On the plus side, some of the biggest tech companies around have done a pretty good job of lowering their impact on the environment in the past few years. Got an email address? Use a computer? Is that a smartphone in your pocket? Then you need to get… Read more Read In 2014, Time reported that the bulk of electricity used for Amazon’s data centers, which power the company’s own site along with other services like Netflix, relied largely on coal power. But by 2016, Amazon claimed its cloud was powered by 40 percent renewable energy, with plans to hit 50 percent by the end of 2017. Google’s also committed to a greener cloud, and promises to offset 100 percent of the power its data centers use with clean energy. As for Apple, the company recently confirmed that it relies on Google’s servers for iCloud. At the end of the day, it doesn’t really matter which cloud you use — at least as far as the environment is concerned. Just pick the one you like best, and try not to overdo it with those emails.","Technology is unavoidable, whether it’s the smartphone in your pocket, the computer on your desk, or the emails in your inbox. Unfortunately, all those gadgets and apps also have a pretty terrible impact on the environment. Read more...",2018-04-06T15:00:00Z,"{'id': None, 'name': 'Lifehacker.com'}",How to Be an Environmentally Responsible Technology User,https://lifehacker.com/how-to-be-an-environmentally-responsible-technology-use-1824996679,"https://i.kinja-img.com/gawker-media/image/upload/s--M2eLj1Tg--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/bbjn7ecltmdqfvpi82rn.jpg",,Lifehacker.com,cloud,1
JORI FINKEL,"A main conceit of True is that nature is alive and sentient. The series features its own cloud character, Cumulo, who helps the heroine, True, by whisking her to destinations like the Wishing Tree. Other characters include the Rainbow King, an omnipotent wizard who is also lovable and fallible, like a cute grandpa, Mr. Borkson said. Since partnering with the Toronto-based Guru Studio in 2012 to create the show, the artists have stayed involved with True well beyond the pilot and are still designing new characters and writing premises for stories. They wanted to teach the ABCs and 123s, Mr. Borkson said. And we wanted to teach love and empathy. FriendsWithYou has also been hands-on in developing the balloon for the Macys parade, working to make a fatter and cuter cloud form. The Macys design team suggested adding raindrop figures costumed characters on the ground which inspired the artists to add an inflatable rainbow. A rainbow is so magical, Mr. Sandoval said. For us its this metaphysical and spiritual symbol about believing in something larger than yourself. The artists plan to carry the rainbow together, each supporting one end, so its also a symbol of their partnership. Or, as Mr. Sandoval put it, I think if we combined our two brains together, wed make one amazing sandwich.","The Little Cloud balloon, by the art duo FriendsWithYou, during a test flight. One of its creators describes it as “a simplified symbol of light and hope.”",2018-10-31T18:00:35Z,"{'id': 'the-new-york-times', 'name': 'The New York Times'}",New at the Macy’s Thanksgiving Parade: A Sunny Little Cloud,https://www.nytimes.com/2018/10/31/arts/design/macys-thanksgiving-day-parade.html,https://static01.nyt.com/images/2018/11/01/arts/01balloon-oak2/02balloon-oak2-facebookJumbo.jpg,the-new-york-times,The New York Times,cloud,1
Ron Miller,"Pivotal has always been about making open source tools for enterprise developers, but surprisingly up until now the arsenal has lacked a serverless component. That changed today with the alpha launch of Pivotal Function Service. “ Pivotal Function Service is a Kubernetes-based, multi-cloud function service. Its part of the broader Pivotal vision of offering you a single platform for all your workloads on any cloud,” the company wrote in a blog post announcing the new service. What’s interesting about Pivotal’s flavor of serverless besides the fact that it’s open source, is that it has been designed to work both on-prem and in the cloud in a cloud native fashion, hence the Kubernetes-based aspect of it. This is unusual to say the least. The idea up until now has been that the large-scale cloud providers like Amazon, Google and Microsoft could dial up whatever infrastructure your functions require, then dial them down when you’re finished without you ever having to think about the underlying infrastructure. The cloud provider deals with whatever compute, storage and memory you need to run the function and no more. Pivotal wants to take that same idea and make it available in the cloud across any cloud service. It also wants to make it available on-prem, which may seem curious at first, but Pivotal’s Onsi Fakhouri says customers want that same abilities both on-prem and in the cloud. “One of the key values that you often hear about serverless is that it will run down to zero and there is less utilization, but at the same time there are customers who want to explore and embrace the serverless programming paradigm on-prem,” Fakhouri said. Of course, then it is up to IT to ensure that there are sufficient services to meet the demands of the serverless programs. The new package includes several key components for developers including an environment for building, deploying and managing your functions, a native eventing ability that provides a way to build rich event triggers to call whatever functionality you require, and the ability to do this within a Kubernetes-based environment. This is particularly important as companies embrace a hybrid use case to manage the events across on-prem and cloud in a seamless way. One of the advantages of Pivotal’s approach is that Pivotal can work on any cloud as an open product. This in contrast to the cloud providers like Amazon, Google and Microsoft who provide similar services that run exclusively on their clouds. Pivotal is not the first to build an open source Function as a Service, but they are attempting to package it in a way that makes it easier to use. Serverless doesn’t actually mean there are no underlying servers. Instead, it means that developers don’t have to point to any servers because the cloud provider takes care of whatever infrastructure is required. In an on-prem scenario, IT has to make those resources available.","Pivotal has always been about making open source tools for enterprise developers, but surprisingly up until now the arsenal has lacked a serverless component. That changed today with the alpha launch of Pivotal Function Service. “Pivotal Function Service is a…",2018-12-07T17:11:39Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Pivotal announces new serverless framework,http://techcrunch.com/2018/12/07/pivotal-announces-new-serverless-framework/,https://techcrunch.com/wp-content/uploads/2018/12/GettyImages-876551248.jpg?w=587,techcrunch,TechCrunch,cloud,1
David Lumb,"If you were looking for an update to PDP's basic PS4 media remote, you're in luck: The new version, announced today, uses ""advanced Cloud technology"" to automatically detect all the devices connected to your console to help you program them at your whim. In theory, this makes it easier to track and link up all the disparate parts of your entertainment diaspora.","If you were looking for an update to PDP's basic PS4 media remote, you're in luck: The new version, announced today, uses ""advanced Cloud technology"" to automatically detect all the devices connected to your console to help you program them at your whim. In t…",2018-08-23T22:03:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Sony's new PS4 remote automatically links up with your A/V gear,https://www.engadget.com/2018/08/23/sony-new-ps4-remote-automatically-links-up-av-gear/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fs.aolcdn.com%2Fhss%2Fstorage%2Fmidas%2Fed9fb2d12834b7a775a8dabb65fcbe21%2F206615054%2FCloudRemote-755x425.jpg&client=amp-blogside-v2&signature=761454e6e3eea05cacf38a2a0a3f5926ebf22b00,engadget,Engadget,cloud,1
Steve Dent,"If you think I'm exaggerating about how dramatic these storms can look, a quick Google image search of "" dirty storm "" will change your mind. All suggest that some kind of impending apocalypse/horsemen scenario is about to take place. Many of the best ones were taken by Negroni himself, but even he admits that the image above is special. ""The photograph with which I obtained second place in the Pano Awards was taken during the violent eruption of the Volcán Calbuco, located in the south of Chile,"" he told Engadget via email. ""The technique was simple: long exposure, tripod and a 80-200mm lens. Approximately 10 minutes to achieve that incredible image that, without a doubt, is my best photograph of an eruption and I think the best taken in the world."" How does this happen? In a regular thunderstorm cloud, lightning is created when rising air makes ice crystals and water droplets bump together, forming static electricity. Once the charges build up enough to surpass the atmosphere's natural insulating tendency, lightning discharges either from one cloud to another or to the ground. Volcanic lightning works on the same principle, but via different actions. Recently, researchers studying one of the world's most active volcanoes, Japan's Mount Sakurajima (above), learned more about how it forms. Using high-speed cameras, they discovered that charge created by churning magma builds up just above the rim, electrifying the ash just above it. That creates an electrostatic potential in the lower portion of the cloud that eventually causes lightning to discharge into the cloud or air, often in the opposite direction of regular lightning. As such, lightning formation is usually limited to the bottom part of the ash plume, and depends strongly on how the plume develops. Chile is located on the Pacific ""Rim of Fire,"" and has 90 active volcanoes, the second most in the world after Indonesia. Negroni has photographed volcanoes like Llaima in the Araucania region, the 2011 eruption of Puyehue-Cordon Caulle, and in 2015, the Villarrica Volcano and Calbuco pictured here. Volcanologists and seismologists have become extremely adept at detecting eruptions ahead of time, so endangered inhabitants were evacuated well ahead of time. Though volcano science has improved dramatically, getting near active ones that produce lightning isn't for the faint of heart. ""Photographing volcanoes is very dangerous and I do not recommend it to anyone,"" Negroni said. ""But if you have the opportunity to do it from a very safe place, it will be an incredible show that nature gives us.""","What do you get when you marry two of Earth's most dramatic natural events, lightning and volcanoes ? The answer is a ""dirty storm,"" an infernal melange of lightning, magmatic fire and ash that surpasses even the wildest Hollywood disaster movie effects. If t…",2017-11-27T14:00:00Z,"{'id': 'engadget', 'name': 'Engadget'}",Catching lightning in a volcanic bottle,https://www.engadget.com/2017/11/27/volcano-lightning-dirty-storm-francisco-negroni/,https://o.aolcdn.com/images/dims?thumbnail=1200%2C630&quality=80&image_uri=https%3A%2F%2Fs.aolcdn.com%2Fhss%2Fstorage%2Fmidas%2F2099e398052d8adabccfecd6492d7988%2F205894899%2Fbig-picture-volcanic-lightning-2017-11-26-04.jpg&client=cbc79c14efcebee57402&signature=018d2191925654f75326f6176d30b5cdb2096eb5,engadget,Engadget,cloud,1
Monica Chin,"Can't get enough Google? You're in luck. The company, a division of Alphabet Inc., has announced that it will expand its Cloud services to five new regions, and build three new submarine cables to service its capacity needs. SEE ALSO: Google wants your phone screen to double as a speaker Google's Cloud platform is already up and running in thirteen regions including Tokyo, Taiwan, Mumbai, Singapore, Sydney, London, Belgium, Frankfurt, Sao Paulo, Oregon, Iowa, Northern Virginia, and South Carolina. The company estimates that its network currently accounts for 25% of all internet traffic. The platform will expand to Hong Kong, Los Angeles, Montreal, the Netherlands, and Finland. This means companies in these regions who leverage Google's Cloud Platform for their server capacity in these regions will experience higher performance and fewer service disruptions. Those who live in these regions, according to Google, should be excited. ""Overall, these investments mean faster and more reliable connectivity for all users and customers,"" a Google spokesperson told Mashable. The new regions will ""give customers high performance levels, high availability local computing, flexibility of deployment and scalability for their applications and data."" Image: google Google is also constructing, or taking part in constructing, three undersea cables, which will be commissioned in 2019. Customers in the areas these cables connect will experience faster and better Google Cloud Platform and G Suite services as a result of the increased network capacity and connectivity. They join eight additional cables in which Google has direct investment, as well as many more from which the company leases capacity. Google is also invested in Indigo which connects Australia, Indonesia, and Singapore, and PLCN which connects Hong Kong and Los Angeles, both of which will also enter service in 2019. ""Overall, these investments mean faster and more reliable connectivity for all our users and customers,"" the company says. ""Subsea cables increase capacity and latency to these regions."" The Curie cable will connect California to Chile. Of the three cables, Curie is the only one that Google privately owns. Google will become the first major non-telecom company to build a private intercontinental cable. As the sole owner, Google will have complete control over Curie's design and construction. ""Once deployed, we can make routing decisions that optimize for latency and availability,"" the company said. Google is also working with Facebook, subsea network manufacturer Aqua Comms, and industrial group Bulk Infrastructure on the Havfrue cable, which will connect the U.S. east coast to Denmark and Ireland. The conglomerate has tapped undersea communications system supplier TE Connectivity to construct the cable. TE has already begun scoping out the cable's potential route. Google is also working with telecommunications infrastructure company RTI Connectivity and IT manufacturer NEC Corporation on the HK-G Cable, connecting Hong Kong and Guam. The cable will deliver what NEC refers to as ""much-needed expansion of communications networks between Asia and the United States."" Basically, watch out, world. Google is coming to you, and it's coming fast.","Can't get enough Google? You're in luck. The company, a division of Alphabet Inc., has announced that it will expand its Cloud services to five new regions, and build three new submarine cables to service its capacity needs. SEE ALSO: Google wants your phone …",2018-01-16T23:33:14Z,"{'id': 'mashable', 'name': 'Mashable'}",Google will construct three new undersea cables in 2019,http://mashable.com/2018/01/16/google-constructing-three-undersea-cables/,https://i.amz.mshcdn.com/3rtY7lqo2jyV20u9hpV_tsawKPw=/1200x630/2018%2F01%2F16%2F70%2F212809d65a0e490db84ddfaf26d17064.c0bd2.jpg,mashable,Mashable,cloud,1
Ron Miller,"Oracle filed suit in federal court last week alleging yet again that the decade-long $10 billion Pentagon JEDI contract with its single-vendor award is unfair and illegal. The complaint, which has been sealed at Oracle’s request, is available in the public record with redactions. If all of this sounds familiar, it’s because it’s the same argument the company used when it filed a similar complaint with the Government Accountability Office (GAO) last August. The GAO ruled against Oracle last month stating, “the Defense Departments decision to pursue a single-award approach to obtain these cloud services is consistent with applicable statutes (and regulations) because the agency reasonably determined that a single-award approach is in the governments best interests for various reasons, including national security concerns, as the statute allows. That hasn’t stopped Oracle from trying one more time, this time filing suit in the United States Court of Federal Claims this week, alleging pretty much the same thing it did with the GAO, that the process was unfair and violated federal procurement law. Oracle Senior Vice President, Ken Glueck reiterated this point in a statement to TechCrunch. “The technology industry is innovating around next generation cloud at an unprecedented pace and JEDI as currently envisioned virtually assures DoD will be locked into legacy cloud for a decade or more. The single-award approach is contrary to well established procurement requirements and is out of sync with industrys multi-cloud strategy, which promotes constant competition, fosters rapid innovation and lowers prices,” he said echoing the language in the complaint. The JEDI contract process is about determining the cloud strategy for the Department of Defense for the next decade, but it’s important to point out that even though it is framed as a 10 year contract, it has been designed with several opt out points for DOD with an initial two year option, two three year options and a final two year option, leaving open the possibility it might never go the full 10 years. Oracle has complained for months that it believes the contract has been written to favor the industry leader, Amazon Web Services. Company co-CEO Safra Catz even complained directly to the president in April, before the RFP process even started. IBM filed a similar protest in October citing many of the same arguments. Oracle’s federal court complaint filing cites the IBM complaint and language from other bidders including Google (which has since withdrawn from the process ) and Microsoft that supports their point that a multi-vendor solution would make more sense. The Department of Justice, which represents the US government in the complaint, declined to comment. The DOD also indicated it wouldn’t comment on pending litigation, but in September spokesperson Heather Babb told TechCrunch that the contract RFP was not written to favor any vendor in advance. “The JEDI Cloud final RFP reflects the unique and critical needs of DOD, employing the best practices of competitive pricing and security. No vendors have been pre-selected, she said at the time. That hasn’t stopped Oracle from continually complaining about the process to whoever would listen. This time they have literally made a federal case out of it. The lawsuit is only the latest move by the company. It’s worth pointing out that the RFP process closed in October and a winner won’t be chosen until April. In other words, they appear to be assuming they will lose before the vendor selection process is even completed.","Oracle filed suit in federal court last week alleging yet again that the decade-long $10 billion Pentagon JEDI contract with its single-vendor award is unfair and illegal. The complaint, which has been sealed at Oracle’s request, is available in the public re…",2018-12-12T19:45:29Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Oracle is suing the U.S. government over $10B Pentagon JEDI cloud contract process,http://techcrunch.com/2018/12/12/oracle-is-suing-the-u-s-government-over-10b-pentagon-jedi-cloud-contract-process/,https://techcrunch.com/wp-content/uploads/2018/12/GettyImages-1052770676.jpg?w=613,techcrunch,TechCrunch,cloud,1
"David Nield on Field Guide, shared by Alex Cranz to Gizmodo","These days it’s hard to do without a little bit of cloud storage space, whether it’s for your holiday photos or your digital music, and Google just revamped its offerings with new price points, a new name, and some new extras. So how does Google One stack up against what you can get from Apple, Microsoft, and others? Here we’ll look at both the price tiers on offer and everything else you get with your cloud storage—like file syncing, online apps, and so on. For now, there’s not all that much to Google One, but it gives us a peek at what Google’s planning for the future. Google One cloud storage to rule them all Right now if you buy some cloud storage space from Google and it gets spread between Gmail, Google Drive, and Google Photos, on top of the 15GB you get free of charge (YouTube videos don’t count)—that’s been the case for years, but Google One shakes everything up a bit. If you already pay for Google Drive storage, you’ll automatically get upgraded soon. Otherwise there’ll be a limited roll out for the rest of us. There’s a brand new URL for a start, presumably just to manage your storage and your plan (the likes of Gmail and Google Drive won’t actually change). A couple of the tiers are changing too: You can now get 200GB for $2.99 a month (a new tier), and 2TB for $9.99 a month (twice the storage you previously got). Google says customers already on the 1TB plan will soon get upgraded automatically. The other plans remain the same: 100GB for $1.99 a month, 10TB for $99.99 a month, 20TB for $199.99 a month, and 30TB for $299.99 a month. Remember Google launched a Backup &amp; Sync tool for Windows and macOS last year, letting you archive any folder on your computer (like the desktop or your documents), whether it was inside your Google Drive folder or not. With the new lower cost tiers we’re now talking about one of the most comprehensive, most straightforward, and cheapest backup solutions on the market. Google is adding benefits to Google One too. Family sharing will soon be an option, so you can split your storage between as many as five family members and get billed to one account. Google is also promising “one-tap access to experts for help with our consumer products and services” for Google One customers. Apparently more benefits are on the way, including credits in Google Play and deals for hotels that turn up in Google Search. It sounds a little like Amazon Prime but for Google, though as yet the perks are a little sparse—better technical support and a few special deals. If these perks start to get ramped up over time, and maybe Google throws in the new YouTube Music service, then we’ll really be talking. The competition: iCloud, OneDrive, Dropbox That’s Google One; what has everyone else got? If you’re using Apple hardware you may well have defaulted to iCloud, which offers 50GB of room for $0.99 a month, 200GB for $2.99 a month, and 2TB for $9.99 a month, on top of 5GB you get for free. It all works seamlessly with Apple apps like Photos and Mail, and you can now back up certain folders from Mac and Windows computers to iCloud too. Microsoft’s OneDrive is tightly tied to Office 365. You get 5GB for free and can up that to 50GB for $1.99 a month, but you can also pay $69.99 a year for 1TB of room, with Office 365 for Mac or Windows thrown in as well. That’s $6 a month for the storage and the office suite, so it’s an appealing option for fans of Microsoft or heavy Office users. Dropbox, meanwhile, gives users 2GB of room for free and has two paid options for non-business customers: $9.99 a month or $19.99 a month. Both give you 1TB of cloud space, but the pricier tier includes a few extras aimed at creative professionals, like the option to build up a shareable portfolio of your work online. Those costs are broadly comparable, with the exception of Dropbox, which is now looking rather limited in both its range of plans and the price points it’s offering. You do at least get the added bonus of Paper, a better-than-you-might-expect tool for collaborating on documents inside your browser. But what are they like to use? Dropbox mastered the file syncing game long before Google, Apple, and Microsoft joined the party, and it remains a super-simple way of keeping folders in sync with the cloud and multiple devices. The syncing clients for Windows and macOS blend seamlessly into the background and you’d hardly know they were there. It works everywhere and has a ton of useful sharing features—like expiry times and password protection on your shared links. You can also recover deleted files for a period of 30 days, stream music and video on the web, sync files to phones for offline use, and so on. For syncing and sharing files, and all the tools you need for that, it’s hard to beat. iCloud and OneDrive both have similar advantages and disadvantages: Superb integration with other products from Apple and Microsoft respectively, but a certain clunkiness on the web, and limited support for jumping across mobile and desktop platforms (there’s no iCloud app for Android, of course). iCloud and OneDrive eschew the ‘one synced folder’ approach of Dropbox to back up other bits of macOS and Windows (such as folders on the desktop), if you want them to, but neither implementation is particularly intuitive—as you might expect from cloud services built retroactively to fit desktop OSes that were already well established. Apple’s service has the edge in terms of invisibly and elegantly taking care of all your app and iOS backups, but Microsoft’s offering is perfect for Office users, so it’s a question of which software packages you’re spending more time in. At the same time, neither iCloud nor OneDrive do syncing and sharing quite as intuitively as Dropbox. And Google Drive’s syncing and sharing options aren’t quite up to the standard of Dropbox either. It doesn’t work as well with Office as OneDrive does, or fit as neatly into iOS as iCloud does, but when you take all of the features into account and treat them as a whole, it nevertheless ends up being the best option overall. We all know about Google’s web app expertise, and when you log into the (newly redesigned) Google Drive interface in your browser, it feels like you’re using a fully featured file management app rather than something developed as an afterthought (take note, Microsoft and Apple). It’ll work and sync files across Windows, macOS, Android, and iOS, and when you throw Google Photos into the equation and the online office apps built into Drive, it’s easily the most comprehensive option out there for getting at your stuff from anywhere. For now, Google One hasn’t moved the needle much, besides a couple of new pricing tiers and the promise of some extra benefits that we’ll have to wait to see. But if Google can get it up and running quickly, then the others have an even bigger gap to make up for.","These days it’s hard to do without a little bit of cloud storage space, whether it’s for your holiday photos or your digital music, and Google just revamped its offerings with new price points, a new name, and some new extras. So how does Google One stack up …",2018-05-16T13:57:00Z,"{'id': None, 'name': 'Gizmodo.com'}",How Google One Actually Stacks Up to the Cloud Competition,https://fieldguide.gizmodo.com/how-google-one-actually-stacks-up-to-the-cloud-competit-1826066389,"https://i.kinja-img.com/gawker-media/image/upload/s--VYXCLNOO--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/ytiweocb3eq4xshclmgs.jpg",,Gizmodo.com,cloud,1
Kate Conger,"Google Cloud has a whopping 20 new product announcements out today, most of them aimed at enterprise customers—which means they probably won’t matter much to you unless you’re in the position to make IT decisions for a company. But we lowly consumers sometimes get the trickle-down effect from good privacy decisions at the enterprise Cloud level, and there are some bits and pieces in this announcement that will have a positive impact for regular old users—especially if you use products or services that are hosted in Google Cloud. And because Google counts the likes of Spotify, Walgreens, and even Apple among its Cloud customers, it’s holding a lot more of your data than you probably realize. All of Google’s announcements today focus on trust. Of course, any cloud provider needs its users to trust it enough to hand over their data. But Google’s approach is a little different—it’s trying to build the most trustworthy cloud it can, but Google is also giving customers ways to verify what it’s up to in case they don’t trust Google enough to take the company at its word. For instance, Google is expanding Access Transparency by offering customers a real-time log of any accesses to their data by Google’s own engineers and support staff, as well as justifications for that access. In doing so, Google is giving enterprise customers a way to make sure that its employees aren’t poking around when they shouldn’t be. “We want to be as open and transparent as possible, allowing customers to see what happens to their data,” said Jennifer Lin, Cloud’s product management director for security and privacy. Google is also giving consumers more visibility of security threats, with the same tools it uses to protect its own products. Its Cloud Security Command Center will highlight botnets, cryptocurrency mining, and other threats using the Google Security team’s own tools as well as vendor data from security companies like Cloudflare, CrowdStrike, Dome9, RedLock, Palo Alto Networks, and Qualys.","Google Cloud has a whopping 20 new product announcements out today, most of them aimed at enterprise customers—which means they probably won’t matter much to you unless you’re in the position to make IT decisions for a company. Read more...",2018-03-21T13:30:00Z,"{'id': None, 'name': 'Gizmodo.com'}",Google Opens Up About How Its Cloud Stores Your Secrets,https://gizmodo.com/google-opens-up-about-how-its-cloud-stores-your-secrets-1823934803,"https://i.kinja-img.com/gawker-media/image/upload/s--1RlTjLhd--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/ro9fyhsybmfzufgq2bmd.jpg",,Gizmodo.com,cloud,1
Ron Miller,"MongoDB ‘s Atlas service has been giving companies a managed database service in the cloud for some time. Mongo deals with all the heavy lifting behind the scenes, relieving the developer of creating it all themselves. Today the company announced it was taking that a step further by allowing customers to have granular control over where the data lives with a new feature called Global Clusters. This allows companies to choose a cloud provider, then move data from a MongoDB database running in Atlas to any location in the world. As MongoDB CTO and co-founder, Eliot Horowitz explained, it doesn’t matter who the cloud provider is, you can set a data location policy, select a cloud vendor and data center location, and see what the results will look like on a graphical representation of the world map. When you give the OK, Mongo moves the data automatically for you in the background without shutting anything down to do it. Global Clusters interface. Screenshot: MongoDB Lots of countries are requiring proof of data sovereignty, including the EU’s GDPR rules that went into effect last month. It has been challenging for many businesses to comply with these kinds of rules on their own. Horowitz said he created geographic partitions before Atlas and it required a tremendous amount of engineering effort. By providing it as a service in this fashion, the company is putting this kind of data migration in the hands of the smallest business owners, giving them that geographic granularity from the start. “I think what you’re going to see is a lot of [small businesses], who feel they can now compete with some of the larger websites and actually have that high level of service on day one, rather than having to wait to hire a team of engineers.” The beauty of this approach from Mongo’s perspective is that they don’t even have to worry about building a worldwide data center presence of their own. Instead they simply piggy back on the global locations of each of the main public cloud providers, AWS, Microsoft and Google. “The cool thing is that those data centers come from any of the cloud providers. So you can actually do this on any cloud provider that you want in any region [they support],” he said. This feature will be available starting today for all Atlas users.","MongoDB‘s Atlas service has been giving companies a managed database service in the cloud for some time. Mongo deals with all the heavy lifting behind the scenes, relieving the developer of creating it all themselves. Today the company announced it was taking…",2018-06-27T15:05:48Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",MongoDB launches Global Clusters to put geographic data control within reach of anyone,http://techcrunch.com/2018/06/27/mongodb-launches-global-clusters-to-put-geographic-data-control-within-reach-of-anyone/,https://techcrunch.com/wp-content/uploads/2018/06/GettyImages-871641050.jpg?w=600,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Amazon today announced that its AWS cloud computing service now offers a new region that’s specifically designed for the workloads of the U.S. Intelligence Community. This new AWS Secret Region (that’s really its name) can run workloads up to the “secret” security classification level and will complement the service’s existing $600 million contract with the CIA and other agencies for running Top Secret workloads on its cloud. AWS’s announcement comes about a month after Microsoft made a similar announcement. With Azure Government Secret, Microsoft’s Government Cloud will also soon support workloads for agencies and their partners who are working with data that is classified as “secret.” “The U.S. Intelligence Community can now execute their missions with a common set of tools, a constant flow of the latest technology and the flexibility to rapidly scale with the mission,” said Teresa Carlson, Vice President, Amazon Web Services Worldwide Public Sector. “The AWS Top Secret Region was launched three years ago as the first air-gapped commercial cloud and customers across the U.S. Intelligence Community have made it a resounding success. Ultimately, this capability allows more agency collaboration, helps get critical information to decision makers faster, and enables an increase in our Nation’s Security.” It’s worth noting that the original and air-gapped Top Secret cloud, which AWS operates for the intelligence community, was limited to intelligence agencies. This new Secret Region is available to all government agencies and stands separate from the earlier work AWS did with the CIA and others, as well as the existing Amazon GovCloud. While Google has long offered its G Suite to government customers, the company’s effort to bring on more enterprise users hasn’t quite extended to government agencies and their cloud computing needs. Chances are, thought, that Google, too, is working on getting the necessary certifications to handle more classified government data on its servers. Featured Image: Getty Images",Amazon today announced that its AWS cloud computing service now offers a new region that’s specifically designed for the workloads of the U.S. Intelligence Community. This new AWS Secret Region (that’s really its name) can run workloads up to the “secret” sec…,2017-11-20T18:44:03Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",AWS launches a Secret region for the U.S. intelligence community,https://techcrunch.com/2017/11/20/aws-launches-a-secret-region-for-the-u-s-intelligence-community/,https://tctechcrunch2011.files.wordpress.com/2017/11/gettyimages-635268902.jpg,techcrunch,TechCrunch,cloud,1
Frederic Lardinois,"Google Cloud’s new region in Los Angeles is now online, the company announced today. This isn’t exactly a surprise, given that Google had previously announced a July launch for the region, but it’s a big step for Google, which now boasts five cloud regions in the United States. It was only three years ago that Google opened its second U.S. region and while it was slow to expand its physical cloud footprint, the company now features 17 regions around the world. When it first announced this new region, Google positioned it as the ideal region for the entertainment industry. And while that’s surely true, I’m sure we’ll see plenty of other companies use this new region, which features three availability zones, to augment their existing deployments in Google’s other West Coast region in Oregon or as part of their overall global cloud strategy. The new region is launching with all the core Google Cloud compute services like App Engine, Compute Engine and Kubernetes Engine, as well as all of Google’s standard database and file storage tools, including the recently launched NAS-like Cloud Filestore service. For businesses that have a physical presence close to L.A., Google also offers two dedicated interconnects to Equinix’s and CoreSite’s local LA1 data centers. It’s worth nothing that Microsoft, which has long favored a strategy of quickly launching as many regions as possible, already offered its users a region in Southern California. AWS doesn’t currently have a presence in the area, though unlike Google, AWS does offer a region in Northern California.","Google Cloud’s new region in Los Angeles is now online, the company announced today. This isn’t exactly a surprise, given that Google had previously announced a July launch for the region, but it’s a big step for Google, which now boasts five cloud regions in…",2018-07-16T16:51:03Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Google Cloud’s LA region goes online,http://techcrunch.com/2018/07/16/google-clouds-la-region-goes-online/,https://techcrunch.com/wp-content/uploads/2018/07/LA_region_announcement.png?w=603,techcrunch,TechCrunch,cloud,1
Ron Miller,"Last Fall at Dreamforce, Salesforce announced a deepening friendship with Google. That began to take shape in January with integration between Salesforce CRM data and Google Analytics 360 and Google BigQuery. Today, the two cloud giants announced the next step as the companies will share data between Google Analytics 360 and the Salesforce Marketing Cloud. This particular data sharing partnership makes even more sense as the companies can share web analytics data with marketing personnel to deliver ever more customized experiences for users (or so the argument goes, right?). That connection certainly didn’t escape Salesforce’s VP of product marketing, Bobby Jania. “Now, marketers are able to deliver meaningful consumer experiences powered by the world’s number one marketing platform and the most widely adopted web analytics suite,” Jania told TechCrunch. Brent Leary, owner of the consulting firm CRM Essentials says the partnership is going to be meaningful for marketers. “The tighter integration is a big deal because a large portion of Marketing Cloud customers are Google Analytics/GA 360 customers, and this paves the way to more seamlessly see what activities are driving successful outcomes,” he explained. The partnership involves four integrations that effectively allow marketers to round-trip data between the two platforms. For starters, consumer insights from both Marketing Cloud and Google Analytics 360, will be brought together into a single analytics dashboard inside Marketing Cloud. Conversely, Market Cloud data will be viewable inside Google Analytics 360 for attribution analysis and also to use the Marketing Cloud information to deliver more customized web experiences. All three of these integrations will be generally available starting today. A fourth element of the partnership being announced today won’t be available in Beta until the third quarter of this year. “For the first time ever audiences created inside the Google Analytics 360 platform can be activated outside of Google. So in this case, I’m able to create an audience inside of Google Analytics 360 and then I’m able to activate that audience in Marketing Cloud,” Jania explained. An audience is like a segment, so if you have a group of like-minded individuals in the Google analytics tool, you can simply transfer it to Salesforce Marketing Cloud and send more relevant emails to that group. This data sharing capability removes a lot of the labor involved in trying to monitor data stored in two places, but of course it also raises questions about data privacy. Jania was careful to point out that the two platforms are not sharing specific information about individual consumers, which could be in violation of the new GDPR data privacy rules that went into effect in Europe at the end of last month. “What we’re [we’re sharing] is either metadata or aggregated reporting results. Just to be clear there’s no personal identifiable data that is flowing between the systems so everything here is 100% GDPR-compliant,” Jania said. But Leary says it might not be so simple, especially in light of recent data sharing abuses. “With Facebook having to open up about how they’re sharing consumer data with other organizations, companies like Salesforce and Google will have to be more careful than ever before about how the consumer data they make available to their corporate customers will be used by them. It’s a whole new level of scrutiny that has to be apart of the data sharing equation,” Leary said. The announcements were made today at the Salesforce Connections conference taking place in Chicago this week.","Last Fall at Dreamforce, Salesforce announced a deepening friendship with Google. That began to take shape in January with integration between Salesforce CRM data and Google Analytics 360 and Google BigQuery. Today, the two cloud giants announced the next ste…",2018-06-13T12:00:02Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Salesforce deepens data sharing partnership with Google,http://techcrunch.com/2018/06/13/salesforce-deepens-data-sharing-partnership-with-google/,https://techcrunch.com/wp-content/uploads/2018/06/GettyImages-146423736-1.jpg?w=600,techcrunch,TechCrunch,cloud,1
SETH BERKMAN,"Taylor Crosby has always been proud of her brother’s success. And at 21, she has come to peace with the merits of her own hockey career by recognizing that the game does not have to be her defining legacy. “The older I get, the more I see how important that is,” Crosby said. Since she was 2 weeks old, the rinks around Cole Harbour, Nova Scotia, became Crosby’s second home. There, she watched her brother garner national media attention as he dominated youth and junior leagues. Crosby was an invested fan and protective sister, yelling at players when they hit her brother, but she never had much inclination to play, at least at first. When she finally decided she wanted to try being a goalie, it took her two years to persuade her parents, Troy and Trina. Photo Crosby transferred to St. Cloud from Northeastern and enjoys the small-town atmosphere that is similar to her hometown in Nova Scotia. Credit Tim Gruber for The New York Times Troy, a former goalie who was drafted by the Montreal Canadiens, relented and bought her a set of pads when she was 10. “Both my parents were a little skeptical,” Crosby said. “I think they wanted to make sure I wanted to play for me and not just because of my brother. They wanted me to feel like I was my own person and I didn’t pick goalie just to be kind of different from him, but that I actually wanted to be a goalie.” Crosby attended Shattuck-St. Mary’s in Faribault, Minn., a hockey talent factory whose alumni include several members of the United States women’s national team. (Her brother also attended the boarding school for one year.) When she was 17, Crosby, too, started gaining recognition at the highest levels of women’s hockey. During the summer of 2013, she was invited to a junior camp held by Hockey Canada and traveled to Sheffield, England, for a camp run by the International Ice Hockey Federation. After graduating from Shattuck, Crosby enrolled at Northeastern. But she never found her footing in Boston and transferred after one year. The invitations to prestigious camps and tryouts also tapered off. As her hockey career reached its first detour, she began thinking about the game in more abstract terms. “To sum it up, it just wasn’t the right fit,” Crosby said. “It seemed like the right fit at the time, but when I got there, whether it was playing time — it just didn’t feel like home.” Crosby thought a return to Minnesota could be stabilizing. At St. Cloud State, she was not given any promises for playing time, but she saw an environment similar to Cole Harbour’s. With Shattuck classmates in abundance there, the small-town atmosphere meshed well with the pace she sought in her life and granted her a degree of anonymity. When a new class arrives on campus, incoming freshmen can become star-struck when they discover Crosby’s lineage. But older teammates have become cognizant to quickly address any overbearing inquiries. “We try to tell them she has a name, too — it’s Taylor,” the junior forward Jordan Stewart said. “Regardless of her brother, get to know her for her. But she’s pretty open about it.” Photo When people ask Crosby about her famous brother, her teammates often say, “She has a name too — it’s Taylor.” Credit Tim Gruber for The New York Times New teammates quickly learn Crosby has a sense of humor about her surname. She wrote in her player bio that her older brother “also plays hockey.” They also recognize Crosby’s presence as a team leader. Before games, she breaks up tension by dancing or emitting primordial animal sounds. Between periods, when not in net, she relays feedback to forwards on where the ice is opening up and the opposing goalie’s tendencies. Katie Fitzgerald, a goaltender for the Metropolitan Riveters of the National Women’s Hockey League, was Crosby’s teammate at St. Cloud State during the 2015-16 season. Fitzgerald started a majority of games that year and said Crosby was crucial to how she changed her mental and physical preparation for games. “She’s a big voice on this team, and without her it would not be the same,” Stewart said. “She is just the most humble, hard-working person I think I’ve ever met.” Crosby is currently a backup goaltender for the Huskies, who are 3-12. In an exhibition game on Oct. 13 against the Minnesota Whitecaps, a semipro team consisting of some of the Midwest’s top players, Crosby made 35 saves in a 2-1 win. She is unsure what role hockey will play in her life after college. She would like to help grow the sport — the Canadian Women’s Hockey League and the N.W.H.L. are two professional options — but envisions herself being just as content traveling or staying in St. Cloud. “It changes every day,” Crosby said. “I’m not sure where the wind will blow.” Carrying the indecision and wonder of most college students her age, Crosby embraces the current ambiguity surrounding her future. It also brings peace to her brother, watching his little sister come of age on her own accord. “That’s so important to me,” Sidney Crosby said last month after a game against the Rangers. “As her brother, you just want to see her happy and having fun and doing what she loves to do, and she’s doing that. Unfortunately, I don’t get to see her as often as you’d probably like. The fact that she does change her mind often, she’s pretty open-minded, that’s good. “She’s learning, and she’ll figure it out. I kind of laugh because we’re kind of similar in that way. I can certainly relate to that and hopefully help her out and let her know there’s not quite a big rush, and take her time to figure out what she wants to do.” A version of this article appears in print on November 28, 2017, on Page B9 of the New York edition with the headline: This Crosby Has Goals, Too, Beyond the One She Tends. Order Reprints | Today's Paper | Subscribe Continue reading the main story","Taylor Crosby, a goalie for St. Cloud State, is used to talking about her famous older brother. But to her teammates, she’s a leader on her own terms.",2017-11-28T03:46:58Z,"{'id': 'the-new-york-times', 'name': 'The New York Times'}",Hockey’s Other Crosby Toils Outside the Spotlight,https://www.nytimes.com/2017/11/27/sports/hockey/taylor-crosby-st-cloud-state.html,https://static01.nyt.com/images/2017/11/28/sports/00CROSBY-01/00CROSBY-01-facebookJumbo.jpg,the-new-york-times,The New York Times,cloud,1
Ron Miller,"Palo Alto Networks launched in 2005 in the age of firewalls. As we all know by now, the enterprise expanded beyond the cozy confines of a firewall long ago and vendors like Palo Alto have moved to securing data in the cloud now too. To that end, the company announced its intent to pay $173 million for RedLock today, an early-stage startup that helps companies make sure their cloud instances are locked down and secure. The cloud vendors take responsibility for securing their own infrastructure, and for the most part the major vendors have done a decent job. What they can’t do is save their customers from themselves and that’s where a company like RedLock comes in. As we’ve seen time and again, data has been exposed in cloud storage services like Amazon S3, not through any fault of Amazon itself, but because a faulty configuration has left the data exposed to the open internet. RedLock watches configurations like this and warns companies when something looks amiss. When the company emerged from stealth just a year ago, Varun Badhwar, company founder and CEO told TechCrunch that this is part of Amazon’s shared responsibility model. “They have diagrams where they have responsibility to secure physical infrastructure, but ultimately its the customers responsibility to secure the content, applications and firewall settings,” Badhwar told TechCrunch last year. Badhwar speaking in a video interview about the acquisition says they have been focused on helping developers build cloud applications safely and securely, whether that’s Amazon Web Services, Microsoft Azure or Google Cloud Platform. “We think about [RedLock] as guardrails or as bumper lanes in a bowling alley and just not letting somebody get that gutter ball and from a security standpoint, just making sure we don’t deviate from the best practices,” he explained. “We built a technology platform that’s entirely cloud-based and very quick time to value since customers can just turn it on through API’s, and we love to shine the light and show our customers how to safely move into public cloud,” he added. He believes that customers will benefit from RedLock’s compliance capabilities being combined with Palo Alto’s analytics capabilities to provide a more complete cloud security solution. It will also fit nicely with Evident.io, a cloud infrastructure security startup, the company acquired in March for $300 million. RedLock launched in 2015 and has raised $12 million. The $173 million purchase would appear to be a great return for the investors who put their faith in the startup.","Palo Alto Networks launched in 2005 in the age of firewalls. As we all know by now, the enterprise expanded beyond the cozy confines of a firewall long ago and vendors like Palo Alto have moved to securing data in the cloud now too. To that end, the company a…",2018-10-03T13:42:04Z,"{'id': 'techcrunch', 'name': 'TechCrunch'}",Palo Alto Networks to acquire RedLock for $173 M to beef up cloud security,http://techcrunch.com/2018/10/03/palo-alto-networks-to-acquire-redlock-to-beef-up-cloud-security/,https://techcrunch.com/wp-content/uploads/2018/10/GettyImages-847491272.jpg?w=764,techcrunch,TechCrunch,cloud,1
https://www.facebook.com/bbcnews,"Image copyright Getty Images Image caption Hamburg, July 2017: Mr Trump and Mrs Merkel have major policy differences It's hardly a match made in political heaven. German Chancellor Angela Merkel has never had much in common with Donald Trump. A free-trading, peace-loving supporter of the Iran nuclear deal will sit down for talks in Washington on Friday with a bellicose protectionist, who described as ""insane"" and ""the worst ever"" the accord that saw Iran limit its nuclear activities in return for the lifting of economic sanctions. When the US president was first elected, few in Berlin thought the liberal, steady chancellor, not known for flamboyant display, would strike up any kind of relationship with the impulsive, dramatic Donald Trump. Post-Obama difficulties Certainly no-one expected a close relationship like the warm alliance which developed between Mrs Merkel and Barack Obama (once they had overcome the awkward revelation that the Americans had been listening to her mobile phone calls). That relationship bore political fruit for Mrs Merkel - and her country. Germany's leader was the voice of Europe for America and the go-to interlocutor between the US and Russia, a result in part of President Vladimir Putin's grudging respect for Mrs Merkel. Nevertheless, behind the scenes, officials believe that Mrs Merkel's first visit to Washington, in March 2017, was not the complete disaster they had feared. Despite some tricky moments (the unforgettable handshake that never happened), Mrs Merkel and Mr Trump do have a working relationship and are said to speak with some regularity on the phone. Media caption Merkel handshake offer to Trump falls on deaf ears So as her second visit approaches should Mrs Merkel - as the tabloid newspaper Bild asked - be jealous of that kiss between Donald Trump and Emmanuel Macron, during the latter's visit to the White House this week? Is the French president - the newspaper agonised between the lines - poised to replace Mrs Merkel as the leader to call in Europe? After all, the daily Die Zeit notes, Donald Trump has far more in common with President Macron: they share a business background and both take a populist approach to politics. Until recently, the newspaper argues, Mr Macron has avoided overt criticism of the US president. Angela Merkel has clearly and repeatedly voiced her disdain for some of his polices. Image copyright AFP Image caption There was plenty of tactile body language between presidents Trump and Macron Berlin has, doubtless, watched Mr Macron's visit closely. But, in reality, there is little concern about a race to become Mr Trump's new friend in Europe. Concerned by Mr Trump's apparent disinterest in the EU, the French and German leaders prepared together for their respective visits to Washington. Character and style differ, but the message is broadly the same. Both the French and German delegations have been so closely co-ordinated that, sources say, this should be viewed as a single, orchestrated European approach. Read more on this topic: And so the agenda for Mrs Merkel's visit is already set and mirrors Emmanuel Macron's. At the very top, tariffs and trade. Mr Macron's visit has failed to produce a commitment from Donald Trump to exempt the EU from his tariffs on metal imports. With a 1 May deadline approaching, Mrs Merkel is under pressure from her EU peers to broker a permanent exemption. Image copyright AFP Image caption Mr Obama and Mrs Merkel were happy with the Iran nuclear deal; Mr Trump isn't Then there is that other looming deadline. On 12 May Mr Trump must decide whether to reimpose sanctions on Iran, which would in effect scupper the nuclear deal. Mr Macron has conceded that he failed to persuade the US president of the deal's value. Mrs Merkel will be prioritising the issue. No one is pretending this will be easy. The leaders are also due to discuss Nordstream 2, Russia's controversial new pipeline that will bring natural gas to Europe, bypassing Ukraine and Poland. Donald Trump is not a fan. Mrs Merkel is, although she has recently begun to mute her support for the project, worried about the geopolitical ramifications. Image copyright AFP Image caption Mr Trump wants a bigger German contribution to defence And, as if the leaders needed more controversy, the subject of Germany's contributions to Nato are also listed for discussion. Commentators note that Mrs Merkel won't want to be seen to cave in to Mr Trump's demand for higher payments (and a commitment to increase German defence spending was in place prior to his election). The Trump administration is not popular among the German electorate; even if she could, few here want to see their leaders cosying up to Mr Trump. As Bild also noted, the newspaper unable to resist a gleeful tone, Mr Macron may have flattered the Trump ego, but he did not achieve much. Angela Merkel is not one for turning on the charm in these situations. She'll take a pragmatic approach, but no one in Berlin is expecting miracles.","Tensions over trade, Iran and Nato cloud Chancellor Merkel's Washington talks with President Trump.",2018-04-26T23:54:03Z,"{'id': 'bbc-news', 'name': 'BBC News'}",Merkel visits Trump - so will she envy Macron's bromance?,http://www.bbc.co.uk/news/world-europe-43904659,https://ichef.bbci.co.uk/news/1024/branded_news/3593/production/_101051731_gettyimages-810723450.jpg,bbc-news,BBC News,cloud,1
